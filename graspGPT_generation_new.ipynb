{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraspGPT 模型推理与生成\n",
    "\n",
    "本notebook用于从checkpoint加载GraspGPT模型，加载sequence序列数据，并进行token生成预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库和模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "from deepspeed import comm as dist\n",
    "\n",
    "# 设置路径并导入graspGPT模块\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "sys.path.append(os.path.join(current_dir, 'graspGPT'))\n",
    "\n",
    "from graspGPT.model.model import graspGPT\n",
    "from graspGPT.model.utils import CfgNode as CN\n",
    "from graspGPT.model.token_manager import get_token_manager, decode_sequence, encode_sequence\n",
    "from graspGPT.model.parser_and_serializer import Serializer, Parser,Seq,UNSEG, Scene\n",
    "from graspGPT.model.core import generate_amodal_sequence, generate_seg_sequence\n",
    "from graspGPT.model.precomputed_dataset import PrecomputedDataset\n",
    "import random\n",
    "\n",
    "print(\"模块导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "checkpoint_path = \"output/checkpoints/exp34\"  # 修改为实际的checkpoint路径\n",
    "sequence_file = \"output/scene_0000_objects_merged_aligned_seq.pth\"  # 修改为align_coords.py生成的pth文件路径\n",
    "deepspeed_config_path = \"deepspeed_config.json\"  # DeepSpeed配置文件路径\n",
    "\n",
    "# 生成参数\n",
    "max_new_tokens = 2000\n",
    "temperature = 0.3\n",
    "do_sample = True\n",
    "top_k = None\n",
    "num_sequences = 1\n",
    "seed = 42\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "print(f\"Checkpoint路径: {checkpoint_path}\")\n",
    "print(f\"序列文件路径: {sequence_file}\")\n",
    "print(f\"生成参数: max_new_tokens={max_new_tokens}, temperature={temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 辅助函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config_string(config_str):\n",
    "    \"\"\"解析字符串格式的配置到字典\"\"\"\n",
    "    config_dict = {}\n",
    "    for line in config_str.strip().split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            \n",
    "            # 尝试解析值\n",
    "            try:\n",
    "                if value.lower() == 'true':\n",
    "                    value = True\n",
    "                elif value.lower() == 'false':\n",
    "                    value = False\n",
    "                elif value.lower() == 'none':\n",
    "                    value = None\n",
    "                elif value.startswith('(') and value.endswith(')'):\n",
    "                    value = eval(value)\n",
    "                elif value.startswith('[') and value.endswith(']'):\n",
    "                    value = eval(value)\n",
    "                elif '.' in value:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        value = int(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            config_dict[key] = value\n",
    "    \n",
    "    return config_dict\n",
    "\n",
    "def load_training_config(checkpoint_dir):\n",
    "    \"\"\"从checkpoint目录加载训练配置\"\"\"\n",
    "    # 尝试从training_state.json加载配置\n",
    "    training_state_path = os.path.join(checkpoint_dir, 'training_state.json')\n",
    "    if os.path.exists(training_state_path):\n",
    "        with open(training_state_path, 'r') as f:\n",
    "            training_state = json.load(f)\n",
    "            if 'config' in training_state:\n",
    "                config_data = training_state['config']\n",
    "                \n",
    "                if isinstance(config_data, dict):\n",
    "                    parsed_config = {}\n",
    "                    for section_name, section_value in config_data.items():\n",
    "                        if isinstance(section_value, str):\n",
    "                            parsed_config[section_name] = parse_config_string(section_value)\n",
    "                        else:\n",
    "                            parsed_config[section_name] = section_value\n",
    "                    return CN.from_dict(parsed_config)\n",
    "                else:\n",
    "                    return CN.from_dict(config_data)\n",
    "    \n",
    "    # 备用：查找config.json文件\n",
    "    search_dirs = [checkpoint_dir, os.path.dirname(checkpoint_dir)]\n",
    "    config_names = ['config.json', 'training_config.json']\n",
    "    \n",
    "    for search_dir in search_dirs:\n",
    "        for config_name in config_names:\n",
    "            config_path = os.path.join(search_dir, config_name)\n",
    "            if os.path.exists(config_path):\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config_dict = json.load(f)\n",
    "                return CN.from_dict(config_dict)\n",
    "    \n",
    "    raise FileNotFoundError(f\"未找到配置文件在 {checkpoint_dir} 或其父目录中\")\n",
    "\n",
    "print(\"辅助函数定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 第一部分：从checkpoint目录加载模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练配置\n",
    "print(\"正在加载训练配置...\")\n",
    "config = load_training_config(checkpoint_path)\n",
    "print(\"训练配置加载成功\")\n",
    "\n",
    "\n",
    "config.model.tags =  CN()\n",
    "config.model.tags.sort_unseg = getattr(config.model, 'sort_unseg', False)\n",
    "config.model.tags.add_unlabel_noise = getattr(config.model, 'add_unlabel_noise', True)\n",
    "config.model.tags.translation_argument = getattr(config.model, 'translation_argument', False)\n",
    "config.model.tags.translate_scale = getattr(config.model, 'translate_scale', 5)\n",
    "config.model.tags.enable_grasp = getattr(config.model, 'enable_grasp', False)\n",
    "\n",
    "\n",
    "config.model.tags.enable_grasp = False\n",
    "config.model.tags.add_unlabel_noise = False\n",
    "\n",
    "print(config.model.tags.sort_unseg) \n",
    "\n",
    "print(config.model.tags.add_unlabel_noise)\n",
    "\n",
    "print(config.model.tags.translation_argument)\n",
    "\n",
    "print(config.model.tags.enable_grasp)\n",
    "\n",
    "\n",
    "\n",
    "# 打印模型配置信息\n",
    "print(f\"模型类型: {getattr(config.model, 'model_type', 'custom')}\")\n",
    "print(f\"词汇大小: {config.model.vocab_size}\")\n",
    "print(f\"块大小: {config.model.block_size}\")\n",
    "\n",
    "# 修复模型配置以满足XOR条件\n",
    "if hasattr(config.model, 'model_type') and config.model.model_type:\n",
    "    if hasattr(config.model, 'n_layer'):\n",
    "        config.model.n_layer = None\n",
    "    if hasattr(config.model, 'n_head'):\n",
    "        config.model.n_head = None  \n",
    "    if hasattr(config.model, 'n_embd'):\n",
    "        config.model.n_embd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取token管理器和词汇表\n",
    "print(\"正在获取token管理器...\")\n",
    "token_manager = get_token_manager()\n",
    "\n",
    "# 生成token映射\n",
    "img_h, img_w, img_d =80, 54, 34  # 默认体积维度\n",
    "config.dataset.data_path = 'output/precomputed_data/'\n",
    "# 尝试从数据中获取实际维度\n",
    "if hasattr(config.dataset, 'data_path') and config.dataset.data_path:\n",
    "    import glob\n",
    "    data_files = glob.glob(os.path.join(config.dataset.data_path, \"*.pth\"))\n",
    "    if data_files:\n",
    "        sample_file = data_files[0]\n",
    "        print(f\"从样本文件获取体积维度: {os.path.basename(sample_file)}\")\n",
    "        raw_data = torch.load(sample_file, weights_only=False)\n",
    "        if 'volume_dims' in raw_data:\n",
    "            img_h, img_w, img_d = raw_data['volume_dims']\n",
    "            print(f\"从数据获取的体积维度: {img_h}x{img_w}x{img_d}\")\n",
    "\n",
    "token_mapping = token_manager.generate_mapping(img_h, img_w, img_d)\n",
    "vocab_size = len(token_mapping)\n",
    "\n",
    "# 更新配置中的词汇大小\n",
    "config.model.vocab_size = vocab_size\n",
    "\n",
    "print(f\"Token管理器词汇大小: {vocab_size}\")\n",
    "print(f\"使用的体积维度: {img_h}x{img_w}x{img_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "print(\"正在创建模型...\")\n",
    "model = graspGPT(config.model)\n",
    "param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"模型创建完成: {param_count:.2f}M 参数\")\n",
    "\n",
    "# 加载DeepSpeed配置\n",
    "print(\"正在加载DeepSpeed配置...\")\n",
    "with open(deepspeed_config_path, 'r') as f:\n",
    "    ds_config = json.load(f)\n",
    "\n",
    "# 配置推理模式\n",
    "ds_config.update({\n",
    "    \"train_batch_size\": 1,\n",
    "    \"train_micro_batch_size_per_gpu\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "})\n",
    "\n",
    "if ds_config.get(\"bf16\", {}).get(\"enabled\", False):\n",
    "    print(\"bf16已启用用于推理\")\n",
    "\n",
    "print(\"DeepSpeed配置加载完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载checkpoint\n",
    "print(f\"正在加载checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# 解析checkpoint路径\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    parent_dir = os.path.dirname(checkpoint_path)\n",
    "    tag = os.path.basename(checkpoint_path)\n",
    "elif os.path.isdir(checkpoint_path):\n",
    "    parent_dir = os.path.dirname(checkpoint_path)\n",
    "    tag = os.path.basename(checkpoint_path)\n",
    "else:\n",
    "    raise ValueError(f\"Checkpoint路径不存在: {checkpoint_path}\")\n",
    "\n",
    "# 尝试使用DeepSpeed加载\n",
    "try:\n",
    "    # 初始化DeepSpeed引擎用于推理\n",
    "    model_engine, _, _, _ = deepspeed.initialize(\n",
    "        model=model,\n",
    "        config=ds_config,\n",
    "        model_parameters=model.parameters()\n",
    "    )\n",
    "    \n",
    "    # 加载checkpoint\n",
    "    _, client_state = model_engine.load_checkpoint(parent_dir, tag=tag)\n",
    "    print(f\"使用DeepSpeed成功加载checkpoint\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"DeepSpeed加载失败: {e}\")\n",
    "    print(\"回退到PyTorch加载...\")\n",
    "    \n",
    "    # 回退：使用常规PyTorch加载\n",
    "    checkpoint_file = os.path.join(checkpoint_path, 'mp_rank_00_model_states.pt')\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"使用PyTorch从以下位置加载checkpoint: {checkpoint_file}\")\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        \n",
    "        state_dict = checkpoint.get('module', checkpoint)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        \n",
    "        print(f\"使用PyTorch成功加载checkpoint\")\n",
    "        \n",
    "        # 创建模型包装器以兼容DeepSpeed接口\n",
    "        class ModelWrapper:\n",
    "            def __init__(self, model):\n",
    "                self.module = model\n",
    "                self.model = model\n",
    "                self.local_rank = 0\n",
    "                \n",
    "            def eval(self):\n",
    "                self.module.eval()\n",
    "                \n",
    "            def __call__(self, *args, **kwargs):\n",
    "                return self.module(*args, **kwargs)\n",
    "        \n",
    "        model_engine = ModelWrapper(model)\n",
    "    else:\n",
    "        raise ValueError(f\"找不到checkpoint文件: {checkpoint_file}\")\n",
    "\n",
    "print(\"模型加载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 第二部分：从外部pth文件加载sequence序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PrecomputedDataset(\n",
    "    data_path='output/real_data/test_m15_pths',\n",
    "    max_sequence_length=config.dataset.max_sequence_length,\n",
    "    tags = config.model.tags\n",
    ")\n",
    "\n",
    "print(getattr(config.dataset, 'real_filter_mode', 'allow_all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PrecomputedDataset(\n",
    "    data_path='output/real_data/tmp',\n",
    "    max_sequence_length=config.dataset.max_sequence_length,\n",
    "    tags = config.model.tags\n",
    ")\n",
    "\n",
    "print(getattr(config.dataset, 'real_filter_mode', 'allow_all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------(training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 加载序列数据\n",
    "sequence_data,lens, loss_mask = dataset.__getitem__(220)\n",
    "    \n",
    "sequence_data = sequence_data.squeeze().numpy()\n",
    "\n",
    "print(sequence_data)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"使用键 raw_tokens 作为token序列\")\n",
    "\n",
    "token_sequence = decode_sequence(sequence_data, token_mapping)\n",
    "#parser = Parser(token_sequence)\n",
    "#token_sequence = parser.parse()\n",
    "#print(len(token_sequence.items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 第三部分：在序列尾部加入新的指定tokens作为prompt，让model接下去预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grasp prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_tokens = token_sequence\n",
    "scene_promt = encode_sequence(token_sequence, token_mapping)\n",
    "\n",
    "scene_promt = scene_promt[:scene_promt.index(token_mapping['detectgrasp'])+1]\n",
    "print(len(scene_promt),scene_promt[-10:])\n",
    "\n",
    "# 指定要添加到序列尾部的tokens作为prompt\n",
    "# 这里可以根据需要修改，例如添加特定的命令tokens\n",
    "additional_tokens = [\n",
    "    #token_mapping['detectgrasp'],  # 检测抓取命令\n",
    "    token_mapping['grasp'],         # 抓取命令\n",
    "    #token_mapping['object24']\n",
    "]\n",
    "\n",
    "print(f\"要添加的tokens: {additional_tokens}\")\n",
    "print(f\"对应的token名称: {[k for k, v in token_mapping.items() if v in additional_tokens]}\")\n",
    "\n",
    "# 将新的tokens添加到序列尾部\n",
    "prompt_sequence = scene_promt + additional_tokens\n",
    "#prompt_sequence = scene_promt\n",
    "\n",
    "print(f\"添加prompt后的序列长度: {len(prompt_sequence)}\")\n",
    "print(f\"完整prompt序列的最后20个tokens: {prompt_sequence[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNSEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = token_sequence\n",
    "gt_tokens = tokens\n",
    "tokens = tokens[:tokens.index(\"segment\")+1]\n",
    "prompt_sequence = encode_sequence(tokens, token_mapping)\n",
    "print(len(tokens),len(gt_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备输入张量\n",
    "def prepare_input_from_tokens(token_ids, max_length=None):\n",
    "    \"\"\"从token ID列表准备输入\"\"\"\n",
    "    if not token_ids:\n",
    "        raise ValueError(\"提供的token ID列表为空\")\n",
    "    \n",
    "    # 转换为张量\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long)\n",
    "    \n",
    "    # 如果需要，进行截断\n",
    "    if max_length and input_ids.size(1) > max_length:\n",
    "        input_ids = input_ids[:, -max_length:]\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# 准备输入\n",
    "input_ids = prepare_input_from_tokens(prompt_sequence, config.model.block_size)\n",
    "input_ids = input_ids.repeat(2,1)\n",
    "original_length = input_ids.size(1)\n",
    "\n",
    "print(f\"输入张量形状: {input_ids.shape}\")\n",
    "print(f\"原始序列长度: {original_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置生成参数\n",
    "generation_config = {\n",
    "    'max_new_tokens': max_new_tokens+2500,\n",
    "    'temperature': temperature,\n",
    "    'do_sample': do_sample,\n",
    "    'top_k': top_k,\n",
    "    'eos_token_id': token_mapping.get('end', None)\n",
    "}\n",
    "\n",
    "print(f\"生成配置: {generation_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行生成\n",
    "def generate_with_model(model_engine, input_ids, generation_config):\n",
    "    \"\"\"使用模型生成序列\"\"\"\n",
    "    # 将输入移动到正确的设备\n",
    "    if hasattr(model_engine, 'local_rank') and model_engine.local_rank is not None:\n",
    "        device = f'cuda:{model_engine.local_rank}'\n",
    "    else:\n",
    "        # 回退：从模型参数获取设备\n",
    "        device = next(model_engine.module.parameters()).device\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # 将模型设置为评估模式\n",
    "    model_engine.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 生成序列\n",
    "        if hasattr(model_engine.module, 'generate'):\n",
    "            generated = model_engine.module.generate_ori(\n",
    "                idx=input_ids,\n",
    "                max_new_tokens=generation_config.get('max_new_tokens', 50),\n",
    "                temperature=generation_config.get('temperature', 1.0),\n",
    "                do_sample=generation_config.get('do_sample', True),\n",
    "                top_k=generation_config.get('top_k', None),\n",
    "                end_token=generation_config.get('eos_token_id', None),\n",
    "                #allow_unseg = True\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"模型没有generate方法\")\n",
    "    \n",
    "    return generated\n",
    "\n",
    "print(\"开始生成...\")\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"输入长度:\",input_ids.size())\n",
    "# 执行生成\n",
    "generated = generate_with_model(model_engine, input_ids, generation_config)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"生成完成，耗时: {generation_time:.2f}秒\")\n",
    "print(f\"生成的序列长度: {generated.size(1)} tokens. {generated.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from extract_sample_and_export import visualize_tokens\n",
    "\n",
    "all_scene  = generated.squeeze().detach().cpu()\n",
    "all_scene[:,-1] = token_mapping.get('end', None)\n",
    "for i in range(all_scene.size(0)):\n",
    "    visualize_tokens(all_scene[i], token_mapping, volume_dims =(img_h, img_w, img_d), bbox_min = np.array([-0.3, -0.2, 0]), voxel_size = 0.0075, output_dir = f'./output/tokens_visual/{i}')\n",
    "\n",
    "\n",
    "visualize_tokens(encode_sequence(gt_tokens, token_mapping), token_mapping, volume_dims =(img_h, img_w, img_d), bbox_min = np.array([-0.3, -0.2, 0]), voxel_size = 0.0075, output_dir = f'./output/tokens_visual/gt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(decode_sequence(generated[0].cpu().squeeze().numpy().tolist(),token_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_sequence = decode_sequence(generated[0].cpu().squeeze().numpy().tolist(),token_mapping)\n",
    "ast = Parser(token_sequence).parse()\n",
    "\n",
    "new_scene = None\n",
    "for item in ast.items:\n",
    "    if isinstance(item,UNSEG):\n",
    "        new_scene = Scene(sbs=item.sbs)\n",
    "        break\n",
    "\n",
    "new_seq = Seq(items=[new_scene])\n",
    "new_seq = Serializer.serialize(new_seq)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_promt = encode_sequence(new_seq, token_mapping)\n",
    "\n",
    "scene_promt = scene_promt[:-1]\n",
    "print(len(scene_promt),scene_promt[-10:])\n",
    "\n",
    "# 指定要添加到序列尾部的tokens作为prompt\n",
    "# 这里可以根据需要修改，例如添加特定的命令tokens\n",
    "additional_tokens = [\n",
    "    token_mapping['detectgrasp'],  # 检测抓取命令\n",
    "    token_mapping['grasp'],         # 抓取命令\n",
    "    #token_mapping['object24']\n",
    "]\n",
    "\n",
    "print(f\"要添加的tokens: {additional_tokens}\")\n",
    "print(f\"对应的token名称: {[k for k, v in token_mapping.items() if v in additional_tokens]}\")\n",
    "\n",
    "# 将新的tokens添加到序列尾部\n",
    "prompt_sequence = scene_promt + additional_tokens\n",
    "#prompt_sequence = scene_promt\n",
    "\n",
    "print(f\"添加prompt后的序列长度: {len(prompt_sequence)}\")\n",
    "print(f\"完整prompt序列的最后20个tokens: {prompt_sequence[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备输入\n",
    "input_ids = prepare_input_from_tokens(prompt_sequence, config.model.block_size)\n",
    "input_ids = input_ids.repeat(2,1)\n",
    "original_length = input_ids.size(1)\n",
    "\n",
    "print(f\"输入张量形状: {input_ids.shape}\")\n",
    "print(f\"原始序列长度: {original_length}\")\n",
    "\n",
    "# 配置生成参数\n",
    "generation_config = {\n",
    "    'max_new_tokens': max_new_tokens+1500,\n",
    "    'temperature': temperature,\n",
    "    'do_sample': do_sample,\n",
    "    'top_k': top_k,\n",
    "    'eos_token_id': token_mapping.get('end', None)\n",
    "}\n",
    "\n",
    "print(f\"生成配置: {generation_config}\")\n",
    "print(\"开始生成...\")\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"输入长度:\",input_ids.size())\n",
    "# 执行生成\n",
    "generated = generate_with_model(model_engine, input_ids, generation_config)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"生成完成，耗时: {generation_time:.2f}秒\")\n",
    "print(f\"生成的序列长度: {generated.size(1)} tokens. {generated.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_sample_and_export import visualize_tokens\n",
    "\n",
    "all_scene  = generated.squeeze().detach().cpu()\n",
    "all_scene[:,-1] = token_mapping.get('end', None)\n",
    "for i in range(all_scene.size(0)):\n",
    "    visualize_tokens(all_scene[i], token_mapping, volume_dims =(img_h, img_w, img_d), bbox_min = np.array([-0.3, -0.2, 0]), voxel_size = 0.0075, output_dir = f'./output/tokens_visual/grasp_{i}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 可选：可视化生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(sequence_data).unsqueeze(0).unsqueeze(-1).cuda().int()\n",
    "y = x[:,1:,:]\n",
    "x = x[:,:-1,:]\n",
    "loss_mask_a = loss_mask[1:]\n",
    "print(x.shape)\n",
    "print(loss_mask.shape)\n",
    "model_engine.module.eval()\n",
    "res = model_engine.module.forward(x, targets=y, loss_mask= loss_mask_a)\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存结果到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def analyze_token_confidence_and_contrast(\n",
    "    logits: torch.Tensor,\n",
    "    gt_token_ids: torch.Tensor,\n",
    "    pad_token_id: int = -1,\n",
    "    eps: float = 1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    对每个 token 计算自信度与“对比度”（正确答案 vs. 其他可能）。\n",
    "    \n",
    "    参数：\n",
    "        logits: [B, L, V]  模型输出的 logits\n",
    "        gt_token_ids: [B, L]  对齐的 ground-truth token id\n",
    "        pad_token_id: (可选) 若提供，则按此 id 进行 mask 聚合\n",
    "        eps: 数值稳定用\n",
    "\n",
    "    返回：\n",
    "        result: dict，包含以下同形张量（除聚合统计字段外均为 [B, L] 或相应 dtype）\n",
    "            - confidence: p(gt)\n",
    "            - top1_id: argmax token id\n",
    "            - top1_prob: 最大概率\n",
    "            - correct_top1: bool，是否 top-1 命中 gt\n",
    "            - rank: int，gt 的排名（1=第一）\n",
    "            - competitor_id: 除 gt 外概率最大的 token id\n",
    "            - competitor_prob: 其概率\n",
    "            - margin_prob: p(gt) - p(competitor)\n",
    "            - margin_logit: z(gt) - z(competitor)\n",
    "            - log_ratio: log( p(gt) / p(competitor) )\n",
    "            - ratio_to_other: p(gt) / (p(competitor)+eps)\n",
    "            - is_overruled: bool，是否被“错误答案”压过（== top1!=gt）\n",
    "            - mask: bool，非 pad 的有效位置\n",
    "            - summary: dict，聚合统计（忽略 pad）\n",
    "                * mean_confidence\n",
    "                * overruled_rate\n",
    "                * mean_margin_prob\n",
    "                * mean_margin_logit\n",
    "                * mean_log_ratio\n",
    "                * mean_rank\n",
    "    \"\"\"\n",
    "    B, L, V = logits.shape\n",
    "    gt = gt_token_ids[...,0].long()\n",
    "\n",
    "    # 概率与对数概率（一次性算好，避免重复 softmax）\n",
    "    log_probs = F.log_softmax(logits, dim=-1)      # [B, L, V]\n",
    "    probs = log_probs.exp()                        # [B, L, V]\n",
    "\n",
    "    # p(gt)、log p(gt)、z(gt)\n",
    "    gather_idx = gt.unsqueeze(-1)                  # [B, L, 1]\n",
    "    p_gt = torch.gather(probs, -1, gather_idx).squeeze(-1)           # [B, L]\n",
    "    logp_gt = torch.gather(log_probs, -1, gather_idx).squeeze(-1)    # [B, L]\n",
    "    z_gt = torch.gather(logits, -1, gather_idx).squeeze(-1)          # [B, L]\n",
    "\n",
    "    # top-1（全体最大）\n",
    "    top1_prob, top1_id = probs.max(dim=-1)         # [B, L], [B, L]\n",
    "    correct_top1 = top1_id.eq(gt)\n",
    "\n",
    "    # competitor（除 gt 外的最大者）\n",
    "    # 做法：把 gt 位置屏蔽为 -inf（对 logits），然后找最大；这样既能拿到 prob，也能拿到 logit 差\n",
    "    neg_inf = torch.finfo(logits.dtype).min\n",
    "    logits_others = logits.clone()\n",
    "    # 利用 scatter_ 在 gt 处写入 -inf\n",
    "    logits_others.scatter_(-1, gather_idx, neg_inf)\n",
    "    # 其他候选的最大 logit 及其 id\n",
    "    z_other, competitor_id = logits_others.max(dim=-1)               # [B, L]\n",
    "    # 对应的概率（从 softmax 后取，与 z_other 一致）\n",
    "    competitor_prob = torch.gather(probs, -1, competitor_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # 各种“对比度”与判定\n",
    "    margin_logit = z_gt - z_other                                     # >0 表示 gt 超过其他\n",
    "    margin_prob = p_gt - competitor_prob\n",
    "    log_ratio = logp_gt - torch.log(competitor_prob + eps)            # == log(p_gt / p_other)\n",
    "    ratio_to_other = p_gt / (competitor_prob + eps)\n",
    "    is_overruled = ~correct_top1                                       # 或者 (margin_prob < 0)\n",
    "\n",
    "    # gt 的排名：统计有多少概率严格大于 p_gt，再 +1\n",
    "    # 使用 (probs > p_gt[..., None]).sum(-1) + 1\n",
    "    rank = (probs > p_gt.unsqueeze(-1)).sum(dim=-1) + 1               # [B, L], int\n",
    "\n",
    "    # mask（忽略 pad）\n",
    "    if pad_token_id is not None:\n",
    "        mask = gt.ne(pad_token_id)\n",
    "    else:\n",
    "        mask = torch.ones_like(gt, dtype=torch.bool)\n",
    "\n",
    "    # 汇总统计（忽略 pad）\n",
    "    def _mean(x):\n",
    "        if mask.any():\n",
    "            return (x[mask].float().mean().item())\n",
    "        else:\n",
    "            return float('nan')\n",
    "\n",
    "    summary = {\n",
    "        \"mean_confidence\": _mean(p_gt),\n",
    "        \"overruled_rate\": _mean(is_overruled.float()),\n",
    "        \"mean_margin_prob\": _mean(margin_prob),\n",
    "        \"mean_margin_logit\": _mean(margin_logit),\n",
    "        \"mean_log_ratio\": _mean(log_ratio),\n",
    "        \"mean_rank\": _mean(rank.float()),\n",
    "    }\n",
    "\n",
    "    result = {\n",
    "        \"confidence\": p_gt,\n",
    "        \"top1_id\": top1_id,\n",
    "        \"top1_prob\": top1_prob,\n",
    "        \"correct_top1\": correct_top1,\n",
    "        \"rank\": rank,\n",
    "        \"competitor_id\": competitor_id,\n",
    "        \"competitor_prob\": competitor_prob,\n",
    "        \"margin_prob\": margin_prob,\n",
    "        \"margin_logit\": margin_logit,\n",
    "        \"log_ratio\": log_ratio,\n",
    "        \"ratio_to_other\": ratio_to_other,\n",
    "        \"is_overruled\": is_overruled,\n",
    "        \"mask\": mask,\n",
    "        \"summary\": summary,\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_token_confidence_and_contrast(res[0][0], y)\n",
    "confidences = result[\"confidence\"].cpu().float()  # [batch_size, seq_len]\n",
    "mask = result[\"mask\"]\n",
    "ratio_to_otherb = result[\"ratio_to_other\"].cpu().float()\n",
    "competitor_id = result['competitor_id'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 选择一个样本（例如 batch 的第 0 个）\n",
    "b = 0\n",
    "conf = confidences[b].detach().cpu().float()\n",
    "valid_mask = mask[b].detach().cpu()\n",
    "\n",
    "# 仅绘制有效 token（非 pad）\n",
    "conf = conf[valid_mask]\n",
    "\n",
    "plt.figure(figsize=(22, 4))\n",
    "plt.plot(range(len(conf)), conf, marker='o', label='Token Confidence')\n",
    "plt.title(f\"Token Confidence Curve (Sample {b})\")\n",
    "plt.xlabel(\"Token Index\")\n",
    "plt.ylabel(\"Confidence (p(gt))\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ratio_to_other = ratio_to_otherb[b].detach().cpu()\n",
    "\n",
    "ratio_to_other = ratio_to_other[valid_mask]\n",
    "\n",
    "plt.figure(figsize=(22, 4))\n",
    "plt.plot(range(len(conf)), ratio_to_other, marker='o', label='competitor ratio')\n",
    "plt.title(f\"Competitor Ratio Curve (Sample {b})\")\n",
    "plt.xlabel(\"Token Index\")\n",
    "plt.ylabel(\"competitor ratio)\")\n",
    "plt.ylim(0, 1.2)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def _to_float(x):\n",
    "    try:\n",
    "        return float(x.item() if hasattr(x, \"item\") else x)\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def _tokens_to_str(tokens):\n",
    "    if type(tokens)==str:\n",
    "        return tokens\n",
    "    try:\n",
    "        return \" \".join(tokens)\n",
    "    except Exception:\n",
    "        return str(tokens)\n",
    "\n",
    "seq = gt_tokens[1:]\n",
    "\n",
    "rows = []\n",
    "for i in range(len(tokens)-1, confidences.shape[-1]):\n",
    "#for i in range(0, confidences.shape[-1]):\n",
    "    if _to_float(ratio_to_otherb[0][i]) < 1:\n",
    "        rows.append({\n",
    "            \"idx\": i,\n",
    "            \"confidence\": _to_float(confidences[0][i]),\n",
    "            \"ratio\": _to_float(ratio_to_otherb[0][i]),\n",
    "            \"prev_5\": _tokens_to_str(seq[i-3:i]),\n",
    "            \"token_i\": _tokens_to_str(seq[i]),\n",
    "            \"next_4\": _tokens_to_str(seq[i+1:i+3]),\n",
    "            \"competitor token\": decode_sequence([competitor_id[0,i].item()],token_mapping)[0],\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"idx\", \"confidence\", \"ratio\", \"prev_5\", \"token_i\", \"next_4\",\"competitor token\"])\n",
    "\n",
    "print(len(rows))\n",
    "# 1) 终端打印“表格”（等宽文本）\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitor_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
