{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraspGPT 模型推理与生成\n",
    "\n",
    "本notebook用于从checkpoint加载GraspGPT模型，加载sequence序列数据，并进行token生成预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库和模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 16:43:32,891] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuminye/miniconda3/envs/grasp/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/wuminye/miniconda3/envs/grasp/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 16:43:35,792] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "模块导入完成\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "from deepspeed import comm as dist\n",
    "\n",
    "# 设置路径并导入graspGPT模块\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "sys.path.append(os.path.join(current_dir, 'graspGPT'))\n",
    "\n",
    "from graspGPT.model.model import graspGPT\n",
    "from graspGPT.model.utils import CfgNode as CN\n",
    "from graspGPT.model.token_manager import get_token_manager, decode_sequence, encode_sequence\n",
    "from graspGPT.model.parser_and_serializer import Serializer, Parser,Seq\n",
    "from graspGPT.model.core import generate_amodal_sequence, generate_seg_sequence\n",
    "from graspGPT.model.precomputed_dataset import PrecomputedDataset\n",
    "import random\n",
    "\n",
    "print(\"模块导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint路径: output/checkpoints/exp10\n",
      "序列文件路径: output/scene_0000_objects_merged_aligned_seq.pth\n",
      "生成参数: max_new_tokens=2000, temperature=0.3\n"
     ]
    }
   ],
   "source": [
    "# 设置参数\n",
    "checkpoint_path = \"output/checkpoints/exp10\"  # 修改为实际的checkpoint路径\n",
    "sequence_file = \"output/scene_0000_objects_merged_aligned_seq.pth\"  # 修改为align_coords.py生成的pth文件路径\n",
    "deepspeed_config_path = \"deepspeed_config.json\"  # DeepSpeed配置文件路径\n",
    "\n",
    "# 生成参数\n",
    "max_new_tokens = 2000\n",
    "temperature = 0.3\n",
    "do_sample = True\n",
    "top_k = None\n",
    "num_sequences = 1\n",
    "seed = 42\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "print(f\"Checkpoint路径: {checkpoint_path}\")\n",
    "print(f\"序列文件路径: {sequence_file}\")\n",
    "print(f\"生成参数: max_new_tokens={max_new_tokens}, temperature={temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 辅助函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "辅助函数定义完成\n"
     ]
    }
   ],
   "source": [
    "def parse_config_string(config_str):\n",
    "    \"\"\"解析字符串格式的配置到字典\"\"\"\n",
    "    config_dict = {}\n",
    "    for line in config_str.strip().split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            \n",
    "            # 尝试解析值\n",
    "            try:\n",
    "                if value.lower() == 'true':\n",
    "                    value = True\n",
    "                elif value.lower() == 'false':\n",
    "                    value = False\n",
    "                elif value.lower() == 'none':\n",
    "                    value = None\n",
    "                elif value.startswith('(') and value.endswith(')'):\n",
    "                    value = eval(value)\n",
    "                elif value.startswith('[') and value.endswith(']'):\n",
    "                    value = eval(value)\n",
    "                elif '.' in value:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        value = int(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            config_dict[key] = value\n",
    "    \n",
    "    return config_dict\n",
    "\n",
    "def load_training_config(checkpoint_dir):\n",
    "    \"\"\"从checkpoint目录加载训练配置\"\"\"\n",
    "    # 尝试从training_state.json加载配置\n",
    "    training_state_path = os.path.join(checkpoint_dir, 'training_state.json')\n",
    "    if os.path.exists(training_state_path):\n",
    "        with open(training_state_path, 'r') as f:\n",
    "            training_state = json.load(f)\n",
    "            if 'config' in training_state:\n",
    "                config_data = training_state['config']\n",
    "                \n",
    "                if isinstance(config_data, dict):\n",
    "                    parsed_config = {}\n",
    "                    for section_name, section_value in config_data.items():\n",
    "                        if isinstance(section_value, str):\n",
    "                            parsed_config[section_name] = parse_config_string(section_value)\n",
    "                        else:\n",
    "                            parsed_config[section_name] = section_value\n",
    "                    return CN.from_dict(parsed_config)\n",
    "                else:\n",
    "                    return CN.from_dict(config_data)\n",
    "    \n",
    "    # 备用：查找config.json文件\n",
    "    search_dirs = [checkpoint_dir, os.path.dirname(checkpoint_dir)]\n",
    "    config_names = ['config.json', 'training_config.json']\n",
    "    \n",
    "    for search_dir in search_dirs:\n",
    "        for config_name in config_names:\n",
    "            config_path = os.path.join(search_dir, config_name)\n",
    "            if os.path.exists(config_path):\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config_dict = json.load(f)\n",
    "                return CN.from_dict(config_dict)\n",
    "    \n",
    "    raise FileNotFoundError(f\"未找到配置文件在 {checkpoint_dir} 或其父目录中\")\n",
    "\n",
    "print(\"辅助函数定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 第一部分：从checkpoint目录加载模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载训练配置...\n",
      "训练配置加载成功\n",
      "模型类型: gpt2\n",
      "词汇大小: 147219\n",
      "块大小: 13000\n"
     ]
    }
   ],
   "source": [
    "# 加载训练配置\n",
    "print(\"正在加载训练配置...\")\n",
    "config = load_training_config(checkpoint_path)\n",
    "print(\"训练配置加载成功\")\n",
    "\n",
    "# 打印模型配置信息\n",
    "print(f\"模型类型: {getattr(config.model, 'model_type', 'custom')}\")\n",
    "print(f\"词汇大小: {config.model.vocab_size}\")\n",
    "print(f\"块大小: {config.model.block_size}\")\n",
    "\n",
    "# 修复模型配置以满足XOR条件\n",
    "if hasattr(config.model, 'model_type') and config.model.model_type:\n",
    "    if hasattr(config.model, 'n_layer'):\n",
    "        config.model.n_layer = None\n",
    "    if hasattr(config.model, 'n_head'):\n",
    "        config.model.n_head = None  \n",
    "    if hasattr(config.model, 'n_embd'):\n",
    "        config.model.n_embd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取token管理器...\n",
      "从样本文件获取体积维度: precomputed_batch_36024_0.pth\n",
      "Token管理器词汇大小: 147219\n",
      "使用的体积维度: 80x54x34\n"
     ]
    }
   ],
   "source": [
    "# 获取token管理器和词汇表\n",
    "print(\"正在获取token管理器...\")\n",
    "token_manager = get_token_manager()\n",
    "\n",
    "# 生成token映射\n",
    "img_h, img_w, img_d =80, 54, 34  # 默认体积维度\n",
    "config.dataset.data_path = 'output/precomputed_data/'\n",
    "# 尝试从数据中获取实际维度\n",
    "if hasattr(config.dataset, 'data_path') and config.dataset.data_path:\n",
    "    import glob\n",
    "    data_files = glob.glob(os.path.join(config.dataset.data_path, \"*.pth\"))\n",
    "    if data_files:\n",
    "        sample_file = data_files[0]\n",
    "        print(f\"从样本文件获取体积维度: {os.path.basename(sample_file)}\")\n",
    "        raw_data = torch.load(sample_file, weights_only=False)\n",
    "        if 'volume_dims' in raw_data:\n",
    "            img_h, img_w, img_d = raw_data['volume_dims']\n",
    "            print(f\"从数据获取的体积维度: {img_h}x{img_w}x{img_d}\")\n",
    "\n",
    "token_mapping = token_manager.generate_mapping(img_h, img_w, img_d)\n",
    "vocab_size = len(token_mapping)\n",
    "\n",
    "# 更新配置中的词汇大小\n",
    "config.model.vocab_size = vocab_size\n",
    "\n",
    "print(f\"Token管理器词汇大小: {vocab_size}\")\n",
    "print(f\"使用的体积维度: {img_h}x{img_w}x{img_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在创建模型...\n",
      "Using Qwen2 model with RoPE position encoding\n",
      "number of parameters: 216.91M\n",
      "模型创建完成: 216.91M 参数\n",
      "正在加载DeepSpeed配置...\n",
      "bf16已启用用于推理\n",
      "DeepSpeed配置加载完成\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "print(\"正在创建模型...\")\n",
    "model = graspGPT(config.model)\n",
    "param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"模型创建完成: {param_count:.2f}M 参数\")\n",
    "\n",
    "# 加载DeepSpeed配置\n",
    "print(\"正在加载DeepSpeed配置...\")\n",
    "with open(deepspeed_config_path, 'r') as f:\n",
    "    ds_config = json.load(f)\n",
    "\n",
    "# 配置推理模式\n",
    "ds_config.update({\n",
    "    \"train_batch_size\": 1,\n",
    "    \"train_micro_batch_size_per_gpu\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "})\n",
    "\n",
    "if ds_config.get(\"bf16\", {}).get(\"enabled\", False):\n",
    "    print(\"bf16已启用用于推理\")\n",
    "\n",
    "print(\"DeepSpeed配置加载完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载checkpoint: output/checkpoints/exp10\n",
      "[2025-10-14 16:43:41,814] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.5, git-hash=unknown, git-branch=unknown\n",
      "[2025-10-14 16:43:41,814] [INFO] [comm.py:821:init_distributed] cdb=None\n",
      "[2025-10-14 16:43:41,815] [INFO] [comm.py:836:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2025-10-14 16:43:42,206] [INFO] [comm.py:891:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.24.93.140, master_port=29500\n",
      "[2025-10-14 16:43:42,207] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-10-14 16:43:42,254] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 1\n",
      "[2025-10-14 16:43:42,666] [INFO] [engine.py:1356:_configure_distributed_model] ********** distributed groups summary **********\n",
      "\t self.dp_world_size=1\n",
      "\t self.mp_world_size=1\n",
      "\t self.seq_dp_world_size=1\n",
      "\t self.sequence_parallel_size=1\n",
      "***********************************************\n",
      "[2025-10-14 16:43:42,824] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/wuminye/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/wuminye/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...\n",
      "/home/wuminye/miniconda3/envs/grasp/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.023191213607788086 seconds\n",
      "[2025-10-14 16:43:42,852] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-10-14 16:43:42,852] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-10-14 16:43:42,855] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-10-14 16:43:42,856] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2025-10-14 16:43:42,857] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer\n",
      "[2025-10-14 16:43:42,857] [INFO] [stage_1_and_2.py:178:__init__] Reduce bucket size 500000000\n",
      "[2025-10-14 16:43:42,858] [INFO] [stage_1_and_2.py:179:__init__] Allgather bucket size 500000000\n",
      "[2025-10-14 16:43:42,858] [INFO] [stage_1_and_2.py:180:__init__] CPU Offload: False\n",
      "[2025-10-14 16:43:42,860] [INFO] [stage_1_and_2.py:181:__init__] Round robin gradient partitioning: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 16:43:43,295] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-10-14 16:43:43,297] [INFO] [utils.py:782:see_memory_usage] MA 1.21 GB         Max_MA 1.62 GB         CA 1.62 GB         Max_CA 2 GB \n",
      "[2025-10-14 16:43:43,298] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 3.8 GB, percent = 15.6%\n",
      "[2025-10-14 16:43:43,493] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-10-14 16:43:43,494] [INFO] [utils.py:782:see_memory_usage] MA 1.21 GB         Max_MA 2.02 GB         CA 2.43 GB         Max_CA 2 GB \n",
      "[2025-10-14 16:43:43,495] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 3.8 GB, percent = 15.6%\n",
      "[2025-10-14 16:43:43,495] [INFO] [stage_1_and_2.py:605:__init__] optimizer state initialized\n",
      "[2025-10-14 16:43:43,664] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-10-14 16:43:43,665] [INFO] [utils.py:782:see_memory_usage] MA 1.21 GB         Max_MA 1.21 GB         CA 2.43 GB         Max_CA 2 GB \n",
      "[2025-10-14 16:43:43,665] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 3.8 GB, percent = 15.6%\n",
      "[2025-10-14 16:43:43,669] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "DeepSpeed加载失败: unsupported operand type(s) for -: 'str' and 'str'\n",
      "回退到PyTorch加载...\n",
      "使用PyTorch从以下位置加载checkpoint: output/checkpoints/exp10/mp_rank_00_model_states.pt\n",
      "使用PyTorch成功加载checkpoint\n",
      "模型加载完成！\n"
     ]
    }
   ],
   "source": [
    "# 加载checkpoint\n",
    "print(f\"正在加载checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# 解析checkpoint路径\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    parent_dir = os.path.dirname(checkpoint_path)\n",
    "    tag = os.path.basename(checkpoint_path)\n",
    "elif os.path.isdir(checkpoint_path):\n",
    "    parent_dir = os.path.dirname(checkpoint_path)\n",
    "    tag = os.path.basename(checkpoint_path)\n",
    "else:\n",
    "    raise ValueError(f\"Checkpoint路径不存在: {checkpoint_path}\")\n",
    "\n",
    "# 尝试使用DeepSpeed加载\n",
    "try:\n",
    "    # 初始化DeepSpeed引擎用于推理\n",
    "    model_engine, _, _, _ = deepspeed.initialize(\n",
    "        model=model,\n",
    "        config=ds_config,\n",
    "        model_parameters=model.parameters()\n",
    "    )\n",
    "    \n",
    "    # 加载checkpoint\n",
    "    _, client_state = model_engine.load_checkpoint(parent_dir, tag=tag)\n",
    "    print(f\"使用DeepSpeed成功加载checkpoint\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"DeepSpeed加载失败: {e}\")\n",
    "    print(\"回退到PyTorch加载...\")\n",
    "    \n",
    "    # 回退：使用常规PyTorch加载\n",
    "    checkpoint_file = os.path.join(checkpoint_path, 'mp_rank_00_model_states.pt')\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"使用PyTorch从以下位置加载checkpoint: {checkpoint_file}\")\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        \n",
    "        state_dict = checkpoint.get('module', checkpoint)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        \n",
    "        print(f\"使用PyTorch成功加载checkpoint\")\n",
    "        \n",
    "        # 创建模型包装器以兼容DeepSpeed接口\n",
    "        class ModelWrapper:\n",
    "            def __init__(self, model):\n",
    "                self.module = model\n",
    "                self.model = model\n",
    "                self.local_rank = 0\n",
    "                \n",
    "            def eval(self):\n",
    "                self.module.eval()\n",
    "                \n",
    "            def __call__(self, *args, **kwargs):\n",
    "                return self.module(*args, **kwargs)\n",
    "        \n",
    "        model_engine = ModelWrapper(model)\n",
    "    else:\n",
    "        raise ValueError(f\"找不到checkpoint文件: {checkpoint_file}\")\n",
    "\n",
    "print(\"模型加载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 第二部分：从外部pth文件加载sequence序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed_batch_36024_14.pth...\n",
      "Loaded 283 precomputed samples from 1 files\n",
      "Memory optimized: sorted by length, using numpy arrays with appropriate dtypes\n",
      "amodal_only\n"
     ]
    }
   ],
   "source": [
    "dataset = PrecomputedDataset(\n",
    "    data_path='output/real_data/tmp',\n",
    "    max_sequence_length=config.dataset.max_sequence_length,\n",
    "    real_filter_mode=getattr(config.dataset, 'real_filter_mode', 'allow_all'),\n",
    ")\n",
    "\n",
    "print(getattr(config.dataset, 'real_filter_mode', 'allow_all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed_batch_36024_8.pth...\n",
      "Loaded 3000 precomputed samples from 1 files\n",
      "Memory optimized: sorted by length, using numpy arrays with appropriate dtypes\n",
      "amodal_only\n"
     ]
    }
   ],
   "source": [
    "dataset = PrecomputedDataset(\n",
    "    data_path='output/real_data/tmp',\n",
    "    max_sequence_length=config.dataset.max_sequence_length,\n",
    "    real_filter_mode=getattr(config.dataset, 'real_filter_mode', 'allow_all'),\n",
    "    apply_del_amodal_sequence = True\n",
    ")\n",
    "\n",
    "print(getattr(config.dataset, 'real_filter_mode', 'allow_all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------(training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    91      2  43045 ... 146575 119070 133792]\n",
      "使用键 raw_tokens 作为token序列\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 加载序列数据\n",
    "sequence_data = dataset.__getitem__(2500)[0].squeeze().numpy()\n",
    "\n",
    "print(sequence_data)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"使用键 raw_tokens 作为token序列\")\n",
    "\n",
    "token_sequence = decode_sequence(sequence_data, token_mapping)\n",
    "#parser = Parser(token_sequence)\n",
    "#token_sequence = parser.parse()\n",
    "#print(len(token_sequence.items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 第三部分：在序列尾部加入新的指定tokens作为prompt，让model接下去预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grasp prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1156 [146441, 146474, 146475, 146508, 146542, 146575, 146576, 146609, 147187, 96]\n",
      "要添加的tokens: [97]\n",
      "对应的token名称: ['grasp']\n",
      "添加prompt后的序列长度: 1157\n",
      "完整prompt序列的最后20个tokens: [146237, 146270, 146271, 146304, 146305, 146339, 146373, 146407, 146440, 146441, 146474, 146475, 146508, 146542, 146575, 146576, 146609, 147187, 96, 97]\n"
     ]
    }
   ],
   "source": [
    "gt_tokens = token_sequence\n",
    "scene_promt = encode_sequence(token_sequence, token_mapping)\n",
    "\n",
    "scene_promt = scene_promt[:scene_promt.index(token_mapping['detectgrasp'])+1]\n",
    "print(len(scene_promt),scene_promt[-10:])\n",
    "\n",
    "# 指定要添加到序列尾部的tokens作为prompt\n",
    "# 这里可以根据需要修改，例如添加特定的命令tokens\n",
    "additional_tokens = [\n",
    "    #token_mapping['detectgrasp'],  # 检测抓取命令\n",
    "    token_mapping['grasp'],         # 抓取命令\n",
    "    #token_mapping['object24']\n",
    "]\n",
    "\n",
    "print(f\"要添加的tokens: {additional_tokens}\")\n",
    "print(f\"对应的token名称: {[k for k, v in token_mapping.items() if v in additional_tokens]}\")\n",
    "\n",
    "# 将新的tokens添加到序列尾部\n",
    "prompt_sequence = scene_promt + additional_tokens\n",
    "#prompt_sequence = scene_promt\n",
    "\n",
    "print(f\"添加prompt后的序列长度: {len(prompt_sequence)}\")\n",
    "print(f\"完整prompt序列的最后20个tokens: {prompt_sequence[-20:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701 4537\n"
     ]
    }
   ],
   "source": [
    "tokens = token_sequence\n",
    "gt_tokens = tokens\n",
    "tokens = tokens[:tokens.index(\"unlabel\")+1]\n",
    "prompt_sequence = encode_sequence(tokens, token_mapping)\n",
    "print(len(tokens),len(gt_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNSEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = generate_seg_sequence(flat_tokens)\n",
    "gt_tokens = tokens\n",
    "tokens = tokens[:tokens.index(\"segment\")+1]\n",
    "prompt_sequence = encode_sequence(tokens, token_mapping)\n",
    "print(len(tokens),len(gt_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入张量形状: torch.Size([2, 1157])\n",
      "原始序列长度: 1157\n"
     ]
    }
   ],
   "source": [
    "# 准备输入张量\n",
    "def prepare_input_from_tokens(token_ids, max_length=None):\n",
    "    \"\"\"从token ID列表准备输入\"\"\"\n",
    "    if not token_ids:\n",
    "        raise ValueError(\"提供的token ID列表为空\")\n",
    "    \n",
    "    # 转换为张量\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long)\n",
    "    \n",
    "    # 如果需要，进行截断\n",
    "    if max_length and input_ids.size(1) > max_length:\n",
    "        input_ids = input_ids[:, -max_length:]\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# 准备输入\n",
    "input_ids = prepare_input_from_tokens(prompt_sequence, config.model.block_size)\n",
    "input_ids = input_ids.repeat(2,1)\n",
    "original_length = input_ids.size(1)\n",
    "\n",
    "print(f\"输入张量形状: {input_ids.shape}\")\n",
    "print(f\"原始序列长度: {original_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成配置: {'max_new_tokens': 5500, 'temperature': 0.8999999999999999, 'do_sample': True, 'top_k': None, 'eos_token_id': 98}\n"
     ]
    }
   ],
   "source": [
    "# 配置生成参数\n",
    "generation_config = {\n",
    "    'max_new_tokens': max_new_tokens+3500,\n",
    "    'temperature': temperature*3,\n",
    "    'do_sample': do_sample,\n",
    "    'top_k': top_k,\n",
    "    'eos_token_id': token_mapping.get('end', None)\n",
    "}\n",
    "\n",
    "print(f\"生成配置: {generation_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始生成...\n",
      "输入长度: torch.Size([2, 1157])\n",
      "生成完成，耗时: 23.48秒\n",
      "生成的序列长度: 2657 tokens. torch.Size([2, 2657, 1])\n"
     ]
    }
   ],
   "source": [
    "# 执行生成\n",
    "def generate_with_model(model_engine, input_ids, generation_config):\n",
    "    \"\"\"使用模型生成序列\"\"\"\n",
    "    # 将输入移动到正确的设备\n",
    "    if hasattr(model_engine, 'local_rank') and model_engine.local_rank is not None:\n",
    "        device = f'cuda:{model_engine.local_rank}'\n",
    "    else:\n",
    "        # 回退：从模型参数获取设备\n",
    "        device = next(model_engine.module.parameters()).device\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # 将模型设置为评估模式\n",
    "    model_engine.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 生成序列\n",
    "        if hasattr(model_engine.module, 'generate'):\n",
    "            generated = model_engine.module.generate_ori(\n",
    "                idx=input_ids,\n",
    "                max_new_tokens=generation_config.get('max_new_tokens', 50),\n",
    "                temperature=generation_config.get('temperature', 1.0),\n",
    "                do_sample=generation_config.get('do_sample', True),\n",
    "                top_k=generation_config.get('top_k', None),\n",
    "                end_token=generation_config.get('eos_token_id', None),\n",
    "                allow_unseg=True,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"模型没有generate方法\")\n",
    "    \n",
    "    return generated\n",
    "\n",
    "print(\"开始生成...\")\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"输入长度:\",input_ids.size())\n",
    "# 执行生成\n",
    "generated = generate_with_model(model_engine, input_ids, generation_config)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"生成完成，耗时: {generation_time:.2f}秒\")\n",
    "print(f\"生成的序列长度: {generated.size(1)} tokens. {generated.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始可视化tokens ===\n",
      "正在转换token ids到序列...\n",
      "解码得到的tokens数量: 2657\n",
      "前10个tokens: ['scene', 'incomplete', (23, 14, 2), (24, 15, 3), (24, 17, 2), (25, 13, 3), (25, 13, 4), (25, 14, 3), (25, 15, 3), (25, 16, 2)]\n",
      "解析成功，序列包含 2 个项目\n",
      "体素信息: dims=(80, 54, 34), bbox_min=[-0.3 -0.2  0. ], voxel_size=0.0075\n",
      "正在按类别提取点云...\n",
      "Scene SB 'incomplete': 1153 个点\n",
      "合并scene点云: 1153 个点\n",
      "amodal 类别没有点云\n",
      "unseg 类别没有点云\n",
      "合并grasp点云: 900 个点\n",
      "已生成并保存 300 个抓取mesh，目录: output/tokens_visual/0/grasp_meshes\n",
      "tokens可视化完成，结果保存在: output/tokens_visual/0\n",
      "=== 开始可视化tokens ===\n",
      "正在转换token ids到序列...\n",
      "解码得到的tokens数量: 2657\n",
      "前10个tokens: ['scene', 'incomplete', (23, 14, 2), (24, 15, 3), (24, 17, 2), (25, 13, 3), (25, 13, 4), (25, 14, 3), (25, 15, 3), (25, 16, 2)]\n",
      "解析成功，序列包含 2 个项目\n",
      "体素信息: dims=(80, 54, 34), bbox_min=[-0.3 -0.2  0. ], voxel_size=0.0075\n",
      "正在按类别提取点云...\n",
      "Scene SB 'incomplete': 1153 个点\n",
      "合并scene点云: 1153 个点\n",
      "amodal 类别没有点云\n",
      "unseg 类别没有点云\n",
      "合并grasp点云: 900 个点\n",
      "已生成并保存 300 个抓取mesh，目录: output/tokens_visual/1/grasp_meshes\n",
      "tokens可视化完成，结果保存在: output/tokens_visual/1\n",
      "=== 开始可视化tokens ===\n",
      "正在转换token ids到序列...\n",
      "解码得到的tokens数量: 3656\n",
      "前10个tokens: ['scene', 'incomplete', (23, 14, 2), (24, 15, 3), (24, 17, 2), (25, 13, 3), (25, 13, 4), (25, 14, 3), (25, 15, 3), (25, 16, 2)]\n",
      "解析成功，序列包含 2 个项目\n",
      "体素信息: dims=(80, 54, 34), bbox_min=[-0.3 -0.2  0. ], voxel_size=0.0075\n",
      "正在按类别提取点云...\n",
      "Scene SB 'incomplete': 1153 个点\n",
      "合并scene点云: 1153 个点\n",
      "amodal 类别没有点云\n",
      "unseg 类别没有点云\n",
      "合并grasp点云: 1500 个点\n",
      "已生成并保存 500 个抓取mesh，目录: output/tokens_visual/gt/grasp_meshes\n",
      "tokens可视化完成，结果保存在: output/tokens_visual/gt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('output/tokens_visual/gt')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from extract_sample_and_export import visualize_tokens\n",
    "\n",
    "all_scene  = generated.squeeze().detach().cpu()\n",
    "all_scene[:,-1] = token_mapping.get('end', None)\n",
    "for i in range(all_scene.size(0)):\n",
    "    visualize_tokens(all_scene[i], token_mapping, volume_dims =(img_h, img_w, img_d), bbox_min = np.array([-0.3, -0.2, 0]), voxel_size = 0.0075, output_dir = f'./output/tokens_visual/{i}')\n",
    "\n",
    "\n",
    "visualize_tokens(encode_sequence(gt_tokens, token_mapping), token_mapping, volume_dims =(img_h, img_w, img_d), bbox_min = np.array([-0.3, -0.2, 0]), voxel_size = 0.0075, output_dir = f'./output/tokens_visual/gt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(decode_sequence(generated[0].cpu().squeeze().numpy().tolist(),token_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整序列（原始 + prompt + 生成）\n",
    "full_sequence = input_ids[0].cpu().numpy().tolist() + decoded_tokens\n",
    "\n",
    "print(f\"完整序列长度: {len(full_sequence)}\")\n",
    "print(f\"完整序列的最后30个tokens: {full_sequence[-30:]}\")\n",
    "\n",
    "# 保存结果\n",
    "result = {\n",
    "    'original_sequence_length': len(token_sequence),\n",
    "    'prompt_tokens': additional_tokens,\n",
    "    'input_length': original_length,\n",
    "    'generated_tokens': decoded_tokens,\n",
    "    'generated_token_names': decoded_names,\n",
    "    'total_length': len(full_sequence),\n",
    "    'generation_time': generation_time,\n",
    "    'generation_config': generation_config,\n",
    "    'full_sequence': full_sequence\n",
    "}\n",
    "\n",
    "print(\"\\n=== 生成结果总结 ===\")\n",
    "print(f\"原始序列长度: {result['original_sequence_length']}\")\n",
    "print(f\"添加的prompt tokens: {result['prompt_tokens']}\")\n",
    "print(f\"输入长度: {result['input_length']}\")\n",
    "print(f\"生成的新tokens: {result['generated_tokens']}\")\n",
    "print(f\"生成时间: {result['generation_time']:.2f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 可选：可视化生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5818, 1])\n",
      "[tensor([[[  0.2832,  -7.4375,  19.7500,  ...,  -7.5000,  -7.4375,  -7.2812],\n",
      "         [ -8.3125, -10.6875,  -1.1562,  ..., -10.7500, -10.8125, -10.6875],\n",
      "         [ -3.9688,  -3.2188,   0.4180,  ...,  -3.0156,  -3.2500,  -3.0938],\n",
      "         ...,\n",
      "         [ -0.5469,  -0.7969,  -0.3906,  ...,  -0.8359,  -0.8555,  -0.7852],\n",
      "         [ -1.9922,  -4.2188,  -0.3398,  ...,  -4.1562,  -4.0938,  -4.2500],\n",
      "         [  2.2188,   1.4141,   0.9531,  ...,   1.4922,   1.4844,   1.4062]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(sequence_data).unsqueeze(0).unsqueeze(-1).cuda().int()\n",
    "y = x[:,1:,:]\n",
    "x = x[:,:-1,:]\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "res = model_engine.module.forward(x, targets=y)\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4375, device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存结果到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存结果到JSON文件\n",
    "output_file = \"generation_result.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"结果已保存到: {output_file}\")\n",
    "\n",
    "# 同时保存为.pth文件以便后续使用\n",
    "torch.save({\n",
    "    'generated_sequence': full_sequence,\n",
    "    'metadata': result\n",
    "}, \"generation_result.pth\")\n",
    "\n",
    "print(\"结果也已保存为generation_result.pth\")\n",
    "print(\"\\n生成完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
