{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraspGPT 模型推理与生成\n",
    "\n",
    "本notebook用于从checkpoint加载GraspGPT模型，加载sequence序列数据，并进行token生成预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库和模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "from deepspeed import comm as dist\n",
    "\n",
    "# 设置路径并导入graspGPT模块\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "sys.path.append(os.path.join(current_dir, 'graspGPT'))\n",
    "\n",
    "from graspGPT.model.model import graspGPT\n",
    "from graspGPT.model.utils import CfgNode as CN\n",
    "from graspGPT.model.token_manager import get_token_manager, decode_sequence, encode_sequence\n",
    "from graspGPT.model.parser_and_serializer import Serializer, Parser,Seq\n",
    "import random\n",
    "\n",
    "print(\"模块导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "checkpoint_path = \"output/checkpoints/allfullpoints\"  # 修改为实际的checkpoint路径\n",
    "sequence_file = \"output/scene_0000_objects_merged_aligned_seq.pth\"  # 修改为align_coords.py生成的pth文件路径\n",
    "deepspeed_config_path = \"deepspeed_config.json\"  # DeepSpeed配置文件路径\n",
    "\n",
    "# 生成参数\n",
    "max_new_tokens = 2000\n",
    "temperature = 0.3\n",
    "do_sample = True\n",
    "top_k = None\n",
    "num_sequences = 1\n",
    "seed = 42\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "print(f\"Checkpoint路径: {checkpoint_path}\")\n",
    "print(f\"序列文件路径: {sequence_file}\")\n",
    "print(f\"生成参数: max_new_tokens={max_new_tokens}, temperature={temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 辅助函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config_string(config_str):\n",
    "    \"\"\"解析字符串格式的配置到字典\"\"\"\n",
    "    config_dict = {}\n",
    "    for line in config_str.strip().split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            \n",
    "            # 尝试解析值\n",
    "            try:\n",
    "                if value.lower() == 'true':\n",
    "                    value = True\n",
    "                elif value.lower() == 'false':\n",
    "                    value = False\n",
    "                elif value.lower() == 'none':\n",
    "                    value = None\n",
    "                elif value.startswith('(') and value.endswith(')'):\n",
    "                    value = eval(value)\n",
    "                elif value.startswith('[') and value.endswith(']'):\n",
    "                    value = eval(value)\n",
    "                elif '.' in value:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        value = int(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            config_dict[key] = value\n",
    "    \n",
    "    return config_dict\n",
    "\n",
    "def load_training_config(checkpoint_dir):\n",
    "    \"\"\"从checkpoint目录加载训练配置\"\"\"\n",
    "    # 尝试从training_state.json加载配置\n",
    "    training_state_path = os.path.join(checkpoint_dir, 'training_state.json')\n",
    "    if os.path.exists(training_state_path):\n",
    "        with open(training_state_path, 'r') as f:\n",
    "            training_state = json.load(f)\n",
    "            if 'config' in training_state:\n",
    "                config_data = training_state['config']\n",
    "                \n",
    "                if isinstance(config_data, dict):\n",
    "                    parsed_config = {}\n",
    "                    for section_name, section_value in config_data.items():\n",
    "                        if isinstance(section_value, str):\n",
    "                            parsed_config[section_name] = parse_config_string(section_value)\n",
    "                        else:\n",
    "                            parsed_config[section_name] = section_value\n",
    "                    return CN.from_dict(parsed_config)\n",
    "                else:\n",
    "                    return CN.from_dict(config_data)\n",
    "    \n",
    "    # 备用：查找config.json文件\n",
    "    search_dirs = [checkpoint_dir, os.path.dirname(checkpoint_dir)]\n",
    "    config_names = ['config.json', 'training_config.json']\n",
    "    \n",
    "    for search_dir in search_dirs:\n",
    "        for config_name in config_names:\n",
    "            config_path = os.path.join(search_dir, config_name)\n",
    "            if os.path.exists(config_path):\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config_dict = json.load(f)\n",
    "                return CN.from_dict(config_dict)\n",
    "    \n",
    "    raise FileNotFoundError(f\"未找到配置文件在 {checkpoint_dir} 或其父目录中\")\n",
    "\n",
    "print(\"辅助函数定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 第一部分：从checkpoint目录加载模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练配置\n",
    "print(\"正在加载训练配置...\")\n",
    "config = load_training_config(checkpoint_path)\n",
    "print(\"训练配置加载成功\")\n",
    "\n",
    "# 打印模型配置信息\n",
    "print(f\"模型类型: {getattr(config.model, 'model_type', 'custom')}\")\n",
    "print(f\"词汇大小: {config.model.vocab_size}\")\n",
    "print(f\"块大小: {config.model.block_size}\")\n",
    "\n",
    "# 修复模型配置以满足XOR条件\n",
    "if hasattr(config.model, 'model_type') and config.model.model_type:\n",
    "    if hasattr(config.model, 'n_layer'):\n",
    "        config.model.n_layer = None\n",
    "    if hasattr(config.model, 'n_head'):\n",
    "        config.model.n_head = None  \n",
    "    if hasattr(config.model, 'n_embd'):\n",
    "        config.model.n_embd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取token管理器和词汇表\n",
    "print(\"正在获取token管理器...\")\n",
    "token_manager = get_token_manager()\n",
    "\n",
    "# 生成token映射\n",
    "img_h, img_w, img_d =80, 54, 34  # 默认体积维度\n",
    "config.dataset.data_path = 'output/precomputed_data/'\n",
    "# 尝试从数据中获取实际维度\n",
    "if hasattr(config.dataset, 'data_path') and config.dataset.data_path:\n",
    "    import glob\n",
    "    data_files = glob.glob(os.path.join(config.dataset.data_path, \"*.pth\"))\n",
    "    if data_files:\n",
    "        sample_file = data_files[0]\n",
    "        print(f\"从样本文件获取体积维度: {os.path.basename(sample_file)}\")\n",
    "        raw_data = torch.load(sample_file, weights_only=False)\n",
    "        if 'volume_dims' in raw_data:\n",
    "            img_h, img_w, img_d = raw_data['volume_dims']\n",
    "            print(f\"从数据获取的体积维度: {img_h}x{img_w}x{img_d}\")\n",
    "\n",
    "token_mapping = token_manager.generate_mapping(img_h, img_w, img_d)\n",
    "vocab_size = len(token_mapping)\n",
    "\n",
    "# 更新配置中的词汇大小\n",
    "config.model.vocab_size = vocab_size\n",
    "\n",
    "print(f\"Token管理器词汇大小: {vocab_size}\")\n",
    "print(f\"使用的体积维度: {img_h}x{img_w}x{img_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "print(\"正在创建模型...\")\n",
    "model = graspGPT(config.model)\n",
    "param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"模型创建完成: {param_count:.2f}M 参数\")\n",
    "\n",
    "# 加载DeepSpeed配置\n",
    "print(\"正在加载DeepSpeed配置...\")\n",
    "with open(deepspeed_config_path, 'r') as f:\n",
    "    ds_config = json.load(f)\n",
    "\n",
    "# 配置推理模式\n",
    "ds_config.update({\n",
    "    \"train_batch_size\": 1,\n",
    "    \"train_micro_batch_size_per_gpu\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "})\n",
    "\n",
    "if ds_config.get(\"bf16\", {}).get(\"enabled\", False):\n",
    "    print(\"bf16已启用用于推理\")\n",
    "\n",
    "print(\"DeepSpeed配置加载完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载checkpoint\n",
    "print(f\"正在加载checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# 解析checkpoint路径\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    parent_dir = os.path.dirname(checkpoint_path)\n",
    "    tag = os.path.basename(checkpoint_path)\n",
    "elif os.path.isdir(checkpoint_path):\n",
    "    parent_dir = os.path.dirname(checkpoint_path)\n",
    "    tag = os.path.basename(checkpoint_path)\n",
    "else:\n",
    "    raise ValueError(f\"Checkpoint路径不存在: {checkpoint_path}\")\n",
    "\n",
    "# 尝试使用DeepSpeed加载\n",
    "try:\n",
    "    # 初始化DeepSpeed引擎用于推理\n",
    "    model_engine, _, _, _ = deepspeed.initialize(\n",
    "        model=model,\n",
    "        config=ds_config,\n",
    "        model_parameters=model.parameters()\n",
    "    )\n",
    "    \n",
    "    # 加载checkpoint\n",
    "    _, client_state = model_engine.load_checkpoint(parent_dir, tag=tag)\n",
    "    print(f\"使用DeepSpeed成功加载checkpoint\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"DeepSpeed加载失败: {e}\")\n",
    "    print(\"回退到PyTorch加载...\")\n",
    "    \n",
    "    # 回退：使用常规PyTorch加载\n",
    "    checkpoint_file = os.path.join(checkpoint_path, 'mp_rank_00_model_states.pt')\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"使用PyTorch从以下位置加载checkpoint: {checkpoint_file}\")\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        \n",
    "        state_dict = checkpoint.get('module', checkpoint)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        \n",
    "        print(f\"使用PyTorch成功加载checkpoint\")\n",
    "        \n",
    "        # 创建模型包装器以兼容DeepSpeed接口\n",
    "        class ModelWrapper:\n",
    "            def __init__(self, model):\n",
    "                self.module = model\n",
    "                self.model = model\n",
    "                self.local_rank = 0\n",
    "                \n",
    "            def eval(self):\n",
    "                self.module.eval()\n",
    "                \n",
    "            def __call__(self, *args, **kwargs):\n",
    "                return self.module(*args, **kwargs)\n",
    "        \n",
    "        model_engine = ModelWrapper(model)\n",
    "    else:\n",
    "        raise ValueError(f\"找不到checkpoint文件: {checkpoint_file}\")\n",
    "\n",
    "print(\"模型加载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 第二部分：从外部pth文件加载sequence序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载序列数据\n",
    "print(f\"正在加载序列数据: {sequence_file}\")\n",
    "\n",
    "if not os.path.exists(sequence_file):\n",
    "    raise FileNotFoundError(f\"序列文件不存在: {sequence_file}\")\n",
    "\n",
    "# 加载序列数据\n",
    "sequence_data = torch.load(sequence_file, weights_only=False)\n",
    "\n",
    "# 检查数据结构\n",
    "print(f\"序列数据键: {list(sequence_data.keys()) if isinstance(sequence_data, dict) else 'not a dict'}\")\n",
    "\n",
    "# 提取token序列\n",
    "if isinstance(sequence_data, dict):\n",
    "    if 'tokens' in sequence_data:\n",
    "        token_sequence = sequence_data['tokens']\n",
    "    elif 'sequence' in sequence_data:\n",
    "        token_sequence = sequence_data['sequence']\n",
    "    elif 'token_ids' in sequence_data:\n",
    "        token_sequence = sequence_data['token_ids']\n",
    "    else:\n",
    "        # 假设第一个可用的键包含序列\n",
    "        key = list(sequence_data.keys())[0]\n",
    "        token_sequence = sequence_data[key]\n",
    "        print(f\"使用键 '{key}' 作为token序列\")\n",
    "elif isinstance(sequence_data, (list, tuple)):\n",
    "    token_sequence = sequence_data\n",
    "else:\n",
    "    token_sequence = sequence_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------(training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_file = 'output/precomputed_data/precomputed_batch_87750_11.pth'\n",
    "# 加载序列数据\n",
    "print(f\"正在加载序列数据: {sequence_file}\")\n",
    "\n",
    "if not os.path.exists(sequence_file):\n",
    "    raise FileNotFoundError(f\"序列文件不存在: {sequence_file}\")\n",
    "\n",
    "# 加载序列数据\n",
    "sequence_data = torch.load(sequence_file, weights_only=False)\n",
    "\n",
    "sequence_data = sequence_data[5]['raw_tokens']\n",
    "\n",
    "# 检查数据结构\n",
    "print(f\"序列数据键: {list(sequence_data.keys()) if isinstance(sequence_data, dict) else 'not a dict'}\")\n",
    "\n",
    "\n",
    "print(f\"使用键 raw_tokens 作为token序列\")\n",
    "\n",
    "token_sequence = decode_sequence(sequence_data, token_mapping)\n",
    "parser = Parser(token_sequence)\n",
    "token_sequence = parser.parse()\n",
    "print(len(token_sequence.items))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造场景数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs= token_sequence.items\n",
    "#random.shuffle(sbs)\n",
    "New_seq = Seq(items=sbs[:4])\n",
    "print(sbs[0].tag, sbs[1].tag)\n",
    "flat_tokens = Serializer.serialize(New_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_promt = encode_sequence(flat_tokens, token_mapping)[:-1]\n",
    "print(len(scene_promt),scene_promt[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 第三部分：在序列尾部加入新的指定tokens作为prompt，让model接下去预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要添加到序列尾部的tokens作为prompt\n",
    "# 这里可以根据需要修改，例如添加特定的命令tokens\n",
    "additional_tokens = [\n",
    "    token_mapping['detectgrasp'],  # 检测抓取命令\n",
    "    token_mapping['grasp'],         # 抓取命令\n",
    "    #token_mapping['object24']\n",
    "]\n",
    "\n",
    "print(f\"要添加的tokens: {additional_tokens}\")\n",
    "print(f\"对应的token名称: {[k for k, v in token_mapping.items() if v in additional_tokens]}\")\n",
    "\n",
    "# 将新的tokens添加到序列尾部\n",
    "prompt_sequence = scene_promt + additional_tokens\n",
    "#prompt_sequence = scene_promt\n",
    "\n",
    "print(f\"添加prompt后的序列长度: {len(prompt_sequence)}\")\n",
    "print(f\"完整prompt序列的最后20个tokens: {prompt_sequence[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备输入张量\n",
    "def prepare_input_from_tokens(token_ids, max_length=None):\n",
    "    \"\"\"从token ID列表准备输入\"\"\"\n",
    "    if not token_ids:\n",
    "        raise ValueError(\"提供的token ID列表为空\")\n",
    "    \n",
    "    # 转换为张量\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long)\n",
    "    \n",
    "    # 如果需要，进行截断\n",
    "    if max_length and input_ids.size(1) > max_length:\n",
    "        input_ids = input_ids[:, -max_length:]\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# 准备输入\n",
    "input_ids = prepare_input_from_tokens(prompt_sequence, config.model.block_size)\n",
    "input_ids = input_ids.repeat(2,1)\n",
    "original_length = input_ids.size(1)\n",
    "\n",
    "print(f\"输入张量形状: {input_ids.shape}\")\n",
    "print(f\"原始序列长度: {original_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置生成参数\n",
    "generation_config = {\n",
    "    'max_new_tokens': max_new_tokens,\n",
    "    'temperature': temperature,\n",
    "    'do_sample': do_sample,\n",
    "    'top_k': top_k,\n",
    "    'eos_token_id': token_mapping.get('end', None)\n",
    "}\n",
    "\n",
    "print(f\"生成配置: {generation_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行生成\n",
    "def generate_with_model(model_engine, input_ids, generation_config):\n",
    "    \"\"\"使用模型生成序列\"\"\"\n",
    "    # 将输入移动到正确的设备\n",
    "    if hasattr(model_engine, 'local_rank') and model_engine.local_rank is not None:\n",
    "        device = f'cuda:{model_engine.local_rank}'\n",
    "    else:\n",
    "        # 回退：从模型参数获取设备\n",
    "        device = next(model_engine.module.parameters()).device\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # 将模型设置为评估模式\n",
    "    model_engine.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 生成序列\n",
    "        if hasattr(model_engine.module, 'generate'):\n",
    "            generated = model_engine.module.generate(\n",
    "                idx=input_ids,\n",
    "                max_new_tokens=generation_config.get('max_new_tokens', 50),\n",
    "                temperature=generation_config.get('temperature', 1.0),\n",
    "                do_sample=generation_config.get('do_sample', True),\n",
    "                top_k=generation_config.get('top_k', None),\n",
    "                end_token=generation_config.get('eos_token_id', None)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"模型没有generate方法\")\n",
    "    \n",
    "    return generated\n",
    "\n",
    "print(\"开始生成...\")\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"输入长度:\",input_ids.size())\n",
    "# 执行生成\n",
    "generated = generate_with_model(model_engine, input_ids, generation_config)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"生成完成，耗时: {generation_time:.2f}秒\")\n",
    "print(f\"生成的序列长度: {generated.size(1)} tokens. {generated.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_sample_and_export import visualize_tokens\n",
    "\n",
    "all_scene  = generated.squeeze().detach().cpu()\n",
    "all_scene[:,-1] = 98\n",
    "for i in range(all_scene.size(0)):\n",
    "    visualize_tokens(all_scene[i], token_mapping, volume_dims =(img_h, img_w, img_d), bbox_min = np.array([-0.3, -0.2, 0]), voxel_size = 0.0075, output_dir = f'./output/tokens_visual/{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_scene[1][:].cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整序列（原始 + prompt + 生成）\n",
    "full_sequence = input_ids[0].cpu().numpy().tolist() + decoded_tokens\n",
    "\n",
    "print(f\"完整序列长度: {len(full_sequence)}\")\n",
    "print(f\"完整序列的最后30个tokens: {full_sequence[-30:]}\")\n",
    "\n",
    "# 保存结果\n",
    "result = {\n",
    "    'original_sequence_length': len(token_sequence),\n",
    "    'prompt_tokens': additional_tokens,\n",
    "    'input_length': original_length,\n",
    "    'generated_tokens': decoded_tokens,\n",
    "    'generated_token_names': decoded_names,\n",
    "    'total_length': len(full_sequence),\n",
    "    'generation_time': generation_time,\n",
    "    'generation_config': generation_config,\n",
    "    'full_sequence': full_sequence\n",
    "}\n",
    "\n",
    "print(\"\\n=== 生成结果总结 ===\")\n",
    "print(f\"原始序列长度: {result['original_sequence_length']}\")\n",
    "print(f\"添加的prompt tokens: {result['prompt_tokens']}\")\n",
    "print(f\"输入长度: {result['input_length']}\")\n",
    "print(f\"生成的新tokens: {result['generated_tokens']}\")\n",
    "print(f\"生成时间: {result['generation_time']:.2f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 可选：可视化生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可选：使用可视化功能（如果extract_sample_and_export模块可用）\n",
    "try:\n",
    "    sys.path.append('../')\n",
    "    from extract_sample_and_export import visualize_tokens\n",
    "    \n",
    "    # 设置可视化参数（根据实际情况调整）\n",
    "    volume_dims = (img_h, img_w, img_d)\n",
    "    bbox_min = np.array([-0.3, -0.2, 0])  # 根据实际情况调整\n",
    "    voxel_size = 0.0075  # 根据实际情况调整\n",
    "    \n",
    "    # 可视化完整序列\n",
    "    output_dir = \"./output/generation_visual/notebook_result\"\n",
    "    \n",
    "    visualize_tokens(\n",
    "        tokens=full_sequence,\n",
    "        token_mapping=token_mapping,\n",
    "        volume_dims=volume_dims,\n",
    "        bbox_min=bbox_min,\n",
    "        voxel_size=voxel_size,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"可视化结果保存到: {output_dir}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"可视化模块不可用，跳过可视化步骤\")\n",
    "except Exception as e:\n",
    "    print(f\"可视化过程中出错: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存结果到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存结果到JSON文件\n",
    "output_file = \"generation_result.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"结果已保存到: {output_file}\")\n",
    "\n",
    "# 同时保存为.pth文件以便后续使用\n",
    "torch.save({\n",
    "    'generated_sequence': full_sequence,\n",
    "    'metadata': result\n",
    "}, \"generation_result.pth\")\n",
    "\n",
    "print(\"结果也已保存为generation_result.pth\")\n",
    "print(\"\\n生成完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
