{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraspGPT 模型推理与生成\n",
    "\n",
    "本notebook用于从checkpoint加载GraspGPT模型，加载sequence序列数据，并进行token生成预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库和模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-09 23:01:00,891] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuminye/miniconda3/envs/grasp/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/wuminye/miniconda3/envs/grasp/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-09 23:01:03,598] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "模块导入完成\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import deepspeed\n",
    "from deepspeed import comm as dist\n",
    "\n",
    "# 设置路径并导入graspGPT模块\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir)\n",
    "sys.path.append(os.path.join(current_dir, 'graspGPT'))\n",
    "\n",
    "from graspGPT.model.model import graspGPT\n",
    "from graspGPT.model.utils import CfgNode as CN\n",
    "from graspGPT.model.token_manager import get_token_manager, decode_sequence, encode_sequence\n",
    "from graspGPT.model.parser_and_serializer import Serializer, Parser,Seq\n",
    "from graspGPT.model.core import generate_amodal_sequence, generate_seg_sequence\n",
    "import random\n",
    "\n",
    "print(\"模块导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint路径: output/checkpoints/amodal_only_real2\n",
      "序列文件路径: output/scene_0000_objects_merged_aligned_seq.pth\n",
      "生成参数: max_new_tokens=2000, temperature=0.3\n"
     ]
    }
   ],
   "source": [
    "# 设置参数\n",
    "checkpoint_path = \"output/checkpoints/amodal_only_real2\"  # 修改为实际的checkpoint路径\n",
    "sequence_file = \"output/scene_0000_objects_merged_aligned_seq.pth\"  # 修改为align_coords.py生成的pth文件路径\n",
    "deepspeed_config_path = \"deepspeed_config.json\"  # DeepSpeed配置文件路径\n",
    "\n",
    "# 生成参数\n",
    "max_new_tokens = 2000\n",
    "temperature = 0.3\n",
    "do_sample = True\n",
    "top_k = None\n",
    "num_sequences = 1\n",
    "seed = 42\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "print(f\"Checkpoint路径: {checkpoint_path}\")\n",
    "print(f\"序列文件路径: {sequence_file}\")\n",
    "print(f\"生成参数: max_new_tokens={max_new_tokens}, temperature={temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 辅助函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "辅助函数定义完成\n"
     ]
    }
   ],
   "source": [
    "def parse_config_string(config_str):\n",
    "    \"\"\"解析字符串格式的配置到字典\"\"\"\n",
    "    config_dict = {}\n",
    "    for line in config_str.strip().split('\\n'):\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            key = key.strip()\n",
    "            value = value.strip()\n",
    "            \n",
    "            # 尝试解析值\n",
    "            try:\n",
    "                if value.lower() == 'true':\n",
    "                    value = True\n",
    "                elif value.lower() == 'false':\n",
    "                    value = False\n",
    "                elif value.lower() == 'none':\n",
    "                    value = None\n",
    "                elif value.startswith('(') and value.endswith(')'):\n",
    "                    value = eval(value)\n",
    "                elif value.startswith('[') and value.endswith(']'):\n",
    "                    value = eval(value)\n",
    "                elif '.' in value:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        value = int(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            config_dict[key] = value\n",
    "    \n",
    "    return config_dict\n",
    "\n",
    "def load_training_config(checkpoint_dir):\n",
    "    \"\"\"从checkpoint目录加载训练配置\"\"\"\n",
    "    # 尝试从training_state.json加载配置\n",
    "    training_state_path = os.path.join(checkpoint_dir, 'training_state.json')\n",
    "    if os.path.exists(training_state_path):\n",
    "        with open(training_state_path, 'r') as f:\n",
    "            training_state = json.load(f)\n",
    "            if 'config' in training_state:\n",
    "                config_data = training_state['config']\n",
    "                \n",
    "                if isinstance(config_data, dict):\n",
    "                    parsed_config = {}\n",
    "                    for section_name, section_value in config_data.items():\n",
    "                        if isinstance(section_value, str):\n",
    "                            parsed_config[section_name] = parse_config_string(section_value)\n",
    "                        else:\n",
    "                            parsed_config[section_name] = section_value\n",
    "                    return CN.from_dict(parsed_config)\n",
    "                else:\n",
    "                    return CN.from_dict(config_data)\n",
    "    \n",
    "    # 备用：查找config.json文件\n",
    "    search_dirs = [checkpoint_dir, os.path.dirname(checkpoint_dir)]\n",
    "    config_names = ['config.json', 'training_config.json']\n",
    "    \n",
    "    for search_dir in search_dirs:\n",
    "        for config_name in config_names:\n",
    "            config_path = os.path.join(search_dir, config_name)\n",
    "            if os.path.exists(config_path):\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config_dict = json.load(f)\n",
    "                return CN.from_dict(config_dict)\n",
    "    \n",
    "    raise FileNotFoundError(f\"未找到配置文件在 {checkpoint_dir} 或其父目录中\")\n",
    "\n",
    "print(\"辅助函数定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 第一部分：从checkpoint目录加载模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载训练配置...\n",
      "训练配置加载成功\n",
      "模型类型: gpt2\n",
      "词汇大小: 147219\n",
      "块大小: 13000\n"
     ]
    }
   ],
   "source": [
    "# 加载训练配置\n",
    "print(\"正在加载训练配置...\")\n",
    "config = load_training_config(checkpoint_path)\n",
    "print(\"训练配置加载成功\")\n",
    "\n",
    "# 打印模型配置信息\n",
    "print(f\"模型类型: {getattr(config.model, 'model_type', 'custom')}\")\n",
    "print(f\"词汇大小: {config.model.vocab_size}\")\n",
    "print(f\"块大小: {config.model.block_size}\")\n",
    "\n",
    "# 修复模型配置以满足XOR条件\n",
    "if hasattr(config.model, 'model_type') and config.model.model_type:\n",
    "    if hasattr(config.model, 'n_layer'):\n",
    "        config.model.n_layer = None\n",
    "    if hasattr(config.model, 'n_head'):\n",
    "        config.model.n_head = None  \n",
    "    if hasattr(config.model, 'n_embd'):\n",
    "        config.model.n_embd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在获取token管理器...\n",
      "从样本文件获取体积维度: precomputed_batch_63938_0.pth\n",
      "Token管理器词汇大小: 147219\n",
      "使用的体积维度: 80x54x34\n"
     ]
    }
   ],
   "source": [
    "# 获取token管理器和词汇表\n",
    "print(\"正在获取token管理器...\")\n",
    "token_manager = get_token_manager()\n",
    "\n",
    "# 生成token映射\n",
    "img_h, img_w, img_d =80, 54, 34  # 默认体积维度\n",
    "config.dataset.data_path = 'output/precomputed_data/'\n",
    "# 尝试从数据中获取实际维度\n",
    "if hasattr(config.dataset, 'data_path') and config.dataset.data_path:\n",
    "    import glob\n",
    "    data_files = glob.glob(os.path.join(config.dataset.data_path, \"*.pth\"))\n",
    "    if data_files:\n",
    "        sample_file = data_files[0]\n",
    "        print(f\"从样本文件获取体积维度: {os.path.basename(sample_file)}\")\n",
    "        raw_data = torch.load(sample_file, weights_only=False)\n",
    "        if 'volume_dims' in raw_data:\n",
    "            img_h, img_w, img_d = raw_data['volume_dims']\n",
    "            print(f\"从数据获取的体积维度: {img_h}x{img_w}x{img_d}\")\n",
    "\n",
    "token_mapping = token_manager.generate_mapping(img_h, img_w, img_d)\n",
    "vocab_size = len(token_mapping)\n",
    "\n",
    "# 更新配置中的词汇大小\n",
    "config.model.vocab_size = vocab_size\n",
    "\n",
    "print(f\"Token管理器词汇大小: {vocab_size}\")\n",
    "print(f\"使用的体积维度: {img_h}x{img_w}x{img_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在创建模型...\n",
      "Using Qwen2 model with RoPE position encoding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 216.91M\n",
      "模型创建完成: 216.91M 参数\n",
      "正在加载DeepSpeed配置...\n",
      "bf16已启用用于推理\n",
      "DeepSpeed配置加载完成\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "print(\"正在创建模型...\")\n",
    "model = graspGPT(config.model)\n",
    "param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f\"模型创建完成: {param_count:.2f}M 参数\")\n",
    "\n",
    "# 加载DeepSpeed配置\n",
    "print(\"正在加载DeepSpeed配置...\")\n",
    "with open(deepspeed_config_path, 'r') as f:\n",
    "    ds_config = json.load(f)\n",
    "\n",
    "# 配置推理模式\n",
    "ds_config.update({\n",
    "    \"train_batch_size\": 1,\n",
    "    \"train_micro_batch_size_per_gpu\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "})\n",
    "\n",
    "if ds_config.get(\"bf16\", {}).get(\"enabled\", False):\n",
    "    print(\"bf16已启用用于推理\")\n",
    "\n",
    "print(\"DeepSpeed配置加载完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载checkpoint: output/checkpoints/amodal_only_real2\n",
      "[2025-10-09 23:01:08,493] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.17.5, git-hash=unknown, git-branch=unknown\n",
      "[2025-10-09 23:01:08,493] [INFO] [comm.py:821:init_distributed] cdb=None\n",
      "[2025-10-09 23:01:08,494] [INFO] [comm.py:836:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2025-10-09 23:01:08,811] [INFO] [comm.py:891:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.24.93.140, master_port=29500\n",
      "[2025-10-09 23:01:08,811] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-10-09 23:01:08,891] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 1\n",
      "[2025-10-09 23:01:09,152] [INFO] [engine.py:1356:_configure_distributed_model] ********** distributed groups summary **********\n",
      "\t self.dp_world_size=1\n",
      "\t self.mp_world_size=1\n",
      "\t self.seq_dp_world_size=1\n",
      "\t self.sequence_parallel_size=1\n",
      "***********************************************\n",
      "[2025-10-09 23:01:09,291] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/wuminye/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/wuminye/.cache/torch_extensions/py310_cu118/fused_adam/build.ninja...\n",
      "/home/wuminye/miniconda3/envs/grasp/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.019610166549682617 seconds\n",
      "[2025-10-09 23:01:09,313] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-10-09 23:01:09,314] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-10-09 23:01:09,317] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-10-09 23:01:09,318] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2025-10-09 23:01:09,318] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer\n",
      "[2025-10-09 23:01:09,319] [INFO] [stage_1_and_2.py:178:__init__] Reduce bucket size 500000000\n",
      "[2025-10-09 23:01:09,320] [INFO] [stage_1_and_2.py:179:__init__] Allgather bucket size 500000000\n",
      "[2025-10-09 23:01:09,320] [INFO] [stage_1_and_2.py:180:__init__] CPU Offload: False\n",
      "[2025-10-09 23:01:09,321] [INFO] [stage_1_and_2.py:181:__init__] Round robin gradient partitioning: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-09 23:01:09,786] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-10-09 23:01:09,788] [INFO] [utils.py:782:see_memory_usage] MA 1.21 GB         Max_MA 1.62 GB         CA 1.62 GB         Max_CA 2 GB \n",
      "[2025-10-09 23:01:09,789] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.19 GB, percent = 37.6%\n",
      "[2025-10-09 23:01:09,947] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-10-09 23:01:09,948] [INFO] [utils.py:782:see_memory_usage] MA 1.21 GB         Max_MA 2.02 GB         CA 2.43 GB         Max_CA 2 GB \n",
      "[2025-10-09 23:01:09,949] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.19 GB, percent = 37.6%\n",
      "[2025-10-09 23:01:09,950] [INFO] [stage_1_and_2.py:605:__init__] optimizer state initialized\n",
      "[2025-10-09 23:01:10,078] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-10-09 23:01:10,079] [INFO] [utils.py:782:see_memory_usage] MA 1.21 GB         Max_MA 1.21 GB         CA 2.43 GB         Max_CA 2 GB \n",
      "[2025-10-09 23:01:10,081] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.19 GB, percent = 37.6%\n",
      "[2025-10-09 23:01:10,084] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "DeepSpeed加载失败: unsupported operand type(s) for -: 'str' and 'str'\n",
      "回退到PyTorch加载...\n",
      "使用PyTorch从以下位置加载checkpoint: output/checkpoints/amodal_only_real2/mp_rank_00_model_states.pt\n",
      "使用PyTorch成功加载checkpoint\n",
      "模型加载完成！\n"
     ]
    }
   ],
   "source": [
    "# 加载checkpoint\n",
    "print(f\"正在加载checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# 解析checkpoint路径\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    parent_dir = os.path.dirname(checkpoint_path)\n",
    "    tag = os.path.basename(checkpoint_path)\n",
    "elif os.path.isdir(checkpoint_path):\n",
    "    parent_dir = os.path.dirname(checkpoint_path)\n",
    "    tag = os.path.basename(checkpoint_path)\n",
    "else:\n",
    "    raise ValueError(f\"Checkpoint路径不存在: {checkpoint_path}\")\n",
    "\n",
    "# 尝试使用DeepSpeed加载\n",
    "try:\n",
    "    # 初始化DeepSpeed引擎用于推理\n",
    "    model_engine, _, _, _ = deepspeed.initialize(\n",
    "        model=model,\n",
    "        config=ds_config,\n",
    "        model_parameters=model.parameters()\n",
    "    )\n",
    "    \n",
    "    # 加载checkpoint\n",
    "    _, client_state = model_engine.load_checkpoint(parent_dir, tag=tag)\n",
    "    print(f\"使用DeepSpeed成功加载checkpoint\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"DeepSpeed加载失败: {e}\")\n",
    "    print(\"回退到PyTorch加载...\")\n",
    "    \n",
    "    # 回退：使用常规PyTorch加载\n",
    "    checkpoint_file = os.path.join(checkpoint_path, 'mp_rank_00_model_states.pt')\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"使用PyTorch从以下位置加载checkpoint: {checkpoint_file}\")\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        \n",
    "        state_dict = checkpoint.get('module', checkpoint)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "        \n",
    "        print(f\"使用PyTorch成功加载checkpoint\")\n",
    "        \n",
    "        # 创建模型包装器以兼容DeepSpeed接口\n",
    "        class ModelWrapper:\n",
    "            def __init__(self, model):\n",
    "                self.module = model\n",
    "                self.model = model\n",
    "                self.local_rank = 0\n",
    "                \n",
    "            def eval(self):\n",
    "                self.module.eval()\n",
    "                \n",
    "            def __call__(self, *args, **kwargs):\n",
    "                return self.module(*args, **kwargs)\n",
    "        \n",
    "        model_engine = ModelWrapper(model)\n",
    "    else:\n",
    "        raise ValueError(f\"找不到checkpoint文件: {checkpoint_file}\")\n",
    "\n",
    "print(\"模型加载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 第二部分：从外部pth文件加载sequence序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载序列数据: output/scene_0000_objects_merged_aligned_seq.pth\n",
      "序列数据键: ['seq', 'token_sequence', 'data_list', 'volume_dims', 'voxel_size', 'bbox_min', 'bbox_max', 'scene_name']\n",
      "使用键 'seq' 作为token序列\n"
     ]
    }
   ],
   "source": [
    "# 加载序列数据\n",
    "print(f\"正在加载序列数据: {sequence_file}\")\n",
    "\n",
    "if not os.path.exists(sequence_file):\n",
    "    raise FileNotFoundError(f\"序列文件不存在: {sequence_file}\")\n",
    "\n",
    "# 加载序列数据\n",
    "sequence_data = torch.load(sequence_file, weights_only=False)\n",
    "\n",
    "# 检查数据结构\n",
    "print(f\"序列数据键: {list(sequence_data.keys()) if isinstance(sequence_data, dict) else 'not a dict'}\")\n",
    "\n",
    "# 提取token序列\n",
    "if isinstance(sequence_data, dict):\n",
    "    if 'tokens' in sequence_data:\n",
    "        token_sequence = sequence_data['tokens']\n",
    "    elif 'sequence' in sequence_data:\n",
    "        token_sequence = sequence_data['sequence']\n",
    "    elif 'token_ids' in sequence_data:\n",
    "        token_sequence = sequence_data['token_ids']\n",
    "    else:\n",
    "        # 假设第一个可用的键包含序列\n",
    "        key = list(sequence_data.keys())[0]\n",
    "        token_sequence = sequence_data[key]\n",
    "        print(f\"使用键 '{key}' 作为token序列\")\n",
    "elif isinstance(sequence_data, (list, tuple)):\n",
    "    token_sequence = sequence_data\n",
    "else:\n",
    "    token_sequence = sequence_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------(training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载序列数据: output/precomputed_data/precomputed_batch_63938_0.pth\n",
      "序列数据键: not a dict\n",
      "使用键 raw_tokens 作为token序列\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sequence_file = 'output/precomputed_data/precomputed_batch_63938_0.pth'\n",
    "# 加载序列数据\n",
    "print(f\"正在加载序列数据: {sequence_file}\")\n",
    "\n",
    "if not os.path.exists(sequence_file):\n",
    "    raise FileNotFoundError(f\"序列文件不存在: {sequence_file}\")\n",
    "\n",
    "# 加载序列数据\n",
    "sequence_data = torch.load(sequence_file, weights_only=False)\n",
    "\n",
    "sequence_data = sequence_data[5]['raw_tokens']\n",
    "\n",
    "# 检查数据结构\n",
    "print(f\"序列数据键: {list(sequence_data.keys()) if isinstance(sequence_data, dict) else 'not a dict'}\")\n",
    "\n",
    "\n",
    "print(f\"使用键 raw_tokens 作为token序列\")\n",
    "\n",
    "token_sequence = decode_sequence(sequence_data, token_mapping)\n",
    "parser = Parser(token_sequence)\n",
    "token_sequence = parser.parse()\n",
    "print(len(token_sequence.items))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造场景数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object13 object40\n"
     ]
    }
   ],
   "source": [
    "sbs= token_sequence.items\n",
    "#random.shuffle(sbs)\n",
    "sbs[0].sbs = sbs[0].sbs[:4]\n",
    "New_seq = Seq(items=[sbs[0]])\n",
    "print(sbs[0].sbs[0].tag, sbs[0].sbs[1].tag)\n",
    "flat_tokens = Serializer.serialize(New_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 第三部分：在序列尾部加入新的指定tokens作为prompt，让model接下去预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grasp prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2303 [73339, 73371, 73372, 73373, 73406, 75106, 75107, 75140, 75141, 75174]\n",
      "要添加的tokens: [96, 97]\n",
      "对应的token名称: ['detectgrasp', 'grasp']\n",
      "添加prompt后的序列长度: 2305\n",
      "完整prompt序列的最后20个tokens: [73269, 73270, 73271, 73303, 73304, 73305, 73337, 73338, 73339, 73371, 73372, 73373, 73406, 75106, 75107, 75140, 75141, 75174, 96, 97]\n"
     ]
    }
   ],
   "source": [
    "scene_promt = encode_sequence(flat_tokens, token_mapping)[:-1]\n",
    "print(len(scene_promt),scene_promt[-10:])\n",
    "\n",
    "# 指定要添加到序列尾部的tokens作为prompt\n",
    "# 这里可以根据需要修改，例如添加特定的命令tokens\n",
    "additional_tokens = [\n",
    "    token_mapping['detectgrasp'],  # 检测抓取命令\n",
    "    token_mapping['grasp'],         # 抓取命令\n",
    "    #token_mapping['object24']\n",
    "]\n",
    "\n",
    "print(f\"要添加的tokens: {additional_tokens}\")\n",
    "print(f\"对应的token名称: {[k for k, v in token_mapping.items() if v in additional_tokens]}\")\n",
    "\n",
    "# 将新的tokens添加到序列尾部\n",
    "prompt_sequence = scene_promt + additional_tokens\n",
    "#prompt_sequence = scene_promt\n",
    "\n",
    "print(f\"添加prompt后的序列长度: {len(prompt_sequence)}\")\n",
    "print(f\"完整prompt序列的最后20个tokens: {prompt_sequence[-20:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983 3283\n"
     ]
    }
   ],
   "source": [
    "tokens = generate_amodal_sequence(flat_tokens,(img_h, img_w, img_d))\n",
    "gt_tokens = tokens\n",
    "tokens = tokens[:tokens.index(\"unlabel\")+1]\n",
    "prompt_sequence = encode_sequence(tokens, token_mapping)\n",
    "print(len(tokens),len(gt_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNSEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2301 4605\n"
     ]
    }
   ],
   "source": [
    "tokens = generate_seg_sequence(flat_tokens)\n",
    "gt_tokens = tokens\n",
    "tokens = tokens[:tokens.index(\"segment\")+1]\n",
    "prompt_sequence = encode_sequence(tokens, token_mapping)\n",
    "print(len(tokens),len(gt_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入张量形状: torch.Size([2, 983])\n",
      "原始序列长度: 983\n"
     ]
    }
   ],
   "source": [
    "# 准备输入张量\n",
    "def prepare_input_from_tokens(token_ids, max_length=None):\n",
    "    \"\"\"从token ID列表准备输入\"\"\"\n",
    "    if not token_ids:\n",
    "        raise ValueError(\"提供的token ID列表为空\")\n",
    "    \n",
    "    # 转换为张量\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long)\n",
    "    \n",
    "    # 如果需要，进行截断\n",
    "    if max_length and input_ids.size(1) > max_length:\n",
    "        input_ids = input_ids[:, -max_length:]\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "# 准备输入\n",
    "input_ids = prepare_input_from_tokens(prompt_sequence, config.model.block_size)\n",
    "input_ids = input_ids.repeat(2,1)\n",
    "original_length = input_ids.size(1)\n",
    "\n",
    "print(f\"输入张量形状: {input_ids.shape}\")\n",
    "print(f\"原始序列长度: {original_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成配置: {'max_new_tokens': 4000, 'temperature': 0.6, 'do_sample': True, 'top_k': None, 'eos_token_id': 98}\n"
     ]
    }
   ],
   "source": [
    "# 配置生成参数\n",
    "generation_config = {\n",
    "    'max_new_tokens': max_new_tokens+2000,\n",
    "    'temperature': temperature*2,\n",
    "    'do_sample': do_sample,\n",
    "    'top_k': top_k,\n",
    "    'eos_token_id': token_mapping.get('end', None)\n",
    "}\n",
    "\n",
    "print(f\"生成配置: {generation_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始生成...\n",
      "输入长度: torch.Size([2, 983])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1fc2dc3a28448b9243cd40cf960bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成完成，耗时: 70.84秒\n",
      "生成的序列长度: 4571 tokens. torch.Size([2, 4571, 1])\n"
     ]
    }
   ],
   "source": [
    "# 执行生成\n",
    "def generate_with_model(model_engine, input_ids, generation_config):\n",
    "    \"\"\"使用模型生成序列\"\"\"\n",
    "    # 将输入移动到正确的设备\n",
    "    if hasattr(model_engine, 'local_rank') and model_engine.local_rank is not None:\n",
    "        device = f'cuda:{model_engine.local_rank}'\n",
    "    else:\n",
    "        # 回退：从模型参数获取设备\n",
    "        device = next(model_engine.module.parameters()).device\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    \n",
    "    # 将模型设置为评估模式\n",
    "    model_engine.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 生成序列\n",
    "        if hasattr(model_engine.module, 'generate'):\n",
    "            generated = model_engine.module.generate(\n",
    "                idx=input_ids,\n",
    "                max_new_tokens=generation_config.get('max_new_tokens', 50),\n",
    "                temperature=generation_config.get('temperature', 1.0),\n",
    "                do_sample=generation_config.get('do_sample', True),\n",
    "                top_k=generation_config.get('top_k', None),\n",
    "                end_token=generation_config.get('eos_token_id', None),\n",
    "                allow_unseg=True,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"模型没有generate方法\")\n",
    "    \n",
    "    return generated\n",
    "\n",
    "print(\"开始生成...\")\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"输入长度:\",input_ids.size())\n",
    "# 执行生成\n",
    "generated = generate_with_model(model_engine, input_ids, generation_config)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"生成完成，耗时: {generation_time:.2f}秒\")\n",
    "print(f\"生成的序列长度: {generated.size(1)} tokens. {generated.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始可视化tokens ===\n",
      "正在转换token ids到序列...\n",
      "解码得到的tokens数量: 4571\n",
      "前10个tokens: ['scene', 'incomplete', (0, 25, 3), (0, 26, 3), (0, 26, 4), (0, 26, 5), (0, 27, 5), (0, 28, 4), (0, 29, 3), (1, 24, 3)]\n",
      "解析成功，序列包含 3 个项目\n",
      "体素信息: dims=(80, 54, 34), bbox_min=[-0.3 -0.2  0. ], voxel_size=0.0075\n",
      "正在按类别提取点云...\n",
      "Scene SB 'incomplete': 979 个点\n",
      "Amodal SB 'unlabel': 1240 个点\n",
      "合并scene点云: 979 个点\n",
      "合并amodal点云: 1240 个点\n",
      "unseg 类别没有点云\n",
      "合并grasp点云: 1200 个点\n",
      "Warning: 无法从抓取点云重建mesh (tag=incomplete, index=26)，跳过保存。\n",
      "Warning: 无法从抓取点云重建mesh (tag=incomplete, index=118)，跳过保存。\n",
      "Warning: 无法从抓取点云重建mesh (tag=incomplete, index=281)，跳过保存。\n",
      "Warning: 无法从抓取点云重建mesh (tag=incomplete, index=313)，跳过保存。\n",
      "Warning: 无法从抓取点云重建mesh (tag=incomplete, index=337)，跳过保存。\n",
      "已生成并保存 395 个抓取mesh，目录: output/tokens_visual/0/grasp_meshes\n",
      "tokens可视化完成，结果保存在: output/tokens_visual/0\n",
      "=== 开始可视化tokens ===\n",
      "正在转换token ids到序列...\n",
      "解码得到的tokens数量: 4571\n",
      "前10个tokens: ['scene', 'incomplete', (0, 25, 3), (0, 26, 3), (0, 26, 4), (0, 26, 5), (0, 27, 5), (0, 28, 4), (0, 29, 3), (1, 24, 3)]\n",
      "解析成功，序列包含 3 个项目\n",
      "体素信息: dims=(80, 54, 34), bbox_min=[-0.3 -0.2  0. ], voxel_size=0.0075\n",
      "正在按类别提取点云...\n",
      "Scene SB 'incomplete': 979 个点\n",
      "Amodal SB 'unlabel': 1085 个点\n",
      "合并scene点云: 979 个点\n",
      "合并amodal点云: 1085 个点\n",
      "unseg 类别没有点云\n",
      "合并grasp点云: 1500 个点\n",
      "Warning: 无法从抓取点云重建mesh (tag=incomplete, index=89)，跳过保存。\n",
      "Warning: 无法从抓取点云重建mesh (tag=incomplete, index=219)，跳过保存。\n",
      "已生成并保存 498 个抓取mesh，目录: output/tokens_visual/1/grasp_meshes\n",
      "tokens可视化完成，结果保存在: output/tokens_visual/1\n",
      "=== 开始可视化tokens ===\n",
      "正在转换token ids到序列...\n",
      "解码得到的tokens数量: 3283\n",
      "前10个tokens: ['scene', 'incomplete', (0, 25, 3), (0, 26, 3), (0, 26, 4), (0, 26, 5), (0, 27, 5), (0, 28, 4), (0, 29, 3), (1, 24, 3)]\n",
      "解析成功，序列包含 2 个项目\n",
      "体素信息: dims=(80, 54, 34), bbox_min=[-0.3 -0.2  0. ], voxel_size=0.0075\n",
      "正在按类别提取点云...\n",
      "Scene SB 'incomplete': 979 个点\n",
      "Amodal SB 'unlabel': 2298 个点\n",
      "合并scene点云: 979 个点\n",
      "合并amodal点云: 2298 个点\n",
      "unseg 类别没有点云\n",
      "grasp 类别没有点云\n",
      "未生成任何抓取mesh。\n",
      "tokens可视化完成，结果保存在: output/tokens_visual/gt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('output/tokens_visual/gt')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from extract_sample_and_export import visualize_tokens\n",
    "\n",
    "all_scene  = generated.squeeze().detach().cpu()\n",
    "all_scene[:,-1] = token_mapping.get('end', None)\n",
    "for i in range(all_scene.size(0)):\n",
    "    visualize_tokens(all_scene[i], token_mapping, volume_dims =(img_h, img_w, img_d), bbox_min = np.array([-0.3, -0.2, 0]), voxel_size = 0.0075, output_dir = f'./output/tokens_visual/{i}')\n",
    "\n",
    "\n",
    "visualize_tokens(encode_sequence(gt_tokens, token_mapping), token_mapping, volume_dims =(img_h, img_w, img_d), bbox_min = np.array([-0.3, -0.2, 0]), voxel_size = 0.0075, output_dir = f'./output/tokens_visual/gt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(decode_sequence(generated[0].cpu().squeeze().numpy().tolist(),token_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(gt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整序列（原始 + prompt + 生成）\n",
    "full_sequence = input_ids[0].cpu().numpy().tolist() + decoded_tokens\n",
    "\n",
    "print(f\"完整序列长度: {len(full_sequence)}\")\n",
    "print(f\"完整序列的最后30个tokens: {full_sequence[-30:]}\")\n",
    "\n",
    "# 保存结果\n",
    "result = {\n",
    "    'original_sequence_length': len(token_sequence),\n",
    "    'prompt_tokens': additional_tokens,\n",
    "    'input_length': original_length,\n",
    "    'generated_tokens': decoded_tokens,\n",
    "    'generated_token_names': decoded_names,\n",
    "    'total_length': len(full_sequence),\n",
    "    'generation_time': generation_time,\n",
    "    'generation_config': generation_config,\n",
    "    'full_sequence': full_sequence\n",
    "}\n",
    "\n",
    "print(\"\\n=== 生成结果总结 ===\")\n",
    "print(f\"原始序列长度: {result['original_sequence_length']}\")\n",
    "print(f\"添加的prompt tokens: {result['prompt_tokens']}\")\n",
    "print(f\"输入长度: {result['input_length']}\")\n",
    "print(f\"生成的新tokens: {result['generated_tokens']}\")\n",
    "print(f\"生成时间: {result['generation_time']:.2f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 可选：可视化生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可选：使用可视化功能（如果extract_sample_and_export模块可用）\n",
    "try:\n",
    "    sys.path.append('../')\n",
    "    from extract_sample_and_export import visualize_tokens\n",
    "    \n",
    "    # 设置可视化参数（根据实际情况调整）\n",
    "    volume_dims = (img_h, img_w, img_d)\n",
    "    bbox_min = np.array([-0.3, -0.2, 0])  # 根据实际情况调整\n",
    "    voxel_size = 0.0075  # 根据实际情况调整\n",
    "    \n",
    "    # 可视化完整序列\n",
    "    output_dir = \"./output/generation_visual/notebook_result\"\n",
    "    \n",
    "    visualize_tokens(\n",
    "        tokens=full_sequence,\n",
    "        token_mapping=token_mapping,\n",
    "        volume_dims=volume_dims,\n",
    "        bbox_min=bbox_min,\n",
    "        voxel_size=voxel_size,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"可视化结果保存到: {output_dir}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"可视化模块不可用，跳过可视化步骤\")\n",
    "except Exception as e:\n",
    "    print(f\"可视化过程中出错: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 保存结果到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存结果到JSON文件\n",
    "output_file = \"generation_result.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"结果已保存到: {output_file}\")\n",
    "\n",
    "# 同时保存为.pth文件以便后续使用\n",
    "torch.save({\n",
    "    'generated_sequence': full_sequence,\n",
    "    'metadata': result\n",
    "}, \"generation_result.pth\")\n",
    "\n",
    "print(\"结果也已保存为generation_result.pth\")\n",
    "print(\"\\n生成完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
