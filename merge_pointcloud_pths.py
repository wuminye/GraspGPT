#!/usr/bin/env python3
"""Merge multiple .pth batches produced by generate_and_align_scene_pointcloud.py.

The script concatenates the sample dictionaries emitted by the alignment
pipeline into a single file that PrecomputedDataset can consume directly.
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Any, Dict, Iterable, List, Sequence

import torch


META_KEYS: Sequence[str] = ("volume_dims", "bbox_min", "bbox_max", "voxel_size")


def parse_args(argv: Sequence[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Merge multiple .pth files generated by generate_and_align_scene_pointcloud.py "
            "into a single batch compatible with graspGPT.model.precomputed_dataset.PrecomputedDataset."
        )
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default="output/real_data/train",
        help="Directory containing .pth files to merge (non-recursive).",
    )
    parser.add_argument(
        "--inputs",
        type=Path,
        nargs="*",
        default=(),
        help="Explicit list of .pth files to merge (in addition to --input-dir).",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default="output/real_data/train.pth",
        help="Destination .pth file for the merged data.",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Overwrite the output file if it already exists.",
    )
    parser.add_argument(
        "--sort-by-length",
        action="store_true",
        help="Sort merged samples by raw token length to match PrecomputedDataset expectations.",
    )

    args = parser.parse_args(argv)

    if args.input_dir is None and not args.inputs:
        parser.error("Provide at least one input source via --input-dir or --inputs.")

    return args


def collect_input_paths(input_dir: Path | None, explicit: Iterable[Path]) -> List[Path]:
    paths: List[Path] = []
    if input_dir is not None:
        if not input_dir.exists():
            raise FileNotFoundError(f"Input directory not found: {input_dir}")
        if not input_dir.is_dir():
            raise NotADirectoryError(f"--input-dir must be a directory: {input_dir}")
        paths.extend(sorted(input_dir.glob("*.pth")))

    for path in explicit:
        if not path.exists():
            raise FileNotFoundError(f"Explicit input file not found: {path}")
        if path.is_dir():
            raise IsADirectoryError(f"Explicit input path is a directory: {path}")
        if path.suffix != ".pth":
            raise ValueError(f"Explicit input must be a .pth file: {path}")
        paths.append(path)

    if not paths:
        raise FileNotFoundError("No .pth files found to merge.")

    # Deduplicate while preserving order from the combined list.
    seen = set()
    ordered_paths: List[Path] = []
    for path in paths:
        resolved = path.resolve()
        if resolved in seen:
            continue
        seen.add(resolved)
        ordered_paths.append(path)

    return ordered_paths


def load_batch(path: Path) -> List[Dict[str, Any]]:
    batch = torch.load(path, map_location="cpu", weights_only=False)
    if not isinstance(batch, (list, tuple)):
        raise TypeError(f"File {path} does not contain a list/tuple of samples.")

    normalized: List[Dict[str, Any]] = []
    for index, item in enumerate(batch):
        if not isinstance(item, dict):
            raise TypeError(f"Sample {index} in {path} is not a dictionary.")
        if "raw_tokens" not in item:
            raise KeyError(f"Sample {index} in {path} lacks 'raw_tokens'.")
        normalized.append(item)

    return normalized


def ensure_metadata_consistency(ref_meta: Dict[str, Any] | None, sample: Dict[str, Any], source: Path) -> Dict[str, Any]:
    current_meta = {key: sample.get(key) for key in META_KEYS}
    if ref_meta is None:
        return current_meta

    for key in META_KEYS:
        if key not in ref_meta:
            continue
        reference_value = ref_meta[key]
        current_value = current_meta.get(key)

        if key == "volume_dims":
            if tuple(current_value) != tuple(reference_value):
                raise ValueError(
                    f"Mismatch in '{key}' between files. Reference: {reference_value}, current ({source}): {current_value}"
                )
        else:
            ref_tensor = torch.as_tensor(reference_value, dtype=torch.float64)
            cur_tensor = torch.as_tensor(current_value, dtype=torch.float64)
            if not torch.allclose(cur_tensor, ref_tensor):
                raise ValueError(
                    f"Mismatch in '{key}' between files. Reference: {reference_value}, current ({source}): {current_value}"
                )

    return ref_meta


def merge_batches(paths: Sequence[Path]) -> List[Dict[str, Any]]:
    merged: List[Dict[str, Any]] = []
    reference_meta: Dict[str, Any] | None = None

    for path in paths:
        batch = load_batch(path)
        if not batch:
            continue

        for sample in batch:
            reference_meta = ensure_metadata_consistency(reference_meta, sample, path)
            merged.append(sample)

    if not merged:
        raise ValueError("All input files were empty.")

    return merged


def main(argv: Sequence[str]) -> int:
    args = parse_args(argv)

    if args.output.exists() and not args.force:
        print(f"Error: output file {args.output} already exists. Use --force to overwrite.", file=sys.stderr)
        return 2

    try:
        input_paths = collect_input_paths(args.input_dir, args.inputs)
    except Exception as exc:  # pylint: disable=broad-except
        print(f"Error while collecting inputs: {exc}", file=sys.stderr)
        return 1

    try:
        merged_samples = merge_batches(input_paths)
    except Exception as exc:  # pylint: disable=broad-except
        print(f"Error while merging: {exc}", file=sys.stderr)
        return 1

    if args.sort_by_length:
        merged_samples.sort(key=lambda sample: len(sample.get("raw_tokens", ())))

    args.output.parent.mkdir(parents=True, exist_ok=True)
    torch.save(merged_samples, args.output)

    print(
        f"Merged {len(merged_samples)} samples from {len(input_paths)} files into {args.output}"
    )
    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
