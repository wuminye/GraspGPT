"""
PyTorch Dataset for GraspGPT training
Loads voxel data from all_voxel_data.pth generated by obj_to_pointcloud.py
"""

import torch
from torch.utils.data import Dataset
from pathlib import Path
from typing import List, Tuple, Optional, Callable
import numpy as np
import random
try:
    from .token_manager import get_token_manager
    from .parser_and_serializer import Serializer, Seq, SB, CB
except ImportError:
    try:
        from token_manager import get_token_manager
        from parser_and_serializer import Serializer, Seq, SB, CB
    except ImportError:
        from minGPT.token_manager import get_token_manager
        from minGPT.parser_and_serializer import Serializer, Seq, SB, CB

class VoxelDataset(Dataset):
    """
    Dataset for loading voxel data from obj_to_pointcloud.py output
    
    Data format:
    - Each sample is a list of (color, coordinates_tensor) tuples, representing the shape of a 3D object
    - color: numpy.int64 value (0-255), representing the id the of object
    - coordinates_tensor: torch.Tensor of shape [N, 3] with int32 coordinates, where N is the number of voxels for that object
    
    The dataset converts each sample to token sequences using a tokenizer function.
    """
    
    def __init__(self, 
                 data_path: str, 
                 tokenizer_fn: Callable = None,
                 max_sequence_length: int = 1024,
                 weights_only: bool = False):
        """
        Initialize the VoxelDataset
        
        Args:
            data_path (str): Path to the all_voxel_data.pth file
            tokenizer_fn (Callable, optional): Function to convert voxel data to token sequences
                                             If None, will create real grammar-based tokenizer
            max_sequence_length (int): Maximum sequence length for padding/truncation
            weights_only (bool): Whether to load with weights_only=True (safer but may fail)
        """
        self.data_path = Path(data_path)
        self.max_sequence_length = max_sequence_length
        
        # Load the data
        try:
            if weights_only:
                raw_data = torch.load(self.data_path, weights_only=True)
            else:
                raw_data = torch.load(self.data_path, weights_only=False)
        except Exception as e:
            if not weights_only:
                raise e
            print(f"Warning: weights_only=True failed, trying weights_only=False: {e}")
            raw_data = torch.load(self.data_path, weights_only=False)
        
        # Extract data and metadata from new pth format
        if isinstance(raw_data, dict):
            # New format with metadata
            self.voxel_size = raw_data.get('voxel_size')
            self.bbox_min = raw_data.get('bbox_min')
            self.bbox_max = raw_data.get('bbox_max')
            self.volume_dims = raw_data.get('volume_dims')
            self.data = raw_data.get('data_lists', [])
            
            print(f"Loaded {len(self.data)} samples from {self.data_path}")
            print(f"Volume dimensions: {self.volume_dims}")
            print(f"Voxel size: {self.voxel_size}")
        else:
            # Legacy format - just the data lists
            self.data = raw_data
            self.volume_dims = [128, 128, 128]  # Default dimensions
            self.voxel_size = None
            self.bbox_min = None
            self.bbox_max = None
            print(f"Loaded {len(self.data)} samples from {self.data_path} (legacy format)")
            print(f"Using default volume dimensions: {self.volume_dims}")


        #Set up token manager and mapping
        
        # Get token manager instance
        token_manager = get_token_manager()
        
       
        
        # Set up tokenizer - create internally if not provided
        if tokenizer_fn is None:
            print("Creating real grammar-based tokenizer...")
            img_h, img_w, img_d = self.volume_dims
            # Generate mapping
            self.token_mapping = token_manager.generate_mapping(img_h, img_w, img_d)
            self.tokenizer_fn = self._create_tokenizer(self.token_mapping)
        else:
            self.tokenizer_fn = tokenizer_fn
        
        # Validate data format
        if len(self.data) > 0:
            sample = self.data[0]
            if not isinstance(sample, list):
                raise ValueError(f"Expected each sample to be a list, got {type(sample)}")
            if len(sample) > 0 and not isinstance(sample[0], tuple):
                raise ValueError(f"Expected each item in sample to be a tuple, got {type(sample[0])}")
    
    def _create_tokenizer(self, token_mapping: dict) -> Callable:
        """
        Create a real tokenizer based on parser_and_serializer.py grammar definitions.
        
        Args:
            token_mapping: Dictionary mapping tokens to token IDs
            
        Returns:
            function: Tokenizer function
        """

        
        
        
        def tokenizer_fn(voxel_data):
            """
            Convert entire voxel_data sample to a sequence of token IDs using grammar definitions
            
            Args:
                voxel_data: List of (color, coordinates) tuples representing one sample
                
            Returns:
                list: List of token IDs following grammar structure
            """
            # Step 1: Collect all SBs from the sample
            sbs = []
            
            # Randomly shuffle the voxel_data order
            random.shuffle(voxel_data)
            
            for color, coordinates in voxel_data:
                # Map color to shape tag - use object tags based on color value
                if 0 <= color <= 87:
                    shape_tag = f'object{color:02d}'  # object00 to object87
                else:
                    shape_tag = 'unknow'  # fallback for out-of-range colors
                
                # Create coordinate blocks (CB) from coordinates
                cbs = []
                coords_list = coordinates.tolist()
                
                for i, coord in enumerate(coords_list):
                    x, y, z = coord
                    # Ensure coordinates are integers and within bounds
                    x, y, z = int(x), int(y), int(z)
                    coord_tuple = (x, y, z)
                    
                    # Create CB with coordinate
                    cb = CB(coord=coord_tuple)
                    cbs.append(cb)
                
                # Create SB (Segment Block) with the shape tag and coordinate blocks
                sb = SB(tag=shape_tag, cbs=cbs)
                sbs.append(sb)
            
            # Step 2: Create sequence with all SBs from this sample
            seq = Seq(items=sbs)
            
            # Step 3: Serialize AST to flat token list
            flat_tokens = Serializer.serialize(seq)
            
            # Step 4: Convert flat tokens to token IDs using token_manager mapping
            token_ids = []
            
            for token in flat_tokens:
                if token in token_mapping:
                    token_ids.append(token_mapping[token])
                else:
                    # Handle unknown tokens - could add to mapping or use special token
                    print(f"Warning: Unknown token '{token}' not in mapping")
                    # For robustness, you might want to add a special <UNK> token
                    # For now, we'll skip unknown tokens
                    continue
            
            return token_ids
        
        return tokenizer_fn

    def __len__(self) -> int:
        """Return the number of samples in the dataset"""
        return len(self.data)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Get a single sample from the dataset
        
        Args:
            idx (int): Sample index
            
        Returns:
            tuple: (input_tokens, target_tokens, attention_mask)
                - input_tokens: torch.Tensor of shape [seq_len, num_features]
                - target_tokens: torch.Tensor of shape [seq_len, num_features] 
                - attention_mask: torch.Tensor of shape [seq_len] with 1s for valid tokens
        """
        # Get the raw voxel data for this sample
        voxel_data = self.data[idx]
        
        # Collect all SBs from this sample and create one SEQ
        token_sequence = self.tokenizer_fn(voxel_data)
        if isinstance(token_sequence, torch.Tensor):
            token_sequence = token_sequence.tolist()
        elif isinstance(token_sequence, (list, tuple)):
            token_sequence = list(token_sequence)
        else:
            token_sequence = [token_sequence] if token_sequence is not None else []
        
        # Convert to tensor
        if len(token_sequence) == 0:
            # Handle empty sequences
            token_sequence = [0]  # Use a default token
        
        # Ensure we have a 2D tensor structure for compatibility with model
        if not isinstance(token_sequence[0], (list, tuple)):
            # If tokens are 1D, expand to 2D with single feature dimension
            token_sequence = [[token] for token in token_sequence]
        
        tokens = torch.tensor(token_sequence, dtype=torch.long)
        
        # Handle sequence length
        seq_len = tokens.shape[0]
        num_features = tokens.shape[1] if tokens.dim() > 1 else 1

        
        if seq_len > self.max_sequence_length:
            # Truncate if too long
            tokens = tokens[:self.max_sequence_length]
            seq_len = self.max_sequence_length
        
        
        
        
        return tokens, self.max_sequence_length
    
    def get_vocab_size(self) -> int:
        """
        Get vocabulary size from token_manager
        """
        return len(self.token_mapping)
    
    def get_sample_info(self, idx: int) -> dict:
        """
        Get information about a specific sample for debugging
        
        Args:
            idx (int): Sample index
            
        Returns:
            dict: Information about the sample
        """
        if idx >= len(self.data):
            return {"error": f"Index {idx} out of range"}
        
        voxel_data = self.data[idx]
        
        info = {
            "sample_idx": idx,
            "num_color_groups": len(voxel_data),
            "total_voxels": sum(coords.shape[0] for _, coords in voxel_data),
            "colors": [int(color) for color, _ in voxel_data],
            "coordinate_shapes": [coords.shape for _, coords in voxel_data]
        }
        
        return info






# Example usage and testing functions
if __name__ == "__main__":
    # Example of how to use the dataset
    data_path = "../../output/pointclouds/all_voxel_data.pth"
    
    # Create dataset - tokenizer will be created internally
    print("=== Creating Dataset with Internal Grammar-based Tokenizer ===")
    dataset = VoxelDataset(
        data_path=data_path,
        max_sequence_length=3512
    )
    
    print(f"Dataset size: {len(dataset)}")
    print(f"Estimated vocab size: {dataset.get_vocab_size()}")
    
    # Test tokenizer directly with sample data
    print("\n=== Direct Tokenizer Testing ===")
    import torch
    
    # Create sample data - now tokenizer expects entire voxel_data list
    sample_voxel_data = [
        (5, torch.tensor([[10,20,10], [30,20,10]], dtype=torch.int32)),  # object05
        (10, torch.tensor([[10,10,10]], dtype=torch.int32))  # object10
    ]

    print(f"Input voxel_data: {len(sample_voxel_data)} color groups")
    for i, (color, coords) in enumerate(sample_voxel_data):
        print(f"  Group {i}: color={color}, coords shape={coords.shape}")
    
    # Test internal tokenizer
    tokens = dataset.tokenizer_fn(sample_voxel_data)
    print(f"Tokenizer output: {tokens}")
    print(f"Tokenizer output length: {len(tokens)}")
    
    # Test a few samples from dataset
    print("\n=== Dataset Sample Testing ===") 
    for i in range(min(3, len(dataset))):
        input_tokens, target_tokens, attention_mask = dataset[i]
        info = dataset.get_sample_info(i)
        
        print(f"\nSample {i}:")
        print(f"  Sample info: {info}")
        print(f"  Input tokens shape: {input_tokens.shape}")
        print(f"  Target tokens shape: {target_tokens.shape}")
        print(f"  Attention mask shape: {attention_mask.shape}")
        print(f"  First few input tokens: {input_tokens[:10].flatten().tolist()}")
        
    # Test tokenizer sequence reconstruction
    print("\n=== Testing Grammar Sequence Reconstruction ===")
    try:
        from .token_manager import get_token_manager
        from .parser_and_serializer import Parser, Seq, SB, CB
    except ImportError:
        try:
            from token_manager import get_token_manager
            from parser_and_serializer import Parser, Seq, SB, CB
        except ImportError:
            try:
                from minGPT.token_manager import get_token_manager
                from minGPT.parser_and_serializer import Parser, Seq, SB, CB
            except ImportError:
                print("Cannot import grammar modules for reconstruction test")
                exit()
    
    # Get token manager and create reverse mapping using dataset's volume dimensions
    token_manager = get_token_manager()
    img_h, img_w, img_d = dataset.volume_dims
    token_mapping = token_manager.generate_mapping(img_h, img_w, img_d)
    reverse_mapping = {v: k for k, v in token_mapping.items()}
    
    # Convert token IDs back to tokens and show grammar structure
    if tokens:
        reconstructed_tokens = [reverse_mapping.get(token_id, f"<UNK:{token_id}>") 
                              for token_id in tokens]
        print(f"Reconstructed tokens: {reconstructed_tokens}")
        
        # Try to parse the reconstructed sequence
        try:
            parser = Parser(reconstructed_tokens)
            parsed_seq = parser.parse()
            print(f"Parsed AST structure: {parsed_seq}")
        except Exception as e:
            print(f"Parsing failed: {e}")