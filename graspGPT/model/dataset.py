"""
PyTorch Dataset for GraspGPT training
Loads voxel data from all_voxel_data.pth generated by obj_to_pointcloud.py
"""

import torch
from torch.utils.data import Dataset
from pathlib import Path
from typing import List, Tuple, Optional, Callable, Dict
import numpy as np
import random
import glob
import os
import sys

# Add path to parent directories for imports
sys.path.append(str(Path(__file__).parent.parent.parent / "blender"))
from process_grasp import loadGraspLabels, process_grasp_data

try:
    from .token_manager import get_token_manager
    from .parser_and_serializer import Serializer, Seq, SB, CB, GRASP, GB
except ImportError:
    try:
        from token_manager import get_token_manager
        from parser_and_serializer import Serializer, Seq, SB, CB, GRASP, GB
    except ImportError:
        from minGPT.token_manager import get_token_manager
        from minGPT.parser_and_serializer import Serializer, Seq, SB, CB, GRASP, GB

class VoxelDataset(Dataset):
    """
    Dataset for loading voxel data from obj_to_pointcloud.py output
    
    Data format:
    - Each sample is a list of (color, coordinates_tensor) tuples, representing the shape of a 3D object
    - color: numpy.int64 value (0-255), representing the id the of object
    - coordinates_tensor: torch.Tensor of shape [N, 3] with int32 coordinates, where N is the number of voxels for that object
    
    The dataset converts each sample to token sequences using a tokenizer function.
    """
    
    def __init__(self, 
                 data_path: str, 
                 tokenizer_fn: Callable = None,
                 max_sequence_length: int = 1024,
                 weights_only: bool = False,
                 grasp_data_path: str = "../data",
                 max_grasps_per_object: Optional[int] = 4):
        """
        Initialize the VoxelDataset
        
        Args:
            data_path (str): Path to directory containing .pth files or single .pth file
            tokenizer_fn (Callable, optional): Function to convert voxel data to token sequences
                                             If None, will create real grammar-based tokenizer
            max_sequence_length (int): Maximum sequence length for padding/truncation
            weights_only (bool): Whether to load with weights_only=True (safer but may fail)
            grasp_data_path (str): Path to grasp data directory containing grasp_label folder
            max_grasps_per_object (int, optional): Maximum number of grasps to keep per object.
                                                 If None, keeps all valid grasps. If specified and
                                                 the number of valid grasps exceeds this limit,
                                                 randomly samples this many grasps.
        """
        self.data_path = Path(data_path)
        self.max_sequence_length = max_sequence_length
        self.grasp_data_path = grasp_data_path
        self.max_grasps_per_object = max_grasps_per_object
        
        # Check if data_path is a directory or a single file
        if self.data_path.is_dir():
            # Load all .pth files from directory
            self.data, self.voxel_size, self.bbox_min, self.bbox_max, self.volume_dims, self.transforms = self._load_from_directory(weights_only)
        else:
            # Load single file (backward compatibility)
            self.data, self.voxel_size, self.bbox_min, self.bbox_max, self.volume_dims, self.transforms = self._load_single_file(self.data_path, weights_only)
        
       
        # 预加载所有grasp数据到内存
        self._preload_all_grasp_data()

   

        #Set up token manager and mapping
        
        # Get token manager instance
        token_manager = get_token_manager()
        
       
        
        # Set up tokenizer - create internally if not provided
        if tokenizer_fn is None:
            print("Creating real grammar-based tokenizer...")
            img_h, img_w, img_d = self.volume_dims
            # Generate mapping
            self.token_mapping = token_manager.generate_mapping(img_h, img_w, img_d)
            self.tokenizer_fn = self._create_tokenizer(self.token_mapping)
        else:
            self.tokenizer_fn = tokenizer_fn
        
        # Validate data format
        if len(self.data) > 0:
            sample = self.data[0]
            if not isinstance(sample, list):
                raise ValueError(f"Expected each sample to be a list, got {type(sample)}")
            if len(sample) > 0 and not isinstance(sample[0], tuple):
                raise ValueError(f"Expected each item in sample to be a tuple, got {type(sample[0])}")
    
    def _create_tokenizer(self, token_mapping: dict) -> Callable:
        """
        Create a real tokenizer based on parser_and_serializer.py grammar definitions.
        
        Args:
            token_mapping: Dictionary mapping tokens to token IDs
            
        Returns:
            function: Tokenizer function
        """

        
        
        
        def tokenizer_fn(voxel_data):
            """
            Convert entire voxel_data sample to a sequence of token IDs using grammar definitions
            
            Args:
                voxel_data: List of (color, coordinates) tuples representing one sample
                
            Returns:
                list: List of token IDs following grammar structure
            """
            # Step 1: Collect all SBs from the sample
            sbs = []
            
            # Randomly shuffle the voxel_data order
            random.shuffle(voxel_data)
            
            for color, coordinates in voxel_data:
                # Map color to shape tag - use object tags based on color value
                if 0 <= color <= 87:
                    shape_tag = f'object{color:02d}'  # object00 to object87
                else:
                    shape_tag = 'unknow'  # fallback for out-of-range colors
                
                # Create coordinate blocks (CB) from coordinates
                cbs = []
                coords_list = coordinates.tolist()
                
                for i, coord in enumerate(coords_list):
                    x, y, z = coord
                    # Ensure coordinates are integers and within bounds
                    x, y, z = int(x), int(y), int(z)
                    coord_tuple = (x, y, z)
                    
                    # Create CB with coordinate
                    cb = CB(coord=coord_tuple)
                    cbs.append(cb)
                
                # Create SB (Segment Block) with the shape tag and coordinate blocks
                sb = SB(tag=shape_tag, cbs=cbs)
                sbs.append(sb)
            
            # Step 2: Create sequence with all SBs from this sample
            seq = Seq(items=sbs)
            
            # Step 3: Serialize AST to flat token list
            flat_tokens = Serializer.serialize(seq)
            
            # Step 4: Convert flat tokens to token IDs using token_manager mapping
            token_ids = []
            
            for token in flat_tokens:
                if token in token_mapping:
                    token_ids.append(token_mapping[token])
                else:
                    # Handle unknown tokens - could add to mapping or use special token
                    print(f"Warning: Unknown token '{token}' not in mapping")
                    # For robustness, you might want to add a special <UNK> token
                    # For now, we'll skip unknown tokens
                    continue
            
            return token_ids
        
        return tokenizer_fn

    def __len__(self) -> int:
        """Return the number of samples in the dataset"""
        return len(self.data)

    def prepare_data(self, idx: int):
        """
        Get a single sample from the dataset
        
        Args:
            idx (int): Sample index
            
        Returns:
            tuple: (tokens, max_sequence_length, scene_grasps)
                - tokens: torch.Tensor of token sequence
                - max_sequence_length: int, maximum sequence length
                - scene_grasps: Dict mapping object_id to transformed grasp arrays
        """
        # Get the raw voxel data for this sample
        voxel_data = self.data[idx]
        
        # 检测当前数据中出现的Object id
        detected_object_ids = set()
        for color, _ in voxel_data:
            if 0 <= color <= 87:  # 有效的object id范围
                detected_object_ids.add(color)
        
        # 获取对应的transforms信息
        scene_transforms = {}
        if idx < len(self.transforms) and isinstance(self.transforms[idx], dict):
            # 只保留检测到的object_id的transforms
            full_transforms = self.transforms[idx]
            for obj_id in detected_object_ids:
                if obj_id in full_transforms:
                    scene_transforms[obj_id] = full_transforms[obj_id]

        scene_grasps = None
        # 提取并变换grasp信息
        scene_grasps = self._extract_object_grasps(scene_transforms)
        
        
        # 过滤与场景occupancy碰撞的grasps
        scene_grasps = self._filter_colliding_grasps(scene_grasps, voxel_data, self.max_grasps_per_object)

        return voxel_data, scene_grasps
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, Dict[int, np.ndarray]]:
        """
        Get a single sample from the dataset
        
        Args:
            idx (int): Sample index
            
        Returns:
            tuple: (tokens, max_sequence_length, scene_grasps)
                - tokens: torch.Tensor of token sequence
                - max_sequence_length: int, maximum sequence length
                - scene_grasps: Dict mapping object_id to transformed grasp arrays
        """
        # Get the raw voxel data for this sample
        voxel_data = self.data[idx]
        
        # 检测当前数据中出现的Object id
        detected_object_ids = set()
        for color, _ in voxel_data:
            if 0 <= color <= 87:  # 有效的object id范围
                detected_object_ids.add(color)
        
        # 获取对应的transforms信息
        scene_transforms = {}
        if idx < len(self.transforms) and isinstance(self.transforms[idx], dict):
            # 只保留检测到的object_id的transforms
            full_transforms = self.transforms[idx]
            for obj_id in detected_object_ids:
                if obj_id in full_transforms:
                    scene_transforms[obj_id] = full_transforms[obj_id]

        scene_grasps = None
        # 提取并变换grasp信息
        scene_grasps = self._extract_object_grasps(scene_transforms)
        
        
        # 过滤与场景occupancy碰撞的grasps
        scene_grasps = self._filter_colliding_grasps(scene_grasps, voxel_data, self.max_grasps_per_object)


        
        grasp_token_ids = self.convert_scene_grasps_to_tokens(scene_grasps)

        
        
        # 处理token序列（保持原有逻辑）
        token_sequence = self.tokenizer_fn(voxel_data)
        if isinstance(token_sequence, torch.Tensor):
            token_sequence = token_sequence.tolist()
        elif isinstance(token_sequence, (list, tuple)):
            token_sequence = list(token_sequence)
        else:
            token_sequence = [token_sequence] if token_sequence is not None else []
        
        # Convert to tensor
        if len(token_sequence) == 0:
            # Handle empty sequences
            token_sequence = [0]  # Use a default token
        
        # Ensure we have a 2D tensor structure for compatibility with model
        if not isinstance(token_sequence[0], (list, tuple)):
            # If tokens are 1D, expand to 2D with single feature dimension
            token_sequence = [[token] for token in token_sequence]

        
        tokens = torch.tensor(token_sequence, dtype=torch.long)
        grasp_token = torch.tensor(grasp_token_ids, dtype=torch.long).unsqueeze(1)  # Shape [num_grasps, 1]

        tokens = torch.cat([tokens[:-1], grasp_token], dim=0)  # Concatenate along sequence dimension
        
        # Handle sequence length
        seq_len = tokens.shape[0]
        num_features = tokens.shape[1] if tokens.dim() > 1 else 1

        
        if seq_len > self.max_sequence_length:
            # Truncate if too long
            tokens = tokens[:self.max_sequence_length]
            seq_len = self.max_sequence_length
        
        return tokens, self.max_sequence_length, scene_grasps
    
    def get_vocab_size(self) -> int:
        """
        Get vocabulary size from token_manager
        """
        return len(self.token_mapping)
    
    def get_sample_info(self, idx: int) -> dict:
        """
        Get information about a specific sample for debugging
        
        Args:
            idx (int): Sample index
            
        Returns:
            dict: Information about the sample
        """
        if idx >= len(self.data):
            return {"error": f"Index {idx} out of range"}
        
        voxel_data = self.data[idx]
        
        info = {
            "sample_idx": idx,
            "num_color_groups": len(voxel_data),
            "total_voxels": sum(coords.shape[0] for _, coords in voxel_data),
            "colors": [int(color) for color, _ in voxel_data],
            "coordinate_shapes": [coords.shape for _, coords in voxel_data]
        }
        
        return info
    
    def _load_single_file(self, file_path: Path, weights_only: bool) -> Tuple[List, any, any, any, List, List]:
        """
        Load data from a single .pth file
        
        Returns:
            Tuple of (data_lists, voxel_size, bbox_min, bbox_max, volume_dims, transforms)
        """
        try:
            if weights_only:
                raw_data = torch.load(file_path, weights_only=True)
            else:
                raw_data = torch.load(file_path, weights_only=False)
        except Exception as e:
            if not weights_only:
                raise e
            print(f"Warning: weights_only=True failed, trying weights_only=False: {e}")
            raw_data = torch.load(file_path, weights_only=False)
        
        # Extract data and metadata from pth format
        if isinstance(raw_data, dict):
            # New format with metadata
            voxel_size = raw_data.get('voxel_size')
            bbox_min = raw_data.get('bbox_min')
            bbox_max = raw_data.get('bbox_max')
            volume_dims = raw_data.get('volume_dims')
            data_lists = raw_data.get('data_lists', [])
            transforms = raw_data.get('transforms', [])
            
            print(f"Loaded {len(data_lists)} samples from {file_path}")
            print(f"Volume dimensions: {volume_dims}")
            print(f"Voxel size: {voxel_size}")
            if transforms:
                print(f"Loaded {len(transforms)} transform sets")
        else:
            # Legacy format - just the data lists
            data_lists = raw_data
            volume_dims = [128, 128, 128]  # Default dimensions
            voxel_size = None
            bbox_min = None
            bbox_max = None
            transforms = []
            print(f"Loaded {len(data_lists)} samples from {file_path} (legacy format)")
            print(f"Using default volume dimensions: {volume_dims}")
        
        return data_lists, voxel_size, bbox_min, bbox_max, volume_dims, transforms
    
    def _load_from_directory(self, weights_only: bool) -> Tuple[List, any, any, any, List, List]:
        """
        Load and concatenate data from all .pth files in directory
        
        Returns:
            Tuple of (concatenated_data_lists, voxel_size, bbox_min, bbox_max, volume_dims, concatenated_transforms)
        """
        # Find all .pth files in the directory
        pth_files = list(self.data_path.glob('*.pth'))
        if not pth_files:
            raise ValueError(f"No .pth files found in directory {self.data_path}")
        
        print(f"Found {len(pth_files)} .pth files in {self.data_path}")
        
        all_data_lists = []
        all_transforms = []
        metadata_list = []
        
        for file_path in sorted(pth_files):  # Sort for consistent ordering
            print(f"Loading {file_path.name}...")
            data_lists, voxel_size, bbox_min, bbox_max, volume_dims, transforms = self._load_single_file(file_path, weights_only)
            
            # Store metadata for validation
            metadata = {
                'file': file_path.name,
                'voxel_size': voxel_size,
                'bbox_min': bbox_min,
                'bbox_max': bbox_max,
                'volume_dims': volume_dims
            }
            metadata_list.append(metadata)
            
            # Add data to combined list
            all_data_lists.extend(data_lists)
            all_transforms.extend(transforms)
        
        # Validate metadata consistency
        self._validate_metadata_consistency(metadata_list)
        
        # Use metadata from first file (they should all be the same after validation)
        first_metadata = metadata_list[0]
        final_voxel_size = first_metadata['voxel_size']
        final_bbox_min = first_metadata['bbox_min']
        final_bbox_max = first_metadata['bbox_max']
        final_volume_dims = first_metadata['volume_dims']
        
        print(f"Successfully loaded and concatenated {len(all_data_lists)} total samples from {len(pth_files)} files")
        print(f"Final volume dimensions: {final_volume_dims}")
        print(f"Final voxel size: {final_voxel_size}")
        print(f"Final transforms count: {len(all_transforms)}")
        
        return all_data_lists, final_voxel_size, final_bbox_min, final_bbox_max, final_volume_dims, all_transforms
    
    def _validate_metadata_consistency(self, metadata_list: List[dict]):
        """
        Validate that all files have consistent metadata
        
        Args:
            metadata_list: List of metadata dictionaries from all loaded files
        
        Raises:
            ValueError: If metadata is inconsistent across files
        """
        if len(metadata_list) < 2:
            return  # No validation needed for single file
        
        reference_metadata = metadata_list[0]
        
        for i, metadata in enumerate(metadata_list[1:], 1):
            # Check volume_dims (handle numpy arrays)
            ref_dims = reference_metadata['volume_dims']
            cur_dims = metadata['volume_dims']
            
            # Convert to lists for safe comparison
            if hasattr(ref_dims, 'tolist'):
                ref_dims = ref_dims.tolist()
            if hasattr(cur_dims, 'tolist'):
                cur_dims = cur_dims.tolist()
            
            if ref_dims != cur_dims:
                raise ValueError(
                    f"Volume dimensions mismatch: {reference_metadata['file']} has "
                    f"{ref_dims} but {metadata['file']} has {cur_dims}"
                )
            
            # Check voxel_size (handle arrays and None values)
            ref_voxel = reference_metadata['voxel_size']
            cur_voxel = metadata['voxel_size']
            
            # Convert to comparable format
            if hasattr(ref_voxel, 'tolist'):
                ref_voxel = ref_voxel.tolist()
            if hasattr(cur_voxel, 'tolist'):
                cur_voxel = cur_voxel.tolist()
            
            if ref_voxel != cur_voxel:
                if ref_voxel is not None and cur_voxel is not None:
                    raise ValueError(
                        f"Voxel size mismatch: {reference_metadata['file']} has "
                        f"{ref_voxel} but {metadata['file']} has {cur_voxel}"
                    )
            
            # Check bbox_min (handle arrays and None values)
            ref_min = reference_metadata['bbox_min']
            cur_min = metadata['bbox_min']
            
            # Convert to comparable format
            if hasattr(ref_min, 'tolist'):
                ref_min = ref_min.tolist()
            if hasattr(cur_min, 'tolist'):
                cur_min = cur_min.tolist()
                
            if ref_min != cur_min:
                if ref_min is not None and cur_min is not None:
                    raise ValueError(
                        f"bbox_min mismatch: {reference_metadata['file']} has "
                        f"{ref_min} but {metadata['file']} has {cur_min}"
                    )
            
            # Check bbox_max (handle arrays and None values)
            ref_max = reference_metadata['bbox_max']
            cur_max = metadata['bbox_max']
            
            # Convert to comparable format
            if hasattr(ref_max, 'tolist'):
                ref_max = ref_max.tolist()
            if hasattr(cur_max, 'tolist'):
                cur_max = cur_max.tolist()
                
            if ref_max != cur_max:
                if ref_max is not None and cur_max is not None:
                    raise ValueError(
                        f"bbox_max mismatch: {reference_metadata['file']} has "
                        f"{ref_max} but {metadata['file']} has {cur_max}"
                    )
        
        print(f"Metadata validation passed: all {len(metadata_list)} files have consistent metadata")
    
    def _preload_all_grasp_data(self):
        """预加载所有88个物体的grasp数据到内存中"""
        # 直接加载所有88个物体的grasp数据
        all_object_ids = list(range(88))  # 0-87
        
        print(f"Preloading grasp data for all {len(all_object_ids)} objects...")
        
        # 预加载所有grasp数据
        try:
            all_grasp_labels = loadGraspLabels(objIds=all_object_ids, root=self.grasp_data_path)
            self.grasp_cache = all_grasp_labels
            print(f"Successfully preloaded grasp data for {len(all_grasp_labels)} objects")
        except Exception as e:
            print(f"Warning: Failed to preload grasp data: {e}")
            self.grasp_cache = {}
    
    def get_coordinate_transform(self, method=0):
        """Get coordinate transformation matrix (from reconstruct_scene_from_xml.py)"""
        # X和Y都取负
        return np.array([
            [-1, 0,  0,  0],
            [0, -1,  0,  0], 
            [0,  0,  1,  0],
            [0,  0,  0,  1]
        ])

    def _extract_object_grasps(self, scene_transforms: Dict[int, Dict]) -> Dict[int, np.ndarray]:
        """
        Extract and transform grasp data for objects in the current scene
        
        Args:
            scene_transforms: Dictionary mapping object_id to transform information
            
        Returns:
            Dictionary mapping object_id to transformed grasp arrays
        """
        scene_grasps = {}


        
        for obj_id, transform_info in scene_transforms.items():
            # 从预加载的数据中获取grasp数据
            if obj_id not in self.grasp_cache:
                continue
                
            grasp_data = self.grasp_cache[obj_id]
                
            # Process grasp data to get grasp group
            try:
                grasp_group = process_grasp_data({obj_id: grasp_data}, obj_id, 
                                               fric_coef_thresh=0.3, grasp_height=0.02)

                grasp_group = grasp_group.random_sample(numGrasp = 300).to_open3d_geometry_list()  
                


                if len(grasp_group) == 0:
                    continue
                    
                # Get transformation matrix for this object
                transform_matrix = transform_info.get('transform_matrix')
                if transform_matrix is None:
                    continue
                
                # Apply coordinate system correction (from reconstruct_scene_from_xml.py)
                coord_correction = self.get_coordinate_transform()
                final_transform = np.dot(coord_correction, transform_matrix)
                

          
                
                all_grasp_points = []
                for grasp_mesh in grasp_group:
                    # Transform grasps to scene coordinates
                    grasp_mesh.transform(final_transform)
                    # 将三角网格转换为点云（采样表面点）
                    if hasattr(grasp_mesh, 'vertices') and len(grasp_mesh.vertices) > 0:
                        grasp_pcd = grasp_mesh.sample_points_uniformly(number_of_points=100)
                        all_grasp_points.append(np.asarray(grasp_pcd.points))
                        
                all_grasp_points = np.stack(all_grasp_points) # SIze: [num_grasps, 100, 3]
                
                # Store transformed grasps
                scene_grasps[obj_id] = all_grasp_points
                
            except Exception as e:
                print(f"Warning: Failed to process grasps for object {obj_id}: {e}")
                continue
                
        return scene_grasps
    
    def _filter_colliding_grasps(self, scene_grasps: Dict[int, np.ndarray], voxel_data: List[Tuple], max_grasps_per_object: Optional[int] = None) -> Dict[int, np.ndarray]:
        """
        Filter out grasps that collide with scene occupancy
        
        Args:
            scene_grasps: Dictionary mapping object_id to grasp points [num_grasps, 100, 3]
            voxel_data: List of (color, coordinates) tuples representing scene occupancy
            max_grasps_per_object: Maximum number of grasps to keep per object. If None, keeps all valid grasps.
        
        Returns:
            Filtered scene_grasps with colliding grasps removed and optionally randomly sampled
        """
        if not scene_grasps or self.bbox_min is None or self.bbox_max is None:
            return scene_grasps
            
        # Convert scene occupancy to a set of voxel coordinates for fast lookup
        scene_occupancy = set()
        for color, coords in voxel_data:
            if hasattr(coords, 'tolist'):
                # coords is numpy array
                coords_list = coords.tolist()
                if isinstance(coords_list[0], (list, tuple)):
                    # coords is 2D array [[x1,y1,z1], [x2,y2,z2], ...]
                    scene_occupancy.update(tuple(coord) for coord in coords_list)
                else:
                    # coords is 1D array [x, y, z]
                    scene_occupancy.add(tuple(coords_list))
            elif isinstance(coords, (list, tuple)):
                if isinstance(coords[0], (list, tuple)):
                    # coords is nested list [[x1,y1,z1], [x2,y2,z2], ...]
                    scene_occupancy.update(tuple(coord) for coord in coords)
                else:
                    # coords is single coordinate [x, y, z]
                    scene_occupancy.add(tuple(coords))
            else:
                # Single coordinate as tuple or other format
                scene_occupancy.add(tuple(coords))
        
        # Convert bbox info to numpy arrays for computation
        bbox_min = np.array(self.bbox_min)
        bbox_max = np.array(self.bbox_max)
        volume_dims = np.array(self.volume_dims)
        
        filtered_grasps = {}
        total_original_grasps = 0
        total_filtered_grasps = 0
        
        for obj_id, grasp_points in scene_grasps.items():
            original_count = len(grasp_points)
            total_original_grasps += original_count
            # grasp_points shape: [num_grasps, 100, 3]
            valid_grasps = []
            
            for i, grasp in enumerate(grasp_points):
                # grasp shape: [100, 3] - 100 points representing one grasp
                
                # Convert 3D world coordinates to voxel indices
                # Normalize to [0, 1] range
                normalized_coords = (grasp - bbox_min) / (bbox_max - bbox_min)
                
                # Scale to voxel dimensions and convert to integers
                voxel_coords = (normalized_coords * (volume_dims - 1)).astype(int)
                
                # Check if coordinates are within valid range
                valid_range = (
                    (voxel_coords >= 0).all(axis=1) & 
                    (voxel_coords < volume_dims).all(axis=1)
                )
                
                # Skip grasp if any points are outside valid range
                if not valid_range.all():
                    continue
                
                # Convert grasp coordinates to set for efficient intersection
                grasp_coords_set = {tuple(coord) for coord in voxel_coords}
                
                # Check collision using set intersection
                if not grasp_coords_set.intersection(scene_occupancy):
                    # No collision found, keep the deduplicated voxel coordinates
                    deduplicated_coords = np.array(list(grasp_coords_set))
                    valid_grasps.append(deduplicated_coords)

            if valid_grasps:
                # Apply random sampling if max_grasps_per_object is specified
                if max_grasps_per_object is not None and len(valid_grasps) > max_grasps_per_object:
                    # Randomly sample max_grasps_per_object grasps
                    sampled_indices = random.sample(range(len(valid_grasps)), max_grasps_per_object)
                    valid_grasps = [valid_grasps[i] for i in sampled_indices]
                
                filtered_grasps[obj_id] = valid_grasps # 保持为list格式
                valid_count = len(valid_grasps)
                total_filtered_grasps += valid_count
                
                #if max_grasps_per_object is not None and len(valid_grasps) == max_grasps_per_object and original_count > max_grasps_per_object:
                #    print(f"Object {obj_id}: kept {valid_count}/{original_count} grasps ({valid_count/original_count*100:.1f}%) [randomly sampled from collision-free grasps]")
                #else:
                #    print(f"Object {obj_id}: kept {valid_count}/{original_count} grasps ({valid_count/original_count*100:.1f}%)")
            else:
                pass
                #print(f"Warning: All grasps for object {obj_id} were filtered out due to collisions (0/{original_count})")
        
        # 总体统计
        if total_original_grasps > 0:
            overall_percentage = total_filtered_grasps / total_original_grasps * 100
            #print(f"Overall: kept {total_filtered_grasps}/{total_original_grasps} grasps ({overall_percentage:.1f}%)")
        else:
            print("No grasps to filter")

        return filtered_grasps
    
    def _save_grasp_coordinates_ply(self, scene_grasps: Dict[int, List], save_path: str = None):
        """
        保存scene_grasps中的所有离散坐标为PLY文件
        
        Args:
            scene_grasps: Dictionary mapping object_id to list of deduplicated voxel coordinates
            save_path: 保存路径，如果为None则使用默认路径
        """
        
        if not scene_grasps:
            print("No grasps to save")
            return
            
        if save_path is None:
            save_path = "grasp_coordinates.ply"
        
        # 收集所有坐标点和颜色
        all_points = []
        all_colors = []
        
        # 为每个物体分配不同颜色
        colors_per_object = {
            0: [255, 0, 0],    # 红色
            1: [0, 255, 0],    # 绿色  
            2: [0, 0, 255],    # 蓝色
            3: [255, 255, 0],  # 黄色
            4: [255, 0, 255],  # 品红
            5: [0, 255, 255],  # 青色
        }
        
        for obj_id, grasp_list in scene_grasps.items():
            # 获取该物体的颜色
            color = colors_per_object.get(obj_id % len(colors_per_object), [128, 128, 128])
            
            for grasp_coords in grasp_list:
                # grasp_coords 是去重的离散坐标数组
                for coord in grasp_coords:
                    all_points.append(coord)
                    all_colors.append(color)
        
        if not all_points:
            print("No points to save")
            return
            
        # 写入PLY文件
        with open(save_path, 'w') as f:
            # PLY头部
            f.write("ply\n")
            f.write("format ascii 1.0\n")
            f.write(f"element vertex {len(all_points)}\n")
            f.write("property float x\n")
            f.write("property float y\n") 
            f.write("property float z\n")
            f.write("property uchar red\n")
            f.write("property uchar green\n")
            f.write("property uchar blue\n")
            f.write("end_header\n")
            
            # 写入顶点数据
            for point, color in zip(all_points, all_colors):
                f.write(f"{point[0]} {point[1]} {point[2]} {color[0]} {color[1]} {color[2]}\n")
        
        print(f"Saved {len(all_points)} grasp coordinate points to {save_path}")
        return save_path
    
    def convert_scene_grasps_to_sequence(self, scene_grasps: Dict[int, List]) -> Seq:
        """
        Convert scene_grasps data to GRASP sequence following grammar definition
        
        Grammar: GRASP → 'detectgrasp' GB*
                 GB    → 'grasp' TAG CB+
        
        Args:
            scene_grasps: Dictionary mapping object_id to list of grasp coordinates
                         Each grasp is represented as deduplicated voxel coordinates
        
        Returns:
            Seq: Sequence containing GRASP item following the grammar
        """
        if not scene_grasps:
            # Return empty GRASP sequence
            empty_grasp = GRASP(gbs=[])
            return Seq(items=[empty_grasp])
        
        # Create GB blocks for each object with grasps
        gb_blocks = []
        
        for obj_id, grasp_list in scene_grasps.items():
            if not grasp_list:
                continue
                
            # Map object_id to shape tag
            if 0 <= obj_id <= 87:
                shape_tag = f'object{obj_id:02d}'  # object00 to object87
            else:
                shape_tag = 'unknow'  # fallback for out-of-range object_ids
            
            # Create one GB for each grasp in the grasp_list
            for grasp_coords in grasp_list:
                # grasp_coords is an array of deduplicated voxel coordinates for one grasp
                cbs = []
                for coord in grasp_coords:
                    # Ensure coordinate is a tuple of 3 integers
                    if len(coord) == 3:
                        x, y, z = int(coord[0]), int(coord[1]), int(coord[2])
                        coord_tuple = (x, y, z)
                        cb = CB(coord=coord_tuple)
                        cbs.append(cb)
                
                # Create GB for this individual grasp if we have at least one coordinate
                if cbs:
                    # Sort CBs by coordinates for consistent ordering
                    cbs.sort(key=lambda cb: cb.coord)
                    gb = GB(tag=shape_tag, cbs=cbs)
                    gb_blocks.append(gb)
        
        # Randomly shuffle GB blocks for data diversity
        random.shuffle(gb_blocks)
        
        # Create GRASP item with all GB blocks
        grasp_item = GRASP(gbs=gb_blocks)
        
        # Return sequence with the GRASP item
        return Seq(items=[grasp_item])
    
    def convert_scene_grasps_to_tokens(self, scene_grasps: Dict[int, List]) -> List[int]:
        """
        Convert scene_grasps to token IDs using the GRASP grammar
        
        Args:
            scene_grasps: Dictionary mapping object_id to list of grasp coordinates
        
        Returns:
            List[int]: List of token IDs representing the GRASP sequence
        """
        # Convert to GRASP sequence
        grasp_seq = self.convert_scene_grasps_to_sequence(scene_grasps)
        
        # Serialize to flat tokens
        flat_tokens = Serializer.serialize(grasp_seq)
        
        # Convert tokens to token IDs using the mapping
        token_ids = []
        for token in flat_tokens:
            if token in self.token_mapping:
                token_ids.append(self.token_mapping[token])
            else:
                print(f"Warning: Unknown token '{token}' not in mapping")
                # Skip unknown tokens for robustness
                continue
        
        return token_ids






# Example usage and testing functions
if __name__ == "__main__":
    # Example of how to use the dataset
    data_path = "../../output/pointclouds/all_voxel_data.pth"
    
    # Create dataset - tokenizer will be created internally
    print("=== Creating Dataset with Internal Grammar-based Tokenizer ===")
    dataset = VoxelDataset(
        data_path=data_path,
        max_sequence_length=3512
    )
    
    print(f"Dataset size: {len(dataset)}")
    print(f"Estimated vocab size: {dataset.get_vocab_size()}")
    
    # Test tokenizer directly with sample data
    print("\n=== Direct Tokenizer Testing ===")
    import torch
    
    # Create sample data - now tokenizer expects entire voxel_data list
    sample_voxel_data = [
        (5, torch.tensor([[10,20,10], [30,20,10]], dtype=torch.int32)),  # object05
        (10, torch.tensor([[10,10,10]], dtype=torch.int32))  # object10
    ]

    print(f"Input voxel_data: {len(sample_voxel_data)} color groups")
    for i, (color, coords) in enumerate(sample_voxel_data):
        print(f"  Group {i}: color={color}, coords shape={coords.shape}")
    
    # Test internal tokenizer
    tokens = dataset.tokenizer_fn(sample_voxel_data)
    print(f"Tokenizer output: {tokens}")
    print(f"Tokenizer output length: {len(tokens)}")
    
    # Test a few samples from dataset
    print("\n=== Dataset Sample Testing ===") 
    for i in range(min(3, len(dataset))):
        input_tokens, target_tokens, attention_mask = dataset[i]
        info = dataset.get_sample_info(i)
        
        print(f"\nSample {i}:")
        print(f"  Sample info: {info}")
        print(f"  Input tokens shape: {input_tokens.shape}")
        print(f"  Target tokens shape: {target_tokens.shape}")
        print(f"  Attention mask shape: {attention_mask.shape}")
        print(f"  First few input tokens: {input_tokens[:10].flatten().tolist()}")
        
    # Test tokenizer sequence reconstruction
    print("\n=== Testing Grammar Sequence Reconstruction ===")
    try:
        from .token_manager import get_token_manager
        from .parser_and_serializer import Parser, Seq, SB, CB
    except ImportError:
        try:
            from token_manager import get_token_manager
            from parser_and_serializer import Parser, Seq, SB, CB
        except ImportError:
            try:
                from minGPT.token_manager import get_token_manager
                from minGPT.parser_and_serializer import Parser, Seq, SB, CB
            except ImportError:
                print("Cannot import grammar modules for reconstruction test")
                exit()
    
    # Get token manager and create reverse mapping using dataset's volume dimensions
    token_manager = get_token_manager()
    img_h, img_w, img_d = dataset.volume_dims
    token_mapping = token_manager.generate_mapping(img_h, img_w, img_d)
    reverse_mapping = {v: k for k, v in token_mapping.items()}
    
    # Convert token IDs back to tokens and show grammar structure
    if tokens:
        reconstructed_tokens = [reverse_mapping.get(token_id, f"<UNK:{token_id}>") 
                              for token_id in tokens]
        print(f"Reconstructed tokens: {reconstructed_tokens}")
        
        # Try to parse the reconstructed sequence
        try:
            parser = Parser(reconstructed_tokens)
            parsed_seq = parser.parse()
            print(f"Parsed AST structure: {parsed_seq}")
        except Exception as e:
            print(f"Parsing failed: {e}")
    
    # Test scene_grasps to GRASP sequence conversion
    print("\n=== Testing Scene Grasps to GRASP Sequence Conversion ===")
    
    # Create example scene_grasps data
    example_scene_grasps = {
        0: [  # object00
            np.array([[10, 20, 30], [11, 21, 31]]),  # First grasp
            np.array([[15, 25, 35]])                  # Second grasp
        ],
        1: [  # object01
            np.array([[40, 50, 60], [41, 51, 61], [42, 52, 62]])  # One grasp
        ]
    }
    
    try:
        # Test sequence conversion
        grasp_sequence = dataset.convert_scene_grasps_to_sequence(example_scene_grasps)
        print(f"GRASP sequence structure: {grasp_sequence}")
        
        # Test token conversion
        grasp_token_ids = dataset.convert_scene_grasps_to_tokens(example_scene_grasps)
        print(f"GRASP token IDs: {grasp_token_ids}")
        print(f"GRASP token count: {len(grasp_token_ids)}")
        
        # Test empty scene_grasps
        empty_grasps = {}
        empty_sequence = dataset.convert_scene_grasps_to_sequence(empty_grasps)
        print(f"Empty GRASP sequence: {empty_sequence}")
        
    except Exception as e:
        print(f"GRASP conversion test failed: {e}")
        import traceback
        traceback.print_exc()