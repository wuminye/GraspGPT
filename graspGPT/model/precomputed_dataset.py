"""
Streamlined PyTorch Dataset for precomputed GraspGPT data
Reads precomputed data generated by precompute_data.py
"""

import torch
from torch.utils.data import Dataset
from pathlib import Path
from typing import Tuple, Dict, List
import numpy as np
import glob
import random

try:
    from .token_manager import get_token_manager, decode_sequence, encode_sequence
    from .parser_and_serializer import Serializer, Seq, Scene, SB, CB, GRASP, GB, Parser, parse_with_cpp, AMODAL, UNSEG, MaskSerializer
    from .core import generate_amodal_sequence, generate_seg_sequence, maybe_drop_amodal_or_unseg, random_translation_argument, crop_z_coords
except ImportError:
    from token_manager import get_token_manager, decode_sequence, encode_sequence
    from parser_and_serializer import Serializer, Seq, Scene, SB, CB, GRASP, GB, Parser, parse_with_cpp, AMODAL, UNSEG, MaskSerializer
    from core import generate_amodal_sequence, generate_seg_sequence, maybe_drop_amodal_or_unseg, random_translation_argument, crop_z_coords



class PrecomputedDataset(Dataset):
    """
    Dataset for loading precomputed voxel and grasp data
    
    Reads data generated by precompute_data.py and produces the same output 
    format as VoxelDataset: (tokens, max_sequence_length, scene_grasps)
    """
    
    def __init__(self, 
                 data_path: str,
                 max_sequence_length: int = 1024,
                 tags = None):
        """
        Initialize the PrecomputedDataset
        
        Args:
            data_path (str): Path to directory containing precomputed .pth files
            max_sequence_length (int): Maximum sequence length for padding/truncation
        """
        self.data_path = Path(data_path)
        self.max_sequence_length = max_sequence_length

        self.tags = tags
        
        # Load all precomputed data files
        self.volume_dims = None
        self.data = self._load_precomputed_data()
        
        # Setup token manager and tokenizer
        token_manager = get_token_manager()
        if len(self.data) > 0:
            volume_dims = self.volume_dims
            self.token_mapping = token_manager.generate_mapping(volume_dims[0], volume_dims[1], volume_dims[2])
            self.tokenizer_fn = self._create_tokenizer(self.token_mapping)
        else:
            raise ValueError("No data loaded")
    
    def _load_precomputed_data(self) -> List[Dict]:
        """Load all precomputed data files from directory"""

        
        if self.data_path.is_file():
            # Single file
            pth_files = [self.data_path]
        else:
            # Directory with multiple files
            pth_files = list(self.data_path.glob('*.pth'))
        
        if not pth_files:
            raise ValueError(f"No .pth files found in {self.data_path}")
        
        # Collect all data first to analyze lengths
        all_raw_data = []
        for file_path in sorted(pth_files):
            print(f"Loading {file_path.name}...")
            data_batch = torch.load(file_path, weights_only=False)
            if self.volume_dims is None:
                self.volume_dims = data_batch[0]['volume_dims']
                self.bbox_min = data_batch[0]['bbox_min']
                self.bbox_max = data_batch[0]['bbox_max']
                self.voxel_size = data_batch[0]['voxel_size']

            for item in data_batch:
                all_raw_data.append(item['raw_tokens'])

        # Optimize memory layout for better cache performance
        # Sort by length to improve memory locality during sequential access
        all_raw_data.sort(key=len)
        
        # Convert to numpy arrays for better memory efficiency and cache performance
        all_data = []
        for tokens in all_raw_data:
            # Convert to numpy array with specific dtype to save memory
            if len(tokens) > 0:
                # Use int32 if all values fit, otherwise int64
                max_val = max(tokens) if tokens else 0
                dtype = np.int32 if max_val < 2**31 else np.int64
                all_data.append(np.array(tokens, dtype=dtype))
            else:
                all_data.append(np.array([], dtype=np.int32))
        
        print(f"Loaded {len(all_data)} precomputed samples from {len(pth_files)} files")
        print(f"Memory optimized: sorted by length, using numpy arrays with appropriate dtypes")
        return all_data
    
    def _create_tokenizer(self, token_mapping: dict):
        """
        Create a real tokenizer based on parser_and_serializer.py grammar definitions.
        
        Args:
            token_mapping: Dictionary mapping tokens to token IDs
            
        Returns:
            function: Tokenizer function
        """

        
        
        
        def tokenizer_fn(voxel_data):
            """
            Convert entire voxel_data sample to a sequence of token IDs using grammar definitions
            
            Args:
                voxel_data: List of (color, coordinates) tuples representing one sample
                
            Returns:
                list: List of token IDs following grammar structure
            """
            # Step 1: Collect all SBs from the sample
            sbs = []
            
            # Randomly shuffle the voxel_data order
            random.shuffle(voxel_data)
            
            for color, coordinates in voxel_data:
                # Map color to shape tag - use object tags based on color value
                if 0 <= color <= 87:
                    shape_tag = f'object{color:02d}'  # object00 to object87
                else:
                    shape_tag = 'unknow'  # fallback for out-of-range colors
                
                # Create coordinate blocks (CB) from coordinates
                cbs = []
                coords_list = coordinates.tolist()
                
                for i, coord in enumerate(coords_list):
                    x, y, z = coord
                    # Ensure coordinates are integers and within bounds
                    x, y, z = int(x), int(y), int(z)
                    coord_tuple = (x, y, z)
                    
                    # Create CB with coordinate
                    cb = CB(coord=coord_tuple)
                    cbs.append(cb)
                
                # Create SB (Segment Block) with the shape tag and coordinate blocks
                sb = SB(tag=shape_tag, cbs=cbs)
                sbs.append(sb)
            
            # Step 2: Wrap SBs into a Scene node and build sequence
            scene = Scene(sbs=sbs)
            seq = Seq(items=[scene])
            
            # Step 3: Serialize AST to flat token list
            flat_tokens = Serializer.serialize(seq)
            
            # Step 4: Convert flat tokens to token IDs using token_manager mapping
            token_ids = []
            
            for token in flat_tokens:
                if token in token_mapping:
                    token_ids.append(token_mapping[token])
                else:
                    # Handle unknown tokens - could add to mapping or use special token
                    print(f"Warning: Unknown token '{token}' not in mapping")
                    # For robustness, you might want to add a special <UNK> token
                    # For now, we'll skip unknown tokens
                    continue
            
            return token_ids
        
        return tokenizer_fn
    
    def __len__(self) -> int:
        """Return the number of samples in the dataset"""
        return len(self.data)

    
    def filter_grasp_tokens(self, tokens: List) -> List:
        """
        将tokens转成语法AST，对GRASP部分中的GB进行筛选，然后序列化回tokens
        
        筛选标准：对于当前tokens所表示的场景中多个SB，每个SB拥有至多5个与之相同TAG的GB
        
        Args:
            tokens: 原始token列表
            
        Returns:
            List: 筛选后的token列表
        """
        try:
            # Step 1: Parse tokens to AST
            parser = Parser(tokens)
            ast = parser.parse()
            #ast = parse_with_cpp(tokens)
            
            
            # Step 2: 收集所有SB的TAG
            sb_tags = set()
            sb_list = []

            other_items = []
            for item in ast.items:
                if isinstance(item, Scene):
                    sb_list.extend(item.sbs)
                elif isinstance(item, SB):
                    sb_list.append(item)
                elif isinstance(item, AMODAL) or isinstance(item, UNSEG):
                    other_items.append(item)


            for sb in sb_list:
                sb_tags.add(sb.tag)
            

            random.shuffle(sb_list)

            grasp_block = None
            
            # Step 3: 筛选GRASP中的GB
            for item in ast.items:
                if isinstance(item, GRASP):
                    # 统计每个TAG的GB数量
                    tag_count = {}
                    filtered_gbs = []
                    if len(sb_tags)>1:
                        for gb in item.gbs:
                            # 只保留与现有SB相同TAG的GB，且每个TAG最多5个
                            if gb.tag in sb_tags:
                                current_count = tag_count.get(gb.tag, 0)
                                if current_count < 150:
                                    filtered_gbs.append(gb)
                                    tag_count[gb.tag] = current_count + 1
                        
                    else:
                        filtered_gbs = item.gbs

                    random.shuffle(filtered_gbs)
                    if len(filtered_gbs)>700:
                        filtered_gbs = filtered_gbs[:700]
                    # 更新GRASP的GB列表
                    item.gbs = filtered_gbs
                    grasp_block = item
            
            items = []
            items.append(Scene(sbs=sb_list))
            items += other_items
            if grasp_block is not None:
                items.append(grasp_block)

            # Step 4: 序列化AST回tokens
            filtered_tokens = Serializer.serialize(Seq(items=items))
            return filtered_tokens, len(other_items)
            
        except Exception as e:
            print(f"Error in filter_grasp_tokens: {e}")
            # 出错时返回原始tokens
            return tokens


    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, Dict[int, np.ndarray]]:
        """
        Get a single sample from the dataset
        
        Returns same format as VoxelDataset:
            tuple: (tokens, max_sequence_length, scene_grasps)
        """
        tokens = self.data[idx]
        
        # Convert numpy array to list for processing
        if isinstance(tokens, np.ndarray):
            tokens = tokens.tolist()


        tokens = decode_sequence(tokens, self.token_mapping)


        ast = Parser(tokens).parse()
        real_data = True if len(ast.items)>2 else False




        if self.tags.translation_argument:
            tokens = random_translation_argument(tokens, self.volume_dims,scale=self.tags.translate_scale, real_data = real_data)


    

        tokens, num_others = self.filter_grasp_tokens(tokens)

        '''
        rng = random.random()
        if rng < 0.35: # 数据增强
            tokens = generate_seg_sequence(tokens)
        elif rng < 0.7:
            tokens = generate_amodal_sequence(tokens,self.volume_dims)
        '''

        rn_flag= True
        
        if self.tags.token_mode == "unseg_and_scene_grasp":
            rn_flag = random.random() <0.5
        elif self.tags.token_mode == "unseg_only":
            rn_flag =True
        elif self.tags.token_mode == "unseg_grasp":
            rn_flag = True

        if num_others==0:
            if rn_flag:
                tokens = generate_seg_sequence(tokens,self.volume_dims, self.tags)
                if self.tags.del_z>0:
                    tokens = crop_z_coords(tokens, self.volume_dims, self.tags.del_z)
        else: # process real data
            if self.tags.token_mode == "unseg_grasp":
                ast = Parser(tokens).parse()
                new_items = [item for item in ast.items if  isinstance(item, Scene) or isinstance(item, GRASP)]
                for gb in new_items[1].gbs:
                    gb.tag = 'unlabel'
                ast = Seq(items=new_items)
                tokens = Serializer.serialize(ast)
                if self.tags.del_z>0:
                    tokens = crop_z_coords(tokens, self.volume_dims, self.tags.del_z)
            else:
                if rn_flag:
                    ast = Parser(tokens).parse()
                    new_items = [item for item in ast.items if  isinstance(item, Scene) or isinstance(item, UNSEG)]
                    ast = Seq(items=new_items)
                    tokens = Serializer.serialize(ast)
                    if self.tags.del_z>0:
                        tokens = crop_z_coords(tokens, self.volume_dims, self.tags.del_z)
                else:
                    ast = Parser(tokens).parse()
                    new_items = [item for item in ast.items if  isinstance(item, UNSEG) or isinstance(item, GRASP)]
                    new_items[0] = Scene(sbs=new_items[0].sbs)
                    ast = Seq(items=new_items)
                    tokens = Serializer.serialize(ast)
            
                
                

        #    tokens = generate_amodal_sequence(tokens,self.volume_dims)




        ast = Parser(tokens).parse()
        mask = MaskSerializer.serialize(ast)
        mask = torch.tensor(mask, dtype=torch.bool)
        

        tokens = encode_sequence(tokens, self.token_mapping)

        #tokens = tokens[:-1]  # Remove the final EOS token for training
        if self.tags.token_mode in ["unseg_and_scene_grasp", "unseg_grasp"]:
            mask[-1] = False

        seq_len = len(tokens)

        tokens = torch.tensor(tokens, dtype=torch.long)

        if seq_len > self.max_sequence_length:
            # Truncate if too long
            tokens = tokens[:self.max_sequence_length]
            seq_len = self.max_sequence_length

        tokens = tokens.unsqueeze(-1)       

        return tokens, seq_len, mask

    def get_vocab_size(self) -> int:
        """Get vocabulary size from token_manager"""
        return len(self.token_mapping)

    
