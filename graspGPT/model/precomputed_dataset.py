"""
Streamlined PyTorch Dataset for precomputed GraspGPT data
Reads precomputed data generated by precompute_data.py
"""

import torch
from torch.utils.data import Dataset
from pathlib import Path
from typing import Tuple, Dict, List
import numpy as np
import glob
import random

try:
    from .token_manager import get_token_manager, decode_sequence, encode_sequence
    from .parser_and_serializer import Serializer, Seq, SB, CB, GRASP, GB, Parser
except ImportError:
    try:
        from token_manager import get_token_manager, decode_sequence, encode_sequence
        from parser_and_serializer import Serializer, Seq, SB, CB, GRASP, GB, Parser
    except ImportError:
        from minGPT.token_manager import get_token_manager, decode_sequence, encode_sequence
        from minGPT.parser_and_serializer import Serializer, Seq, SB, CB, GRASP, GB, Parser


class PrecomputedDataset(Dataset):
    """
    Dataset for loading precomputed voxel and grasp data
    
    Reads data generated by precompute_data.py and produces the same output 
    format as VoxelDataset: (tokens, max_sequence_length, scene_grasps)
    """
    
    def __init__(self, 
                 data_path: str,
                 max_sequence_length: int = 1024):
        """
        Initialize the PrecomputedDataset
        
        Args:
            data_path (str): Path to directory containing precomputed .pth files
            max_sequence_length (int): Maximum sequence length for padding/truncation
        """
        self.data_path = Path(data_path)
        self.max_sequence_length = max_sequence_length
        
        # Load all precomputed data files
        self.data = self._load_precomputed_data()
        
        # Setup token manager and tokenizer
        token_manager = get_token_manager()
        if len(self.data) > 0:
            volume_dims = self.data[0]['volume_dims']
            self.token_mapping = token_manager.generate_mapping(volume_dims[0], volume_dims[1], volume_dims[2])
            self.tokenizer_fn = self._create_tokenizer(self.token_mapping)

            self.volume_dims = volume_dims
            self.bbox_min = self.data[0]['bbox_min']
            self.bbox_max = self.data[0]['bbox_max']
            self.voxel_size = self.data[0]['voxel_size']
        else:
            raise ValueError("No data loaded")
    
    def _load_precomputed_data(self) -> List[Dict]:
        """Load all precomputed data files from directory"""
        if self.data_path.is_file():
            # Single file
            pth_files = [self.data_path]
        else:
            # Directory with multiple files
            pth_files = list(self.data_path.glob('*.pth'))
        
        if not pth_files:
            raise ValueError(f"No .pth files found in {self.data_path}")
        
        all_data = []
        for file_path in sorted(pth_files):
            print(f"Loading {file_path.name}...")
            data_batch = torch.load(file_path, weights_only=False)
            all_data.extend(data_batch)
        
        print(f"Loaded {len(all_data)} precomputed samples from {len(pth_files)} files")
        return all_data
    
    def _create_tokenizer(self, token_mapping: dict):
        """
        Create a real tokenizer based on parser_and_serializer.py grammar definitions.
        
        Args:
            token_mapping: Dictionary mapping tokens to token IDs
            
        Returns:
            function: Tokenizer function
        """

        
        
        
        def tokenizer_fn(voxel_data):
            """
            Convert entire voxel_data sample to a sequence of token IDs using grammar definitions
            
            Args:
                voxel_data: List of (color, coordinates) tuples representing one sample
                
            Returns:
                list: List of token IDs following grammar structure
            """
            # Step 1: Collect all SBs from the sample
            sbs = []
            
            # Randomly shuffle the voxel_data order
            random.shuffle(voxel_data)
            
            for color, coordinates in voxel_data:
                # Map color to shape tag - use object tags based on color value
                if 0 <= color <= 87:
                    shape_tag = f'object{color:02d}'  # object00 to object87
                else:
                    shape_tag = 'unknow'  # fallback for out-of-range colors
                
                # Create coordinate blocks (CB) from coordinates
                cbs = []
                coords_list = coordinates.tolist()
                
                for i, coord in enumerate(coords_list):
                    x, y, z = coord
                    # Ensure coordinates are integers and within bounds
                    x, y, z = int(x), int(y), int(z)
                    coord_tuple = (x, y, z)
                    
                    # Create CB with coordinate
                    cb = CB(coord=coord_tuple)
                    cbs.append(cb)
                
                # Create SB (Segment Block) with the shape tag and coordinate blocks
                sb = SB(tag=shape_tag, cbs=cbs)
                sbs.append(sb)
            
            # Step 2: Create sequence with all SBs from this sample
            seq = Seq(items=sbs)
            
            # Step 3: Serialize AST to flat token list
            flat_tokens = Serializer.serialize(seq)
            
            # Step 4: Convert flat tokens to token IDs using token_manager mapping
            token_ids = []
            
            for token in flat_tokens:
                if token in token_mapping:
                    token_ids.append(token_mapping[token])
                else:
                    # Handle unknown tokens - could add to mapping or use special token
                    print(f"Warning: Unknown token '{token}' not in mapping")
                    # For robustness, you might want to add a special <UNK> token
                    # For now, we'll skip unknown tokens
                    continue
            
            return token_ids
        
        return tokenizer_fn
    
    def __len__(self) -> int:
        """Return the number of samples in the dataset"""
        return len(self.data)

    
    def filter_grasp_tokens(self, tokens: List) -> List:
        """
        将tokens转成语法AST，对GRASP部分中的GB进行筛选，然后序列化回tokens
        
        筛选标准：对于当前tokens所表示的场景中多个SB，每个SB拥有至多5个与之相同TAG的GB
        
        Args:
            tokens: 原始token列表
            
        Returns:
            List: 筛选后的token列表
        """
        try:
            # Step 1: Parse tokens to AST
            parser = Parser(tokens)
            ast = parser.parse()
            
            
            # Step 2: 收集所有SB的TAG
            sb_tags = set()
            for item in ast.items:
                if isinstance(item, SB):
                    sb_tags.add(item.tag)
            

            
            
            # Step 3: 筛选GRASP中的GB
            for item in ast.items:
                if isinstance(item, GRASP):
                    # 统计每个TAG的GB数量
                    tag_count = {}
                    filtered_gbs = []
                    for gb in item.gbs:
                        # 只保留与现有SB相同TAG的GB，且每个TAG最多5个
                        if gb.tag in sb_tags:
                            current_count = tag_count.get(gb.tag, 0)
                            if current_count < 5:
                                filtered_gbs.append(gb)
                                tag_count[gb.tag] = current_count + 1

                    
                    # 更新GRASP的GB列表
                    item.gbs = filtered_gbs
            
            # Step 4: 序列化AST回tokens
            filtered_tokens = Serializer.serialize(ast)
            return filtered_tokens
            
        except Exception as e:
            print(f"Error in filter_grasp_tokens: {e}")
            # 出错时返回原始tokens
            return tokens

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, Dict[int, np.ndarray]]:
        """
        Get a single sample from the dataset
        
        Returns same format as VoxelDataset:
            tuple: (tokens, max_sequence_length, scene_grasps)
        """
        sample = self.data[idx]
        
        # Extract precomputed data
        tokens = sample['raw_tokens']
        tokens = decode_sequence(tokens, self.token_mapping)

        tokens = self.filter_grasp_tokens(tokens)

        tokens = encode_sequence(tokens, self.token_mapping)

        seq_len = len(tokens)


        tokens = torch.tensor(tokens)

        if seq_len > self.max_sequence_length:
            # Truncate if too long
            tokens = tokens[:self.max_sequence_length]
            seq_len = self.max_sequence_length
       

        return tokens, seq_len, None

    def get_vocab_size(self) -> int:
        """Get vocabulary size from token_manager"""
        return len(self.token_mapping)

    