"""
Streamlined PyTorch Dataset for precomputed GraspGPT data
Reads precomputed data generated by precompute_data.py
"""

import torch
from torch.utils.data import Dataset
from pathlib import Path
from typing import Tuple, Dict, List, Optional
import numpy as np
import random

try:
    from .token_manager import get_token_manager, decode_sequence, encode_sequence
    from .parser_and_serializer import Serializer, Seq, Scene, SB, CB, GRASP, GB, Parser, parse_with_cpp, AMODAL, UNSEG
    from .core import generate_amodal_sequence, generate_seg_sequence, maybe_drop_amodal_or_unseg, del_amodal_sequence
except ImportError:
    from token_manager import get_token_manager, decode_sequence, encode_sequence
    from parser_and_serializer import Serializer, Seq, Scene, SB, CB, GRASP, GB, Parser, parse_with_cpp, AMODAL, UNSEG
    from core import generate_amodal_sequence, generate_seg_sequence, maybe_drop_amodal_or_unseg, del_amodal_sequence








class PrecomputedDataset(Dataset):
    """
    Dataset for loading precomputed voxel and grasp data

    Reads data generated by precompute_data.py and returns (tokens, seq_len, loss_mask)
    where loss_mask pre-selects SB tokens tagged as incomplete.
    """
    
    _REAL_FILTER_MODES = {
        'no_amodal_unseg',  # keep only samples without AMODAL and UNSEG
        'amodal_only',      # keep samples that contain AMODAL but not UNSEG
        'unseg_only',       # keep samples that contain UNSEG but not AMODAL
        'allow_all',        # keep all real samples
        'skip',             # skip entire real files
    }

    def __init__(self, 
                 data_path: str,
                 max_sequence_length: int = 1024,
                 real_filter_mode: str = 'allow_all',
                 apply_del_amodal_sequence: bool = True):
        """
        Initialize the PrecomputedDataset

        Args:
            data_path (str): Path to directory containing precomputed .pth files
            max_sequence_length (int): Maximum sequence length for padding/truncation
            real_filter_mode (str): Filter rule for files whose name contains 'real'.
                Options: 'no_amodal_unseg', 'amodal_only', 'unseg_only', 'allow_all', 'skip'.
            apply_del_amodal_sequence (bool): Whether to strip AMODAL sequences via del_amodal_sequence.
        """
        self.data_path = Path(data_path)
        self.max_sequence_length = max_sequence_length
        self.real_filter_mode = self._validate_filter_mode(real_filter_mode)
        self.apply_del_amodal_sequence = bool(apply_del_amodal_sequence)

        # Internal cache for token mapping and real-data filter ids
        self._token_mapping_cache: Optional[Dict] = None

        # Load all precomputed data files
        self.volume_dims = None
        self.data = self._load_precomputed_data()
        
        # Setup token manager and tokenizer
        token_manager = get_token_manager()
        if len(self.data) > 0:
            volume_dims = self.volume_dims
            if self._token_mapping_cache is None:
                self._token_mapping_cache = token_manager.generate_mapping(volume_dims[0], volume_dims[1], volume_dims[2])
            self.token_mapping = self._token_mapping_cache
            #self.tokenizer_fn = self._create_tokenizer(self.token_mapping)
        else:
            raise ValueError("No data loaded")

    def _validate_filter_mode(self, mode: Optional[str]) -> str:
        normalized = (mode or 'allow_all').strip().lower()
        if normalized not in self._REAL_FILTER_MODES:
            valid = ', '.join(sorted(self._REAL_FILTER_MODES))
            raise ValueError(f"Unsupported real_filter_mode '{mode}'. Supported options: {valid}")
        return normalized

    def _build_incomplete_loss_mask(self, token_tensor: torch.Tensor) -> torch.Tensor:
        """Build a loss mask that keeps SCENE/SB tokens tagged as 'incomplete'."""
        flat_tokens = token_tensor.view(-1).tolist()
        mask = torch.ones(len(flat_tokens), dtype=torch.float32, device=token_tensor.device)

        mask[:flat_tokens.index(self.token_mapping['amodal'])] = 0.0

        return mask

    def _load_precomputed_data(self) -> List[Dict]:
        """Load all precomputed data files from directory"""


        if self.data_path.is_file():
            # Single file
            pth_files = [self.data_path]
        else:
            # Directory with multiple files
            pth_files = list(self.data_path.glob('*.pth'))
        
        if not pth_files:
            raise ValueError(f"No .pth files found in {self.data_path}")
        
        # Collect all data first to analyze lengths
        all_raw_data = []
        token_manager = get_token_manager()

        for file_path in sorted(pth_files):
            print(f"Loading {file_path.name}...")
            is_real_file = 'real' in file_path.name.lower()

            if is_real_file and self.real_filter_mode == 'skip':
                print(f"Skipping real data file {file_path.name} due to real_filter_mode='skip'")
                continue

            data_batch = torch.load(file_path, weights_only=False)
            if self.volume_dims is None:
                self.volume_dims = data_batch[0]['volume_dims']
                self.bbox_min = data_batch[0]['bbox_min']
                self.bbox_max = data_batch[0]['bbox_max']
                self.voxel_size = data_batch[0]['voxel_size']
                if self._token_mapping_cache is None:
                    self._token_mapping_cache = token_manager.generate_mapping(
                        self.volume_dims[0],
                        self.volume_dims[1],
                        self.volume_dims[2]
                    )

            if is_real_file and self.real_filter_mode != 'allow_all':
                filtered_count = 0
                kept_count = 0
                for item in data_batch:
                    tokens = item['raw_tokens']
                    if isinstance(tokens, torch.Tensor):
                        tokens = tokens.cpu().tolist()

                    if not self._should_keep_real_sample(tokens):
                        filtered_count += 1
                        continue

                    all_raw_data.append(tokens)
                    kept_count += 1

                total_samples = len(data_batch)
                if filtered_count:
                    print(
                        f"Filtered {filtered_count} / {total_samples} real samples from {file_path.name} "
                        f"using mode '{self.real_filter_mode}', kept {kept_count}"
                    )
                else:
                    print(
                        f"All {total_samples} real samples from {file_path.name} kept by mode '{self.real_filter_mode}'"
                    )
                continue

            for item in data_batch:
                all_raw_data.append(item['raw_tokens'])

        # Optimize memory layout for better cache performance
        # Sort by length to improve memory locality during sequential access
        all_raw_data.sort(key=len)
        
        # Convert to numpy arrays for better memory efficiency and cache performance
        all_data = []
        for tokens in all_raw_data:
            # Convert to numpy array with specific dtype to save memory
            if len(tokens) > 0:
                # Use int32 if all values fit, otherwise int64
                max_val = max(tokens) if tokens else 0
                dtype = np.int32 if max_val < 2**31 else np.int64
                all_data.append(np.array(tokens, dtype=dtype))
            else:
                all_data.append(np.array([], dtype=np.int32))
        
        print(f"Loaded {len(all_data)} precomputed samples from {len(pth_files)} files")
        print(f"Memory optimized: sorted by length, using numpy arrays with appropriate dtypes")
        return all_data



    def _should_keep_real_sample(self, encoded_tokens: List[int]) -> bool:
        if self.real_filter_mode == 'allow_all':
            return True

        if self._token_mapping_cache is None:
            raise RuntimeError("Token mapping must be initialized before filtering real samples")

        #decoded_tokens = decode_sequence(encoded_tokens, self._token_mapping_cache)
        has_amodal, has_unseg, parse_ok = self._detect_amodal_unseg(encoded_tokens)

        if not parse_ok:
            return True

        if self.real_filter_mode == 'no_amodal_unseg':
            return (not has_amodal) and (not has_unseg)
        if self.real_filter_mode == 'amodal_only':
            return has_amodal and (not has_unseg)
        if self.real_filter_mode == 'unseg_only':
            return has_unseg and (not has_amodal)

        # 'skip' is handled before reaching here.
        return True

    def _detect_amodal_unseg(self, decoded_tokens: List) -> Tuple[bool, bool, bool]:
        has_amodal = self._token_mapping_cache['amodal'] in decoded_tokens
        has_unseg = self._token_mapping_cache['segment'] in decoded_tokens
        '''
        try:
            ast = Parser(decoded_tokens).parse()
            for item in getattr(ast, 'items', []):
                if isinstance(item, AMODAL):
                    has_amodal = True
                elif isinstance(item, UNSEG):
                    has_unseg = True
        except Exception as exc:
            print(f"Warning: failed to parse tokens for real sample filtering ({exc}); keeping sample.")
            return has_amodal, has_unseg, False
        '''

        return has_amodal, has_unseg, True



    def __len__(self) -> int:
        """Return the number of samples in the dataset"""
        return len(self.data)


    def randomly_drop_scene_sb_coords(self, tokens: List, drop_ratio: float = 0.05) -> List:
        """随机删除SCENE中SB的部分坐标以做数据增强。

        Args:
            tokens: 已解码的token序列，不包含EOS。
            drop_ratio: 期望删除比例，默认5%。

        Returns:
            处理后的tokens。如果解析失败或无需修改则返回原tokens。
        """
        if not tokens:
            return tokens

        drop_ratio = max(0.0, min(drop_ratio, 1.0))
        if drop_ratio == 0.0:
            return tokens

        try:
            parser = Parser(tokens)
            ast = parser.parse()
        except Exception as exc:
            print(f"Error in randomly_drop_scene_sb_coords: {exc}")
            return tokens

        modified = False

        for item in getattr(ast, 'items', []):
            if not isinstance(item, Scene):
                continue

            for sb in item.sbs:
                original_cbs = sb.cbs
                if len(original_cbs) <= 1:
                    continue

                kept_cbs = [cb for cb in original_cbs if random.random() >= drop_ratio]

                if not kept_cbs:
                    kept_cbs = [random.choice(original_cbs)]

                ordered_cbs = sorted(
                    kept_cbs,
                    key=lambda cb: (cb.coord[0], cb.coord[1], cb.coord[2])
                )

                if len(ordered_cbs) != len(original_cbs) or ordered_cbs != original_cbs:
                    sb.cbs = ordered_cbs
                    modified = True

        if not modified:
            return tokens

        return Serializer.serialize(ast)


    def filter_grasp_tokens(self, tokens: List) -> List:
        """
        将tokens转成语法AST，对GRASP部分中的GB进行筛选，然后序列化回tokens

        筛选标准：对于当前tokens所表示的场景中多个SB，每个SB拥有至多5个与之相同TAG的GB
        
        Args:
            tokens: 原始token列表
            
        Returns:
            List: 筛选后的token列表
        """
        try:
            # Step 1: Parse tokens to AST
            parser = Parser(tokens)
            ast = parser.parse()
            #ast = parse_with_cpp(tokens)
            
            
            # Step 2: 收集所有SB的TAG
            sb_tags = set()
            sb_list = []

            other_items = []
            for item in ast.items:
                if isinstance(item, Scene):
                    sb_list.extend(item.sbs)
                elif isinstance(item, SB):
                    sb_list.append(item)
                elif isinstance(item, AMODAL) or isinstance(item, UNSEG):
                    other_items.append(item)


            for sb in sb_list:
                sb_tags.add(sb.tag)
            

            random.shuffle(sb_list)

            grasp_block = None
            
            # Step 3: 筛选GRASP中的GB
            for item in ast.items:
                if isinstance(item, GRASP):
                    # 统计每个TAG的GB数量
                    tag_count = {}
                    filtered_gbs = []
                    if len(sb_tags)>1:
                        for gb in item.gbs:
                            if gb.tag in sb_tags:
                                current_count = tag_count.get(gb.tag, 0)
                                if current_count < 100:
                                    filtered_gbs.append(gb)
                                    tag_count[gb.tag] = current_count + 1
                        
                    else:
                        filtered_gbs = item.gbs

                    random.shuffle(filtered_gbs)
                    if len(filtered_gbs)>700:
                        filtered_gbs = filtered_gbs[:700]
                    filtered_gbs.sort(
                        key=lambda gb: gb.cbs[0].coord if gb.cbs else (1 << 30, 1 << 30, 1 << 30)
                    )
                    # 更新GRASP的GB列表
                    item.gbs = filtered_gbs
                    grasp_block = item
            
            items = []
            items.append(Scene(sbs=sb_list))
            items += other_items
            if grasp_block is not None:
                items.append(grasp_block)

            # Step 4: 序列化AST回tokens
            filtered_tokens = Serializer.serialize(Seq(items=items))
            return filtered_tokens, len(other_items)
            
        except Exception as e:
            print(f"Error in filter_grasp_tokens: {e}")
            # 出错时返回原始tokens
            return tokens


    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, torch.Tensor]:
        """
        Get a single sample from the dataset

        Returns:
            tuple: (tokens, seq_len, loss_mask)
        """
        tokens = self.data[idx]
        
        # Convert numpy array to list for processing
        if isinstance(tokens, np.ndarray):
            tokens = tokens.tolist()

        tokens = decode_sequence(tokens, self.token_mapping)


    

        tokens, num_others = self.filter_grasp_tokens(tokens)

        

        '''
        rng = random.random()
        if rng < 0.35: # 数据增强
            tokens = generate_seg_sequence(tokens)
        elif rng < 0.7:
            tokens = generate_amodal_sequence(tokens,self.volume_dims)
        '''

        if num_others==0:
            if self.real_filter_mode == 'amodal_only':
                tokens = generate_amodal_sequence(tokens,self.volume_dims)
            if self.real_filter_mode == 'unseg_only':
                tokens = generate_seg_sequence(tokens)
            if self.real_filter_mode == 'allow_all':
                rng = random.random()
                if rng < 0.35: # 数据增强
                    tokens = generate_seg_sequence(tokens)
                elif rng < 0.7:
                    tokens = generate_amodal_sequence(tokens,self.volume_dims)
        else:
            tokens = self.randomly_drop_scene_sb_coords(tokens)

        if self.apply_del_amodal_sequence:
            tokens = del_amodal_sequence(tokens)

        tokens = encode_sequence(tokens, self.token_mapping)

        #tokens = tokens[:-1]  # Remove the final EOS token for training

        seq_len = len(tokens)

        tokens = torch.tensor(tokens, dtype=torch.long)

        if seq_len > self.max_sequence_length:
            # Truncate if too long
            tokens = tokens[:self.max_sequence_length]
            seq_len = self.max_sequence_length

        tokens = tokens.unsqueeze(-1)

        loss_mask = self._build_incomplete_loss_mask(tokens)

        return tokens, seq_len, loss_mask

    def get_vocab_size(self) -> int:
        """Get vocabulary size from token_manager"""
        return len(self.token_mapping)

    
