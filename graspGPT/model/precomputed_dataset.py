"""
Streamlined PyTorch Dataset for precomputed GraspGPT data
Reads precomputed data generated by precompute_data.py
"""

import torch
from torch.utils.data import Dataset
from pathlib import Path
from typing import Tuple, Dict, List
import numpy as np
import glob
import random

try:
    from .token_manager import get_token_manager
    from .parser_and_serializer import Serializer, Seq, SB, CB, GRASP, GB
except ImportError:
    try:
        from token_manager import get_token_manager
        from parser_and_serializer import Serializer, Seq, SB, CB, GRASP, GB
    except ImportError:
        from minGPT.token_manager import get_token_manager
        from minGPT.parser_and_serializer import Serializer, Seq, SB, CB, GRASP, GB


class PrecomputedDataset(Dataset):
    """
    Dataset for loading precomputed voxel and grasp data
    
    Reads data generated by precompute_data.py and produces the same output 
    format as VoxelDataset: (tokens, max_sequence_length, scene_grasps)
    """
    
    def __init__(self, 
                 data_path: str,
                 max_sequence_length: int = 1024):
        """
        Initialize the PrecomputedDataset
        
        Args:
            data_path (str): Path to directory containing precomputed .pth files
            max_sequence_length (int): Maximum sequence length for padding/truncation
        """
        self.data_path = Path(data_path)
        self.max_sequence_length = max_sequence_length
        
        # Load all precomputed data files
        self.data = self._load_precomputed_data()
        
        # Setup token manager and tokenizer
        token_manager = get_token_manager()
        if len(self.data) > 0:
            volume_dims = self.data[0]['volume_dims']
            self.token_mapping = token_manager.generate_mapping(volume_dims[0], volume_dims[1], volume_dims[2])
            self.tokenizer_fn = self._create_tokenizer(self.token_mapping)

            self.volume_dims = volume_dims
            self.bbox_min = self.data[0]['bbox_min']
            self.bbox_max = self.data[0]['bbox_max']
            self.voxel_size = self.data[0]['voxel_size']
        else:
            raise ValueError("No data loaded")
    
    def _load_precomputed_data(self) -> List[Dict]:
        """Load all precomputed data files from directory"""
        if self.data_path.is_file():
            # Single file
            pth_files = [self.data_path]
        else:
            # Directory with multiple files
            pth_files = list(self.data_path.glob('*.pth'))
        
        if not pth_files:
            raise ValueError(f"No .pth files found in {self.data_path}")
        
        all_data = []
        for file_path in sorted(pth_files):
            print(f"Loading {file_path.name}...")
            data_batch = torch.load(file_path, weights_only=False)
            all_data.extend(data_batch)
        
        print(f"Loaded {len(all_data)} precomputed samples from {len(pth_files)} files")
        return all_data
    
    def _create_tokenizer(self, token_mapping: dict):
        """Create tokenizer function for voxel data"""
        def tokenizer_fn(voxel_data):
            # Use the same tokenization logic as VoxelDataset
            sbs = []
            for color, coordinates in voxel_data:
                # Map color to shape tag
                if 0 <= color <= 87:
                    shape_tag = f'object{color:02d}'
                else:
                    shape_tag = 'unknow'
                
                # Create coordinate blocks
                cbs = []
                coords_list = coordinates.tolist() if hasattr(coordinates, 'tolist') else coordinates
                
                for coord in coords_list:
                    x, y, z = int(coord[0]), int(coord[1]), int(coord[2])
                    cb = CB(coord=(x, y, z))
                    cbs.append(cb)
                
                sb = SB(tag=shape_tag, cbs=cbs)
                sbs.append(sb)
            
            seq = Seq(items=sbs)
            flat_tokens = Serializer.serialize(seq)
            
            # Convert to token IDs
            token_ids = []
            for token in flat_tokens:
                if token in token_mapping:
                    token_ids.append(token_mapping[token])
            
            return token_ids
        
        return tokenizer_fn
    
    def __len__(self) -> int:
        """Return the number of samples in the dataset"""
        return len(self.data)

    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, Dict[int, np.ndarray]]:
        """
        Get a single sample from the dataset
        
        Returns same format as VoxelDataset:
            tuple: (tokens, max_sequence_length, scene_grasps)
        """
        sample = self.data[idx]
        
        # Extract precomputed data
        voxel_data = sample['voxel_data']
        scene_grasps = sample['scene_grasps']
        
        grasp_token_ids = self.convert_scene_grasps_to_tokens(scene_grasps)

        
        
        # 处理token序列（保持原有逻辑）
        token_sequence = self.tokenizer_fn(voxel_data)
        if isinstance(token_sequence, torch.Tensor):
            token_sequence = token_sequence.tolist()
        elif isinstance(token_sequence, (list, tuple)):
            token_sequence = list(token_sequence)
        else:
            token_sequence = [token_sequence] if token_sequence is not None else []
        
        # Convert to tensor
        if len(token_sequence) == 0:
            # Handle empty sequences
            token_sequence = [0]  # Use a default token
        
        # Ensure we have a 2D tensor structure for compatibility with model
        if not isinstance(token_sequence[0], (list, tuple)):
            # If tokens are 1D, expand to 2D with single feature dimension
            token_sequence = [[token] for token in token_sequence]

        
        tokens = torch.tensor(token_sequence, dtype=torch.long)
        grasp_token = torch.tensor(grasp_token_ids, dtype=torch.long).unsqueeze(1)  # Shape [num_grasps, 1]

        tokens = torch.cat([tokens[:-1], grasp_token], dim=0)  # Concatenate along sequence dimension
        
        # Handle sequence length
        seq_len = tokens.shape[0]
        num_features = tokens.shape[1] if tokens.dim() > 1 else 1

        
        if seq_len > self.max_sequence_length:
            # Truncate if too long
            tokens = tokens[:self.max_sequence_length]
            seq_len = self.max_sequence_length
        
        return tokens, self.max_sequence_length, scene_grasps
    
    def get_vocab_size(self) -> int:
        """Get vocabulary size from token_manager"""
        return len(self.token_mapping)
    
    def convert_scene_grasps_to_sequence(self, scene_grasps: Dict[int, List]) -> Seq:
        """
        Convert scene_grasps data to GRASP sequence following grammar definition
        
        Grammar: GRASP → 'detectgrasp' GB*
                 GB    → 'grasp' TAG CB+
        
        Args:
            scene_grasps: Dictionary mapping object_id to list of grasp coordinates
                         Each grasp is represented as deduplicated voxel coordinates
        
        Returns:
            Seq: Sequence containing GRASP item following the grammar
        """
        if not scene_grasps:
            # Return empty GRASP sequence
            empty_grasp = GRASP(gbs=[])
            return Seq(items=[empty_grasp])
        
        # Create GB blocks for each object with grasps
        gb_blocks = []
        
        for obj_id, grasp_list in scene_grasps.items():
            if not grasp_list:
                continue
                
            # Map object_id to shape tag
            if 0 <= obj_id <= 87:
                shape_tag = f'object{obj_id:02d}'  # object00 to object87
            else:
                shape_tag = 'unknow'  # fallback for out-of-range object_ids
            
            # Create one GB for each grasp in the grasp_list
            for grasp_coords in grasp_list:
                # grasp_coords is an array of deduplicated voxel coordinates for one grasp
                cbs = []
                for coord in grasp_coords:
                    # Ensure coordinate is a tuple of 3 integers
                    if len(coord) == 3:
                        x, y, z = int(coord[0]), int(coord[1]), int(coord[2])
                        coord_tuple = (x, y, z)
                        cb = CB(coord=coord_tuple)
                        cbs.append(cb)
                
                # Create GB for this individual grasp if we have at least one coordinate
                if cbs:
                    # Sort CBs by coordinates for consistent ordering
                    cbs.sort(key=lambda cb: cb.coord)
                    gb = GB(tag=shape_tag, cbs=cbs)
                    gb_blocks.append(gb)
        
        # Randomly shuffle GB blocks for data diversity
        random.shuffle(gb_blocks)
        # Create GRASP item with all GB blocks
        grasp_item = GRASP(gbs=gb_blocks)
        
        # Return sequence with the GRASP item
        return Seq(items=[grasp_item])
    
    def convert_scene_grasps_to_tokens(self, scene_grasps: Dict[int, List]) -> List[int]:
        """
        Convert scene_grasps to token IDs using the GRASP grammar
        
        Args:
            scene_grasps: Dictionary mapping object_id to list of grasp coordinates
        
        Returns:
            List[int]: List of token IDs representing the GRASP sequence
        """
        # Convert to GRASP sequence
        grasp_seq = self.convert_scene_grasps_to_sequence(scene_grasps)
        
        # Serialize to flat tokens
        flat_tokens = Serializer.serialize(grasp_seq)
        
        # Convert tokens to token IDs using the mapping
        token_ids = []
        for token in flat_tokens:
            if token in self.token_mapping:
                token_ids.append(self.token_mapping[token])
            else:
                print(f"Warning: Unknown token '{token}' not in mapping")
                # Skip unknown tokens for robustness
                continue
        
        return token_ids